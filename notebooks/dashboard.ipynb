{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Latent Risk Factor - Pipeline Dashboard\n",
    "\n",
    "Central configuration and execution notebook for the full walk-forward validation pipeline.\n",
    "\n",
    "**Workflow:**\n",
    "1. Configure all parameters (Sections 1-2)\n",
    "2. Load data (Section 3)\n",
    "3. Run pipeline (Section 4)\n",
    "4. Inspect results (Sections 5-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from dataclasses import replace, asdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project root: go up from notebooks/ to project root\n",
    "_NB_DIR = Path(os.path.abspath(\"\")).resolve()\n",
    "PROJECT_ROOT = (_NB_DIR / \"..\").resolve() if _NB_DIR.name == \"notebooks\" else _NB_DIR\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.config import (\n",
    "    PipelineConfig,\n",
    "    DataPipelineConfig,\n",
    "    VAEArchitectureConfig,\n",
    "    LossConfig,\n",
    "    TrainingConfig,\n",
    "    InferenceConfig,\n",
    "    RiskModelConfig,\n",
    "    PortfolioConfig,\n",
    "    WalkForwardConfig,\n",
    ")\n",
    "from src.data_pipeline.data_loader import load_data_source\n",
    "from src.data_pipeline.returns import compute_log_returns\n",
    "from src.data_pipeline.features import compute_trailing_volatility\n",
    "from src.integration.pipeline import FullPipeline\n",
    "from src.integration.reporting import export_results, format_summary_table\n",
    "from src.integration.visualization import (\n",
    "    plot_fold_metrics,\n",
    "    plot_e_star_distribution,\n",
    "    plot_pairwise_heatmap,\n",
    "    style_summary_table,\n",
    "    style_fold_table,\n",
    ")\n",
    "from src.utils import get_optimal_device\n",
    "from src.walk_forward.selection import aggregate_fold_metrics, summary_statistics\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\")\n",
    "logger = logging.getLogger(\"dashboard\")\n",
    "\n",
    "print(f\"PyTorch {torch.__version__} | Device: {get_optimal_device()}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration\n",
    "\n",
    "Two configuration profiles are available. **Run ONLY one section:**\n",
    "- **Section 2a** — Synthetic data: minimal parameters for quick end-to-end testing\n",
    "- **Section 2b** — Real data: full production configuration\n",
    "\n",
    "Always run the **Global** cell (below) first, then choose ONE section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GLOBAL\n",
    "# ============================================================\n",
    "SEED = 42\n",
    "DEVICE = str(get_optimal_device())\n",
    "\n",
    "# Data source: \"synthetic\", \"tiingo\", or \"csv\"\n",
    "DATA_SOURCE = \"tiingo\"\n",
    "QUICK_MODE = True  # Set True for minimal config even with real data\n",
    "\n",
    "# Tiingo API keys (used when DATA_SOURCE = \"tiingo\")\n",
    "TIINGO_API_KEYS = [\n",
    "    \"9ba6e57788deaac3b3c38ed47047cabbbd6077e2\",\n",
    "    \"9aad315d49275c400687f41dd26b22328d8b1a26\",\n",
    "]\n",
    "DATA_DIR = \"data/\"  # Directory for Tiingo downloaded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Quick Mode\n",
    "\n",
    "Run **only this cell** to configure the pipeline for a minimal end-to-end test. Skip Section 2b entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SYNTHETIC — Minimal config for end-to-end testing\n",
    "# Run ONLY this cell, then jump to Section 3\n",
    "# ============================================================\n",
    "\n",
    "if QUICK_MODE == True or DATA_SOURCE == \"synthetic\":\n",
    "    DATA_PATH = \"\"\n",
    "    N_STOCKS = 50\n",
    "    N_YEARS = 20\n",
    "\n",
    "    # K=10 for speed; r_max relaxed because C_L floor (384) makes the CNN\n",
    "    # too large for small synthetic universes — acceptable for testing only.\n",
    "    config = PipelineConfig(\n",
    "        data=DataPipelineConfig(\n",
    "            n_stocks=N_STOCKS,\n",
    "            window_length=504,\n",
    "            n_features=2,\n",
    "        ),\n",
    "        vae=VAEArchitectureConfig(\n",
    "            K=100,\n",
    "            window_length=504,\n",
    "            n_features=2,\n",
    "            r_max=1e6,\n",
    "        ),\n",
    "        loss=LossConfig(mode=\"P\"),\n",
    "        training=TrainingConfig(\n",
    "            max_epochs=50,\n",
    "            batch_size=256,\n",
    "            learning_rate=1e-4,\n",
    "            patience=30,\n",
    "        ),\n",
    "        inference=InferenceConfig(),\n",
    "        risk_model=RiskModelConfig(),\n",
    "        portfolio=PortfolioConfig(n_starts=2),\n",
    "        walk_forward=WalkForwardConfig(\n",
    "            total_years=N_YEARS,\n",
    "            min_training_years=max(3, N_YEARS // 3), # At least 3 years to avoid too-short training sets in early folds\n",
    "            holdout_years=max(1, N_YEARS // 5), # At least 1 year holdout to ensure we have a test set in each fold\n",
    "        ),\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    HP_GRID = [{\"mode\": \"P\", \"learning_rate\": 1e-4, \"alpha\": 1.0}]\n",
    "\n",
    "    print(f\"[Quick mode] {N_STOCKS} stocks, {N_YEARS} years, K={config.vae.K}\")\n",
    "    print(f\"  max_epochs={config.training.max_epochs}, patience={config.training.patience}, HP_GRID=1 config, n_starts=2\")\n",
    "    print(f\"  r_max={config.vae.r_max:.0e} (relaxed for testing)\")\n",
    "    print(f\"  Walk-forward: {config.walk_forward.total_years}y total, \"\n",
    "        f\"{config.walk_forward.min_training_years}y min training, \"\n",
    "        f\"{config.walk_forward.holdout_years}y holdout\")\n",
    "    print(f\"  Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Real Data (Production)\n",
    "\n",
    "Run **all cells below** (through \"ASSEMBLE FULL CONFIG\") for full production configuration. Skip Section 2a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA SOURCE — Real data\n",
    "# ============================================================\n",
    "# For CSV source:\n",
    "DATA_PATH = \"data/stock_data.csv\"  # <-- Set path to your stock data CSV\n",
    "\n",
    "# For Tiingo source: run download first:\n",
    "#   python scripts/download_tiingo.py --phase all --keys-file keys.txt\n",
    "# Then set DATA_SOURCE = \"tiingo\" in Global cell above.\n",
    "\n",
    "# Universe and history parameters (overridden by synthetic cell if DATA_SOURCE == \"synthetic\")\n",
    "N_STOCKS = 50                    # Top N stocks by median market cap (50=fast, 200=realistic, 0=all)\n",
    "N_YEARS = 14                     # Years of history to keep (0=all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA PIPELINE (MOD-001)\n",
    "# ============================================================\n",
    "data_cfg = DataPipelineConfig(\n",
    "    n_stocks=N_STOCKS,           # universe cap (same as N_STOCKS above)\n",
    "    window_length=504,           # T: sliding window length (trading days)\n",
    "    n_features=2,                # F: features per timestep (return + realized vol)\n",
    "    vol_window=252,              # trailing vol lookback (days)\n",
    "    vix_lookback_percentile=80.0,# VIX percentile for crisis threshold\n",
    "    min_valid_fraction=0.80,     # minimum valid data fraction per stock\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VAE ARCHITECTURE (MOD-002)\n",
    "# ============================================================\n",
    "vae_cfg = VAEArchitectureConfig(\n",
    "    K=200,                       # latent capacity ceiling\n",
    "    sigma_sq_init=1.0,           # initial observation noise\n",
    "    sigma_sq_min=1e-4,           # lower clamp for sigma^2\n",
    "    sigma_sq_max=10.0,           # upper clamp for sigma^2\n",
    "    window_length=504,           # T (must match data_cfg)\n",
    "    n_features=2,                # F (must match data_cfg)\n",
    "    r_max=5.0,                   # max parameter/data ratio (relaxed for synthetic)\n",
    ")\n",
    "\n",
    "print(f\"Encoder depth L={vae_cfg.encoder_depth}, \"\n",
    "      f\"Final width C_L={vae_cfg.final_layer_width}, \"\n",
    "      f\"D={vae_cfg.D}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOSS FUNCTION (MOD-004)\n",
    "# ============================================================\n",
    "loss_cfg = LossConfig(\n",
    "    mode=\"P\",                    # 'P' (primary), 'F' (fallback), 'A' (advanced)\n",
    "    gamma=3.0,                   # crisis overweighting factor\n",
    "    lambda_co_max=0.5,           # max co-movement loss weight\n",
    "    beta_fixed=1.0,              # fixed beta for Mode A\n",
    "    warmup_fraction=0.20,        # fraction of epochs for Mode F warmup\n",
    "    max_pairs=2048,              # max pairs for co-movement loss\n",
    "    delta_sync=21,               # max date gap for synchronization (days)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING (MOD-005)\n",
    "# ============================================================\n",
    "training_cfg = TrainingConfig(\n",
    "    max_epochs=100,              # maximum training epochs\n",
    "    batch_size=256,              # batch size\n",
    "    learning_rate=1e-4,          # initial learning rate (eta_0)\n",
    "    weight_decay=1e-5,           # Adam weight decay\n",
    "    adam_betas=(0.9, 0.999),     # Adam betas\n",
    "    adam_eps=1e-8,               # Adam epsilon\n",
    "    patience=10,                 # early stopping patience (epochs)\n",
    "    lr_patience=5,               # ReduceLROnPlateau patience\n",
    "    lr_factor=0.5,               # ReduceLROnPlateau factor\n",
    "    n_strata=15,                 # strata for synchronous batching\n",
    "    curriculum_phase1_frac=0.30, # fraction of epochs: sync+stratified batching\n",
    "    curriculum_phase2_frac=0.30, # fraction of epochs: + co-movement loss ramp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INFERENCE (MOD-006)\n",
    "# ============================================================\n",
    "inference_cfg = InferenceConfig(\n",
    "    batch_size=512,              # inference batch size\n",
    "    au_threshold=0.01,           # KL threshold for active unit (nats)\n",
    "    r_min=2,                     # min observations-per-parameter for AU_max\n",
    "    aggregation_method=\"mean\",   # profile aggregation method\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RISK MODEL (MOD-007)\n",
    "# ============================================================\n",
    "risk_model_cfg = RiskModelConfig(\n",
    "    winsorize_lo=5.0,            # lower percentile for vol ratio winsorization\n",
    "    winsorize_hi=95.0,           # upper percentile\n",
    "    d_eps_floor=1e-6,            # floor for idiosyncratic variance\n",
    "    conditioning_threshold=1e6,  # condition number threshold for ridge fallback\n",
    "    ridge_scale=1e-6,            # ridge regularization scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PORTFOLIO OPTIMIZATION (MOD-008)\n",
    "# Constraints identical for VAE and all 6 benchmarks (INV-012)\n",
    "# ============================================================\n",
    "portfolio_cfg = PortfolioConfig(\n",
    "    lambda_risk=1.0,             # risk aversion\n",
    "    w_max=0.05,                  # max weight per stock (hard cap)\n",
    "    w_min=0.001,                 # min active weight (semi-continuous)\n",
    "    w_bar=0.03,                  # concentration penalty threshold\n",
    "    phi=25.0,                    # concentration penalty weight\n",
    "    kappa_1=0.1,                 # linear turnover penalty\n",
    "    kappa_2=7.5,                 # quadratic turnover penalty\n",
    "    delta_bar=0.01,              # turnover penalty threshold\n",
    "    tau_max=0.30,                # max one-way turnover (hard cap)\n",
    "    n_starts=5,                  # multi-start initializations\n",
    "    sca_max_iter=100,            # max SCA iterations\n",
    "    sca_tol=1e-8,               # SCA convergence tolerance\n",
    "    armijo_c=1e-4,               # Armijo sufficient decrease\n",
    "    armijo_rho=0.5,              # Armijo backtracking factor\n",
    "    armijo_max_iter=20,          # max Armijo backtracking steps\n",
    "    max_cardinality_elim=100,    # max cardinality elimination rounds\n",
    "    entropy_eps=1e-30,           # numerical stability for log()\n",
    "    alpha_grid=[0.0, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0],  # frontier alpha grid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# WALK-FORWARD VALIDATION (MOD-009)\n",
    "# ============================================================\n",
    "walk_forward_cfg = WalkForwardConfig(\n",
    "    total_years=N_YEARS,              # total history length\n",
    "    min_training_years=10,       # minimum training window\n",
    "    oos_months=6,                # out-of-sample period (months)\n",
    "    embargo_days=21,             # embargo between train and OOS (trading days)\n",
    "    holdout_years=3,             # final holdout period\n",
    "    val_years=2,                 # nested validation for Phase A\n",
    "    score_lambda_pen=5.0,        # MDD penalty weight in composite score\n",
    "    score_lambda_est=2.0,        # estimation quality penalty weight\n",
    "    score_mdd_threshold=0.20,    # MDD threshold in composite score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HP GRID for Phase A (set to None for default 18-config grid)\n",
    "# ============================================================\n",
    "HP_GRID = None  # None = default: 3 modes x 2 LRs x 3 alphas = 18 configs\n",
    "\n",
    "# Uncomment to define a custom grid:\n",
    "# HP_GRID = [\n",
    "#     {\"mode\": \"P\", \"learning_rate\": 5e-4, \"alpha\": 1.0},\n",
    "#     {\"mode\": \"F\", \"learning_rate\": 1e-3, \"alpha\": 0.5},\n",
    "#     {\"mode\": \"A\", \"learning_rate\": 1e-3, \"alpha\": 2.0},\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ASSEMBLE FULL CONFIG\n",
    "# ============================================================\n",
    "if DATA_SOURCE != \"synthetic\" and QUICK_MODE == False:\n",
    "      config = PipelineConfig(\n",
    "            data=data_cfg,\n",
    "            vae=vae_cfg,\n",
    "            loss=loss_cfg,\n",
    "            training=training_cfg,\n",
    "            inference=inference_cfg,\n",
    "            risk_model=risk_model_cfg,\n",
    "            portfolio=portfolio_cfg,\n",
    "            walk_forward=walk_forward_cfg,\n",
    "            seed=SEED,\n",
    "      )\n",
    "\n",
    "      print(\"PipelineConfig assembled.\")\n",
    "      print(f\"  Walk-forward: {config.walk_forward.total_years}y total, \"\n",
    "            f\"{config.walk_forward.min_training_years}y min training, \"\n",
    "            f\"{config.walk_forward.holdout_years}y holdout\")\n",
    "      print(f\"  VAE: K={config.vae.K}, T={config.vae.window_length}, F={config.vae.n_features}\")\n",
    "      print(f\"  Training: {config.training.max_epochs} max epochs, \"\n",
    "            f\"bs={config.training.batch_size}, lr={config.training.learning_rate}\")\n",
    "      print(f\"  Loss mode: {config.loss.mode}, gamma={config.loss.gamma}\")\n",
    "      print(f\"  Capacity guard r_max: {config.vae.r_max}\")\n",
    "      print(f\"  Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Tiingo Download (run once)\n",
    "\n",
    "Run this cell **only once** to download Tiingo data. After the first run, the data is saved locally and reused automatically.\n",
    "Set `MAX_TICKERS` to a small number (e.g. 5) for testing, or `None` for the full universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TIINGO DOWNLOAD — Run once, data is saved locally\n",
    "# Skip this cell if data is already downloaded or DATA_SOURCE != \"tiingo\"\n",
    "# ============================================================\n",
    "\n",
    "if DATA_SOURCE == \"tiingo\":\n",
    "    import importlib.util\n",
    "\n",
    "    _spec = importlib.util.spec_from_file_location(\n",
    "        \"download_tiingo\", str(PROJECT_ROOT / \"scripts\" / \"download_tiingo.py\")\n",
    "    )\n",
    "    _mod = importlib.util.module_from_spec(_spec)\n",
    "    _spec.loader.exec_module(_mod)  # type: ignore[union-attr]\n",
    "\n",
    "    MAX_TICKERS = None  # Set to None for full universe (~22k tickers)\n",
    "\n",
    "    _mod.run_download(\n",
    "        api_keys=TIINGO_API_KEYS,\n",
    "        data_dir=DATA_DIR,\n",
    "        max_tickers=MAX_TICKERS,\n",
    "        sp500_first=True,  # Download SP500 tickers first (priority)\n",
    "    )\n",
    "else:\n",
    "    print(f\"DATA_SOURCE={DATA_SOURCE}, skipping Tiingo download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "\n",
    "stock_data, start_date = load_data_source(\n",
    "    source=DATA_SOURCE,\n",
    "    data_path=DATA_PATH if DATA_SOURCE == \"csv\" else \"\",\n",
    "    data_dir=DATA_DIR,\n",
    "    n_stocks=N_STOCKS,\n",
    "    n_years=N_YEARS,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Data source: {DATA_SOURCE}\")\n",
    "print(f\"Stock data shape: {stock_data.shape}\")\n",
    "print(f\"Date range: {stock_data['date'].min()} to {stock_data['date'].max()}\")\n",
    "print(f\"Unique stocks: {stock_data['permno'].nunique()}\")\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log-returns and trailing volatility\n",
    "returns = compute_log_returns(stock_data)\n",
    "trailing_vol = compute_trailing_volatility(returns, window=config.data.vol_window)\n",
    "\n",
    "print(f\"Returns: {returns.shape[0]} dates x {returns.shape[1]} stocks\")\n",
    "print(f\"Trailing vol: {trailing_vol.shape} (first {config.data.vol_window-1} rows NaN)\")\n",
    "print(f\"Returns date range: {returns.index[0]} to {returns.index[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Run Pipeline\n",
    "\n",
    "Executes the full walk-forward validation: Phase A (HP selection) + Phase B (deployment) on each fold, then benchmarks, statistical tests, and report generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard Monitoring\n",
    "\n",
    "Launch TensorBoard in a terminal to monitor training in real-time:\n",
    "\n",
    "```bash\n",
    ".venv/bin/tensorboard --logdir runs/\n",
    "```\n",
    "\n",
    "Logs are organized as `runs/fold_XX/phase_a/config_YY_mode_M_lr_L/` and `runs/fold_XX/phase_b/`.\n",
    "\n",
    "**Step-level metrics** (logged every 50 batches) — raw per-batch values, useful to spot instabilities:\n",
    "\n",
    "| Metric | What to look for |\n",
    "|--------|-----------------|\n",
    "| `Step/loss` | Should decrease overall. Spikes are normal early on. |\n",
    "| `Step/reconstruction` | Main driver of total loss. Should decrease steadily. |\n",
    "| `Step/kl_divergence` | Grows as the VAE learns to use latent dimensions. Too high = posterior collapse risk. |\n",
    "| `Step/co_movement` | Ramps up during curriculum phase 2 (epochs 30-60%). Should stabilize. |\n",
    "| `Step/sigma_sq` | Learned observation noise. Should settle between 0.01-1.0. |\n",
    "\n",
    "**Epoch-level metrics** (logged once per epoch) — smoothed averages, useful for overall trends:\n",
    "\n",
    "| Metric | What to look for |\n",
    "|--------|-----------------|\n",
    "| `Loss/total` | Overall training loss (lower = better). |\n",
    "| `Validation/ELBO` | Key metric for early stopping. Gap with training loss = overfitting. |\n",
    "| `Training/active_units` | Number of latent dimensions actually used (KL > 0.01 nats). Target: 15-80% of K. |\n",
    "| `Training/learning_rate` | Should step down when validation plateaus (ReduceLROnPlateau). |\n",
    "| `Training/lambda_co` | Co-movement weight schedule: 0 during phase 1, ramps to lambda_co_max during phase 2. |\n",
    "| `Training/beta_t` | Beta annealing (Mode F only): 0 to 1 during warmup, then fixed at 1. |\n",
    "\n",
    "**Quick diagnostic:**\n",
    "- **Good training**: reconstruction drops, AU grows to 20-60, validation tracks training\n",
    "- **Posterior collapse**: AU stays near 0, KL near 0 — try Mode F or lower learning rate\n",
    "- **Overfitting**: validation diverges from training — early stopping should trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training configuration summary\n",
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE CONFIGURATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Seed: {config.seed} | Device: {DEVICE} | Data: {DATA_SOURCE}\")\n",
    "print()\n",
    "print(f\"  [Data]         n_stocks={config.data.n_stocks}, T={config.data.window_length}, \"\n",
    "      f\"F={config.data.n_features}, vol_window={config.data.vol_window}\")\n",
    "print(f\"  [VAE]          K={config.vae.K}, L={config.vae.encoder_depth}, \"\n",
    "      f\"C_L={config.vae.final_layer_width}, r_max={config.vae.r_max}\")\n",
    "print(f\"  [Loss]         mode={config.loss.mode}, gamma={config.loss.gamma}, \"\n",
    "      f\"lambda_co_max={config.loss.lambda_co_max}, beta_fixed={config.loss.beta_fixed}\")\n",
    "print(f\"  [Training]     max_epochs={config.training.max_epochs}, bs={config.training.batch_size}, \"\n",
    "      f\"lr={config.training.learning_rate}, wd={config.training.weight_decay}\")\n",
    "print(f\"                 patience={config.training.patience}, lr_patience={config.training.lr_patience}, \"\n",
    "      f\"lr_factor={config.training.lr_factor}\")\n",
    "print(f\"                 curriculum: phase1={config.training.curriculum_phase1_frac}, \"\n",
    "      f\"phase2={config.training.curriculum_phase2_frac}, strata={config.training.n_strata}\")\n",
    "print(f\"  [Inference]    bs={config.inference.batch_size}, AU_threshold={config.inference.au_threshold}, \"\n",
    "      f\"r_min={config.inference.r_min}\")\n",
    "print(f\"  [Risk Model]   winsorize=[{config.risk_model.winsorize_lo}, {config.risk_model.winsorize_hi}], \"\n",
    "      f\"cond_threshold={config.risk_model.conditioning_threshold:.0e}\")\n",
    "print(f\"  [Portfolio]    w_max={config.portfolio.w_max}, w_min={config.portfolio.w_min}, \"\n",
    "      f\"tau_max={config.portfolio.tau_max}, n_starts={config.portfolio.n_starts}\")\n",
    "print(f\"  [Walk-Forward] {config.walk_forward.total_years}y total, \"\n",
    "      f\"{config.walk_forward.min_training_years}y min train, \"\n",
    "      f\"{config.walk_forward.oos_months}mo OOS, \"\n",
    "      f\"{config.walk_forward.holdout_years}y holdout, \"\n",
    "      f\"embargo={config.walk_forward.embargo_days}d\")\n",
    "n_hp = len(HP_GRID) if HP_GRID else \"18 (default)\"\n",
    "print(f\"  [HP Grid]      {n_hp} configs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "TB_DIR = \"runs/\"  # TensorBoard log directory (set to None to disable)\n",
    "\n",
    "pipeline = FullPipeline(config, tensorboard_dir=TB_DIR)\n",
    "\n",
    "results = pipeline.run(\n",
    "    stock_data=stock_data,\n",
    "    returns=returns,\n",
    "    trailing_vol=trailing_vol,\n",
    "    skip_phase_a=(DATA_SOURCE == \"synthetic\" or QUICK_MODE == True),\n",
    "    vix_data=None,\n",
    "    start_date=start_date,\n",
    "    hp_grid=HP_GRID,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "print(\"Pipeline complete.\")\n",
    "print(f\"Folds processed: {len(results['vae_results'])}\")\n",
    "print(f\"Benchmarks: {list(results['benchmark_results'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Results - Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text summary\n",
    "print(format_summary_table(results[\"report\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployment recommendation\n",
    "deployment = results[\"report\"][\"deployment\"]\n",
    "print(f\"Scenario: {deployment['scenario']}\")\n",
    "print(f\"Recommendation: {deployment['recommendation']}\")\n",
    "print()\n",
    "print(\"Per-benchmark wins (VAE vs benchmark on primary metrics):\")\n",
    "for bench, info in deployment[\"per_benchmark\"].items():\n",
    "    print(f\"  {bench:20s}: {info['wins']}/{info['total']} metrics won\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE summary statistics\n",
    "vae_df = aggregate_fold_metrics(results[\"vae_results\"])\n",
    "vae_summary = summary_statistics(vae_df)\n",
    "print(\"VAE Summary Statistics:\")\n",
    "style_summary_table(vae_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark summary statistics\n",
    "for bench_name, bench_metrics in results[\"benchmark_results\"].items():\n",
    "    bench_df = aggregate_fold_metrics(bench_metrics)\n",
    "    bench_summary = summary_statistics(bench_df)\n",
    "    print(f\"\\n{bench_name} Summary:\")\n",
    "    display(style_summary_table(bench_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Results - Per-Fold Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE per-fold metrics\n",
    "print(\"VAE Per-Fold Metrics:\")\n",
    "style_fold_table(vae_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E* distribution\n",
    "e_star_summary = results[\"report\"][\"e_star_summary\"]\n",
    "print(f\"E* epochs: mean={e_star_summary['mean']:.1f}, \"\n",
    "      f\"std={e_star_summary['std']:.1f}, \"\n",
    "      f\"range=[{e_star_summary['min']}, {e_star_summary['max']}]\")\n",
    "\n",
    "plot_e_star_distribution(results[\"e_stars\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fold metrics: VAE vs benchmarks\n",
    "plot_fold_metrics(results[\"vae_results\"], results[\"benchmark_results\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Results - Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise tests heatmap\n",
    "plot_pairwise_heatmap(results[\"report\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed pairwise test results\n",
    "tests = results[\"report\"][\"statistical_tests\"]\n",
    "print(f\"Total comparisons: {tests['n_tests']} (alpha={tests['alpha']})\")\n",
    "print()\n",
    "\n",
    "for bench_name, metrics in tests[\"pairwise\"].items():\n",
    "    print(f\"VAE vs {bench_name}:\")\n",
    "    for metric, result in metrics.items():\n",
    "        if result.get(\"skipped\", False):\n",
    "            print(f\"  {metric}: skipped ({result['reason']})\")\n",
    "            continue\n",
    "        sig = \" *\" if result.get(\"significant_corrected\", False) else \"\"\n",
    "        print(f\"  {metric}: delta={result['median_delta']:+.4f} \"\n",
    "              f\"[{result['ci_lower']:+.4f}, {result['ci_upper']:+.4f}] \"\n",
    "              f\"p={result.get('p_corrected', result['p_value']):.4f}{sig}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"results/\"\n",
    "\n",
    "written = export_results(results, asdict(config), output_dir=OUTPUT_DIR)\n",
    "\n",
    "print(f\"Results saved to {OUTPUT_DIR}\")\n",
    "for path in written:\n",
    "    print(f\"  {os.path.basename(path)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Latent_risk_factor (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
