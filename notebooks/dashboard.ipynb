{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install .\n",
    "\n",
    "# MOSEK setup for Colab (required for fast portfolio optimization)\n",
    "# 1. Install MOSEK:\n",
    "# !pip install mosek\n",
    "# 2. Upload your mosek.lic file to Colab, then copy it:\n",
    "# !mkdir -p ~/mosek && cp mosek.lic ~/mosek/mosek.lic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /content/Latent_risk_factor\n",
    "# %run notebooks/dashboard.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Latent Risk Factor - Pipeline Dashboard\n",
    "\n",
    "Central configuration and execution notebook for the full walk-forward validation pipeline.\n",
    "\n",
    "**Workflow:**\n",
    "1. Configure all parameters (Sections 1-2)\n",
    "2. Load data (Section 3)\n",
    "3. Run pipeline (Section 4)\n",
    "4. Inspect results (Sections 5-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathis/Latent_risk_factor/Latent_risk_factor/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-02-21 19:47:20,114 [INFO] src.utils: Device auto-detected: MPS (Apple Silicon)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 2.10.0 | Device: mps\n",
      "Working directory: /Users/mathis/Latent_risk_factor/Latent_risk_factor\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from dataclasses import replace, asdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project root: go up from notebooks/ to project root\n",
    "_NB_DIR = Path(os.path.abspath(\"\")).resolve()\n",
    "PROJECT_ROOT = (_NB_DIR / \"..\").resolve() if _NB_DIR.name == \"notebooks\" else _NB_DIR\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.config import (\n",
    "    PipelineConfig,\n",
    "    DataPipelineConfig,\n",
    "    VAEArchitectureConfig,\n",
    "    LossConfig,\n",
    "    TrainingConfig,\n",
    "    InferenceConfig,\n",
    "    RiskModelConfig,\n",
    "    PortfolioConfig,\n",
    "    WalkForwardConfig,\n",
    ")\n",
    "from src.data_pipeline.data_loader import load_data_source\n",
    "from src.data_pipeline.returns import compute_log_returns\n",
    "from src.data_pipeline.features import compute_trailing_volatility\n",
    "from src.integration.pipeline import FullPipeline\n",
    "from src.integration.reporting import export_results, format_summary_table\n",
    "from src.integration.visualization import (\n",
    "    plot_fold_metrics,\n",
    "    plot_e_star_distribution,\n",
    "    plot_pairwise_heatmap,\n",
    "    style_summary_table,\n",
    "    style_fold_table,\n",
    ")\n",
    "from src.utils import get_optimal_device\n",
    "from src.walk_forward.selection import aggregate_fold_metrics, summary_statistics\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "# force=True required in Colab/Jupyter where root logger is pre-configured\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\", force=True)\n",
    "logger = logging.getLogger(\"dashboard\")\n",
    "\n",
    "print(f\"PyTorch {torch.__version__} | Device: {get_optimal_device()}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration\n",
    "\n",
    "Two configuration profiles are available. **Run ONLY one section:**\n",
    "- **Section 2a** \u2014 Synthetic data: minimal parameters for quick end-to-end testing\n",
    "- **Section 2b** \u2014 Real data: full production configuration\n",
    "\n",
    "Always run the **Global** cell (below) first, then choose ONE section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-21 19:47:20,121 [INFO] src.utils: Device auto-detected: MPS (Apple Silicon)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# GLOBAL\n",
    "# ============================================================\n",
    "SEED = 42\n",
    "DEVICE = str(get_optimal_device())\n",
    "\n",
    "# Data source: \"synthetic\", \"tiingo\", or \"csv\"\n",
    "DATA_SOURCE = \"tiingo\"\n",
    "QUICK_MODE = False          # Set True for minimal config even with real data\n",
    "RUN_FULL_PIPELINE = False   # Set False to skip training and inference (useful for quick visualization)\n",
    "\n",
    "# Pretrained model: set to a checkpoint path to skip VAE training and re-run only portfolio optimization\n",
    "# Set to None to train from scratch\n",
    "CHECKPOINT_PATH = \"results/diagnostic/checkpoints/checkpoint_fold00_train_N3155_Y25_T504_K75_F2.pt\"  # e.g. \"checkpoints/checkpoint_direct_N200_Y10_T504_K200_F2.pt\"\n",
    "\n",
    "# Tiingo API keys (used when DATA_SOURCE = \"tiingo\")\n",
    "TIINGO_API_KEYS = [\n",
    "    \"9ba6e57788deaac3b3c38ed47047cabbbd6077e2\",\n",
    "    \"9aad315d49275c400687f41dd26b22328d8b1a26\",\n",
    "    \"5d0dd679cc88f2a55c833b6e4ffd8d8d56fa0fc5\",\n",
    "    \"66134d425322719f2465e1a021fa43c8054fe157\",\n",
    "    \"1bc7a5bcd30f0d4e32137e92dc74f162da8d2424\",\n",
    "    \"643fbd2e772679625edda7916feb0aaac7d5eb4a\",\n",
    "    \"1d4af3a9a1ce6092fd6dd08ed95c702b6c63c934\",\n",
    "    \"b9a4b731ee5ba3dd02bc09614613ee22e52a7c75\",\n",
    "    \"3b2337b53147ba06aafe33122887ee143bd9ffdd\",\n",
    "    \"561d65cdc9a7dddfd9d891522250bf40f25e9305\",\n",
    "    \"cea76c53d7bafad468b98d32ea600eec2e104561\",\n",
    "    \"e07dce0affdccd565f803a4152364515d2df7f78\",\n",
    "    \"04b97f43d5ef1387395207f5d4388e8cf28fd5f8\",\n",
    "    \"289f4878471f82cee0a533f49aa44bae080dce8f\",\n",
    "    \"1b4e3c553ac4d6078395bdd55cd2f292222919d0\",\n",
    "    \"0a4dd0e82339e617fff7fbb6d8e2affe11194aa8\",\n",
    "    \"43455b994add9f54e6dc629399f851b007f2a038\",\n",
    "    \"a27f7790680f379499ddfe1302ec68a3c9a75a74\",\n",
    "    \"707f2f47875bb820471cff943a7462d91dcbf6ba\",\n",
    "    \"4745d791cd7d08912015bf65edb12fbe256b3cda\",\n",
    "    \"2a2fb9bc6a5d12c3097499a42bbb2afd76f538f4\",\n",
    "    \"15627485874843399ce75d8e984a8c3473e83e65\",\n",
    "    \"c9793a615c339d204454dbac332680fe25c5d471\",\n",
    "    \"669f1bfa6a4b1b51042304af7daf6288f53ea88b\",\n",
    "    \"ffa58b3d42c7b34547574e9318090067621c97d0\",\n",
    "    \"2d18ed83dc9c9b5ea2faa284cdaf6188819cb2ac\",\n",
    "    \"8bfe595273b49c7bafac292261c0e5670002202f\",\n",
    "    \"7f5c957a4129d6bdc8644fc4832e8148828740b8\",\n",
    "    \"fe353f2c950b8e03b4e83e569a0b129ede8e4d35\",\n",
    "    \"3b1ba45faa5f00e1c6fce3beb905c043caefec26\",\n",
    "    \"552b2ca3c474c9694dc417f95b5113175dfdbcce\",\n",
    "    \"6460458e258d2a7f0fc0bb03bc94c2047ba82105\",\n",
    "    \"3062157e1ace50ba07b2a4af9ee4bac16a369b39\"\n",
    "]\n",
    "DATA_DIR = \"data/\"          # Directory for Tiingo downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STOCKS = 0                  #[\u2b50\ufe0f] Top N stocks by median market cap (50=fast, 200=realistic, 0=all)\n",
    "N_YEARS = 40                     #[\u2b50\ufe0f] Years of history to keep\n",
    "MIN_N_YEARS = 10                 # First fold trains on at least MIN_N_YEARS (shorter = unreliable model)\n",
    "T = 504                          #[\u2b50\ufe0f] Window length (trading days, ~2 years)\n",
    "N_FEATURES = 2                   #[\u2b50\ufe0f] Features per timestep (return + realized vol)\n",
    "K = 75                           #[\u2b50\ufe0f] Max latent factors the VAE can discover (only \"active units\" AU<=K are used)\n",
    "LOSS_MODE = \"P\"                  # Loss mode: \"P\"=full ELBO with learned sigma^2, \"F\"=simplified with beta warmup, \"A\"=hybrid\n",
    "CRISIS_OVERWEIGHTING = 3.0       #[\u2b50\ufe0f] Crisis overweighting: crisis windows count 3x more in reconstruction loss\n",
    "MAX_CO_MOVEMENT_WEIGHT = 0.5     # Max co-movement weight: how strongly latent distances must match Spearman correlations\n",
    "\n",
    "MAX_EPOCHS = 800                 #[\u2b50\ufe0f] Max training epochs (early stopping may halt training sooner)\n",
    "BATCH_SIZE = 512                 # Training batch size (windows)\n",
    "LEARNING_RATE = 1e-3             # Adam learning rate\n",
    "EARLY_STOPPING_PATIENCE = 50     #[\u2b50\ufe0f] Early stopping patience (epochs without improvement)\n",
    "ES_MIN_DELTA = 0.5                 #[\u2b50\ufe0f] Min ELBO improvement to count as progress (0=any improvement resets counter)\n",
    "LR_PATIENCE = 30                 # Reduce LR after this many stagnant epochs (triggers before early stop)\n",
    "LR_FACTOR = 0.75                 # LR reduction factor when plateau detected\n",
    "DROPOUT = 0.3\n",
    "TRAINING_STRIDE = 21             # Training window stride (21 = one window per month per stock, faster training)\n",
    "COMPILE_MODEL = True             # torch.compile on CUDA/MPS (faster forward/backward, ~30s warmup)\n",
    "\n",
    "AU_THRESHOLD = 0.01              # KL > 0.01 nats = dimension is \"active\" (below = unused, posterior \u2248 prior)\n",
    "AGGREGATION_METHOD = \"mean\"      # Method to aggregate overlapping window predictions (\"mean\", \"median\", etc.)\n",
    "AGGREGATION_HALF_LIFE = 60       #[\u2b50\ufe0f] Exponential decay half-life (in windows) for profile aggregation.\n",
    "                                 # 0=uniform mean (legacy). 60=recent windows weighted ~5yr half-life at stride=21.\n",
    "                                 # Higher = B_A reflects current market structure, lower = more stable/historical.\n",
    "\n",
    "SIGMA_Z_EIGENVALUE_PCT = 0.95    #[\u2b50\ufe0f] Keep top eigenvalues explaining 95% of Sigma_z variance.\n",
    "                                 # 1.0=no truncation. <1.0=discard noisy factor dimensions.\n",
    "\n",
    "B_A_SHRINKAGE_ALPHA = 0.0        #[\u2b50\ufe0f] Shrink B_A towards zero: B_A *= (1 - alpha).\n",
    "                                 # 0.0=no shrinkage (default). Z-scoring per-factor is sufficient\n",
    "                                 # (Barra USE4). Uniform scaling is absorbed by variance targeting.\n",
    "\n",
    "# --- Eigenvalue power shrinkage ---\n",
    "# Compresses dominant eigenvalues to reduce factor concentration.\n",
    "# eigenvalues = eigenvalues^p where p < 1 flattens the spectrum.\n",
    "EIGENVALUE_POWER = 0.65           #[\u2b50\ufe0f] 0.65=compress dominant eigenvalues, 1.0=off, 0.5=sqrt\n",
    "\n",
    "HP_GRID = None                   # HP GRID for Phase A (set to None for default 18-config grid : 3 loss modes \u00d7 2 learning rates \u00d7 3 alphas = 18 configs)\n",
    "                                 # Phase A tries all configs on nested validation to find the best HP set per fold\n",
    "                \n",
    "# Uncomment to define a custom grid (faster, but less thorough HP search):\n",
    "# HP_GRID = [\n",
    "#     {\"mode\": \"P\", \"learning_rate\": 5e-4, \"alpha\": 1.0},   # mode: loss formulation, alpha: risk-entropy tradeoff\n",
    "#     {\"mode\": \"F\", \"learning_rate\": 1e-3, \"alpha\": 0.5},\n",
    "#     {\"mode\": \"A\", \"learning_rate\": 1e-3, \"alpha\": 2.0},\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Variance (w'Sigma w) = total portfolio risk. Minimizing it concentrates\n",
    "# weight on low-vol stocks \u2014 but can put 85% of risk on 2-3 latent\n",
    "# factors without noticing (fragile to factor shocks).\n",
    "#\n",
    "# Entropy H(w) = Shannon entropy on each factor's risk contribution.\n",
    "# Maximizing it spreads risk evenly across all active factors \u2014 no\n",
    "# single factor dominates (resilient to individual factor crashes).\n",
    "#\n",
    "# lambda_risk and alpha control the tradeoff: more lambda_risk favors\n",
    "# low total variance; more alpha favors even factor risk distribution.\n",
    "#\n",
    "# lambda_risk guide (scales daily variance w'Sigma_daily w to the objective):\n",
    "#   252  \u2192 annualized risk aversion gamma=1 (default, balanced)\n",
    "#   504  \u2192 gamma=2 (conservative)\n",
    "#   1260 \u2192 gamma=5 (near min-variance)\n",
    "#   50   \u2192 gamma~0.2 (entropy-dominated, higher variance OK)\n",
    "#\n",
    "# Penalty tuning guide (defaults calibrated for US mid/large cap):\n",
    "#   Illiquid universe (small caps) \u2192 raise kappa_1 & kappa_2\n",
    "#   Infrequent rebalancing         \u2192 lower kappa_1 & kappa_2\n",
    "#   Allow more concentrated bets   \u2192 raise w_bar or lower phi\n",
    "#   Force tighter diversification  \u2192 lower w_bar or raise phi\n",
    "#   Smoother trades                \u2192 lower delta_bar (e.g. 0.005)\n",
    "#\n",
    "# Valid ranges from DVT spec:\n",
    "#   kappa_1: [0.01, 1.0]  kappa_2: [1, 50]  delta_bar: [0.005, 0.03]\n",
    "# ============================================================\n",
    "\n",
    "LAMBDA_RISK = 252.0              #[\u2b50\ufe0f] Risk aversion (252=annualized gamma=1; higher = more conservative)\n",
    "\n",
    "W_MAX = 0.03                     # Hard cap per stock (CVXPY constraint: w_i <= w_max)\n",
    "W_MIN = 0.001                    # Min active weight: below this, stock is eliminated (0 or >= 0.1%)\n",
    "W_BAR = 0.015                     # Soft cap: concentration penalty threshold (phi penalizes w_i > w_bar)\n",
    "\n",
    "PHI = 15.0                        # Concentration penalty strength: phi * sum(max(0, w_i - w_bar)^2). 0=off, 5=moderate\n",
    "KAPPA_1 = 0.1                    # Linear turnover penalty (penalizes trading costs at rebalance)\n",
    "KAPPA_2 = 7.5                    # Quadratic turnover penalty (penalizes large trades more than small ones)\n",
    "DELTA_BAR = 0.01                 # Turnover below 1% is not penalized (de minimis threshold)\n",
    "MAX_TURNOVER = 0.30              #[\u2b50\ufe0f] Max 30% turnover per rebalance (prevents excessive trading costs)\n",
    "\n",
    "# ============================================================\n",
    "# OOS REBALANCING (DVT \u00a74.2)\n",
    "# ============================================================\n",
    "REBALANCING_FREQUENCY = 63       #[\u2b50\ufe0f] Days between rebalancing (0 = buy-and-hold, 21 = monthly, 63 = quarterly)\n",
    "ENTROPY_TRIGGER_ALPHA = 0.90     # Exceptional rebalancing if H drops below 90% of last rebalancing H\n",
    "\n",
    "ALPHA_GRID = [0, 0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0]  #[\u2b50\ufe0f] 12-point grid for reliable Kneedle elbow detection\n",
    "\n",
    "SCA_N_STARTS = 3                # Multi-start optimizations (more = better optimum, proportionally slower)\n",
    "\n",
    "# --- Entropy gradient normalization (Phase 14, Finding 1) ---\n",
    "# Rescales entropy gradient to match risk gradient magnitude at each SCA iteration.\n",
    "# Without this, entropy dominates risk ~700:1, making the optimizer a pure entropy maximizer.\n",
    "NORMALIZE_ENTROPY_GRADIENT = True  #[\u2b50\ufe0f] True = balance entropy/risk gradients (recommended)\n",
    "\n",
    "# --- Entropy budget mode (Phase 14, Finding 2) ---\n",
    "# \"proportional\" = tilted entropy with budget b_k = lambda_k / sum(lambda)\n",
    "#   Targets risk contributions proportional to eigenvalues (respects spectral structure)\n",
    "# \"uniform\" = standard Shannon entropy (equal risk contributions across factors)\n",
    "ENTROPY_BUDGET_MODE = \"proportional\"  #[\u2b50\ufe0f] \"proportional\" (recommended) or \"uniform\"\n",
    "\n",
    "# --- Cross-sectional momentum (expected return signal) ---\n",
    "# Jegadeesh & Titman (1993): 12-month cumulative return minus last month,\n",
    "# z-scored cross-sectionally. Set MOMENTUM_ENABLED=True to activate.\n",
    "MOMENTUM_ENABLED = True          #[\u2b50\ufe0f] Toggle: True = use momentum signal mu, False = pure risk budgeting (mu=0)\n",
    "MOMENTUM_LOOKBACK = 252           # Lookback window in trading days (~12 months)\n",
    "MOMENTUM_SKIP = 21                # Skip last month (avoids short-term reversal)\n",
    "MOMENTUM_WEIGHT = 0.30            #[\u2b50\ufe0f] Scaling factor for momentum signal (gamma_mom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pipeline Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `n_stocks` | Universe size \u2014 how many stocks to keep (ranked by market cap). More = better diversification, slower training. |\n",
    "| `window_length` | Length of each input window in trading days. 504 \u2248 2 years of daily data. Each stock produces many overlapping windows. |\n",
    "| `n_features` | Features per timestep. 2 = log-return + realized volatility. |\n",
    "| `vol_window` | Lookback (trading days) for trailing volatility computation. 252 \u2248 1 year. |\n",
    "| `vix_lookback_percentile` | VIX percentile above which a day is labeled \"crisis\". Higher = fewer crisis days = less overweighting. |\n",
    "| `min_valid_fraction` | Minimum fraction of non-missing data to keep a stock. 0.80 = stocks missing >20% of their history are dropped. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAE Architecture Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `K` | Maximum latent capacity \u2014 how many risk factors the VAE *can* learn. Only \"active units\" (AU \u2264 K) are actually used. Typical: 100-200. |\n",
    "| `sigma_sq_init` | Starting value of learned observation noise \u03c3\u00b2. 1.0 = assume noise equals signal at first. The model learns the true value during training (Mode P/A). |\n",
    "| `sigma_sq_min` / `sigma_sq_max` | Clamp bounds for \u03c3\u00b2. Prevents extreme values: too small = overfitting (model claims perfect reconstruction), too large = underfitting (model gives up). |\n",
    "| `window_length` / `n_features` | Must match DataPipelineConfig \u2014 determines input tensor shape (T\u00d7F). |\n",
    "| `r_max` | Maximum ratio of model parameters to data points. Safety guard \u2014 if the CNN has more parameters than the data can support, training is rejected. Relaxed automatically for small universes. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `mode` | Loss formulation. **P** = full probabilistic ELBO with learned \u03c3\u00b2 (recommended). **F** = simplified with \u03b2 warmup, \u03c3\u00b2 frozen at 1.0 (use if Mode P diverges). **A** = hybrid with tunable KL weight \u03b2. |\n",
    "| `gamma` | Crisis overweighting factor. 3.0 = windows falling in crisis periods (high VIX) count 3\u00d7 more in reconstruction loss. Forces the model to learn crisis dynamics well. |\n",
    "| `lambda_co_max` | Maximum co-movement loss weight. Controls how strongly latent distances must match stock Spearman correlations. Active during Phases 1-2, decays to 0 in Phase 3. |\n",
    "| `beta_fixed` | Fixed KL weight for Mode A (must be 1.0 for Mode P). Values <1 reduce regularization pressure, giving more freedom to reconstruction. |\n",
    "| `warmup_fraction` | Fraction of training where \u03b2 ramps 0\u21921 (Mode F only). Prevents posterior collapse by letting the model learn to reconstruct before enforcing KL regularization. |\n",
    "| `max_pairs` | Max stock pairs sampled per batch for co-movement loss. Limits compute cost \u2014 full pairwise is O(B\u00b2). |\n",
    "| `delta_sync` | Maximum date gap (calendar days) for windows to be \"synchronized\" in the same time block. 21 \u2248 1 month. Ensures co-movement comparisons are temporally valid. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `max_epochs` | Hard limit on training duration. Training may stop earlier via early stopping. |\n",
    "| `batch_size` | Windows per gradient update. Larger = smoother gradients but more memory. 512 is a good default on A100. |\n",
    "| `learning_rate` | Initial optimizer step size (\u03b7\u2080). Too high \u2192 loss diverges. Too low \u2192 converges very slowly. Typical: 1e-4 to 1e-3. |\n",
    "| `weight_decay` | L2 regularization on model weights. Penalizes large weights to reduce overfitting. 1e-5 is mild. |\n",
    "| `adam_betas` / `adam_eps` | Adam optimizer internals (momentum and numerical stability). Rarely need tuning. |\n",
    "| `patience` | **Early stopping** \u2014 if validation ELBO doesn't improve for this many consecutive epochs, stop training and restore the best checkpoint. |\n",
    "| `lr_patience` | **LR reduction** \u2014 if validation stagnates for this many epochs, multiply LR by `lr_factor`. Triggers before early stopping. |\n",
    "| `lr_factor` | LR reduction multiplier. 0.5 = halve the learning rate each time it triggers. |\n",
    "| `n_strata` | Number of volatility-based groups for stratified batching (Phases 1-2). Ensures each batch contains stocks from all risk profiles, not just one cluster. |\n",
    "| `curriculum_phase1_frac` | Fraction of epochs for Phase 1 (co-movement at full strength + synchronized batching). |\n",
    "| `curriculum_phase2_frac` | Fraction of epochs for Phase 2 (co-movement linearly decaying). Phase 3 = remainder (free refinement, random batching). |\n",
    "| `training_stride` | Window stride for training data. 21 = one window per month per stock (21\u00d7 fewer windows, much faster). Inference always uses stride=1. |\n",
    "| `compile_model` | Enable `torch.compile` on CUDA/MPS for faster forward/backward passes. ~30s warmup cost, then ~20-40% speedup. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `batch_size` | Batch size for inference pass. Can be larger than training (no gradients stored = less memory). |\n",
    "| `au_threshold` | KL threshold in nats to consider a latent dimension \"active\". 0.01 is standard \u2014 dimensions with KL < 0.01 are effectively unused (posterior \u2248 prior). |\n",
    "| `r_min` | Minimum observations-per-parameter ratio. Caps AU_max = \u230a\u221a(2\u00b7N_obs/r_min)\u230b to prevent more active factors than the data can reliably estimate. |\n",
    "| `aggregation_method` | How to combine predictions from overlapping windows for the same stock. \"mean\" averages all windows. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Risk Model Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `winsorize_lo` / `winsorize_hi` | Percentile bounds for clipping extreme volatility ratios during rescaling. [5, 95] = trim the 5% most extreme values on each side. |\n",
    "| `d_eps_floor` | Minimum idiosyncratic (stock-specific) variance. Prevents division by zero when a stock has near-zero residual risk. |\n",
    "| `conditioning_threshold` | If the covariance matrix condition number exceeds this, the factor regression switches to ridge regression for numerical stability. |\n",
    "| `ridge_scale` | Ridge regularization strength when the fallback activates. Small value (1e-6) = minimal regularization, just enough to stabilize. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Portfolio Optimization Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `lambda_risk` | Risk aversion. Higher = portfolio avoids variance more aggressively, at the cost of lower expected return. |\n",
    "| `w_max` | Hard cap per stock. 0.05 = no stock can exceed 5% of the portfolio. |\n",
    "| `w_min` | Minimum active weight. Stocks allocated below this are eliminated entirely (semi-continuous: either 0 or \u2265 w_min). |\n",
    "| `w_bar` | Concentration penalty threshold. Stocks above this weight get penalized to encourage diversification. |\n",
    "| `phi` | Concentration penalty strength. Higher = more aggressively pushes weights below w_bar. |\n",
    "| `kappa_1` / `kappa_2` | Linear and quadratic turnover penalties. Penalize trading costs when rebalancing. Higher = more stable portfolio across rebalances. |\n",
    "| `delta_bar` | Turnover penalty threshold \u2014 small weight changes below this are not penalized. |\n",
    "| `tau_max` | Maximum one-way turnover per rebalance. 0.30 = at most 30% of the portfolio can change in a single rebalance. |\n",
    "| `n_starts` | Multi-start initializations for the SCA optimizer. More starts = higher chance of finding the global optimum, but proportionally slower. |\n",
    "| `sca_max_iter` / `sca_tol` | SCA (Sequential Convex Approximation) iteration limit and convergence tolerance. |\n",
    "| `armijo_*` | Line search parameters (sufficient decrease, backtracking factor, max steps). Controls step size selection within SCA. |\n",
    "| `max_cardinality_elim` | Maximum rounds of sequential stock elimination to enforce the minimum weight constraint. |\n",
    "| `entropy_eps` | Tiny constant (1e-30) added inside log() to avoid log(0). Pure numerical safety. |\n",
    "| `alpha_grid` | Grid of \u03b1 values for the variance-entropy frontier. The optimizer tries each \u03b1 and picks the best tradeoff between risk and factor diversification. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Walk-Forward Validation Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `total_years` | Total history length used for the walk-forward. More years = more folds = more robust evaluation, but requires more data. |\n",
    "| `min_training_years` | Minimum training window. The first fold starts with this many years of training data. Too small = unreliable model, too large = few folds. |\n",
    "| `oos_months` | Out-of-sample test period per fold. After training, the portfolio is tested on this many months, then the window slides forward. |\n",
    "| `embargo_days` | Gap (trading days) between training end and OOS start. Prevents information leakage from overlapping windows near the boundary. 21 \u2248 1 month. |\n",
    "| `holdout_years` | Final holdout period excluded from all training/testing. Reserved for ultimate out-of-sample validation. |\n",
    "| `val_years` | Nested validation window within training for Phase A hyperparameter selection. Used to score HP configs without touching OOS data. |\n",
    "| `score_lambda_pen` | Weight of maximum drawdown penalty in the composite HP scoring function. Higher = favor configs with lower drawdowns. |\n",
    "| `score_lambda_est` | Weight of estimation quality penalty (variance ratio) in scoring. Higher = favor configs with more accurate risk predictions. |\n",
    "| `score_mdd_threshold` | Maximum drawdown threshold. Drawdowns beyond this are heavily penalized in the composite score. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Quick Mode\n",
    "\n",
    "Run **only this cell** to configure the pipeline for a minimal end-to-end test. Skip Section 2b entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# QUICK MODE \u2014 Minimal config for end-to-end testing\n",
    "# Run ONLY this cell, then jump to Section 3\n",
    "# ============================================================\n",
    "\n",
    "if QUICK_MODE == True or DATA_SOURCE == \"synthetic\":\n",
    "    DATA_PATH = \"\"\n",
    "    N_STOCKS = 50   # How many stocks in the universe (ranked by market cap)\n",
    "    N_YEARS = 20    # Years of history to use\n",
    "\n",
    "    config = PipelineConfig(\n",
    "        data=DataPipelineConfig(\n",
    "            n_stocks=N_STOCKS,       # Universe size (top N by market cap)\n",
    "            window_length=504,       # Input window = 504 trading days (~2 years)\n",
    "            n_features=2,            # 2 features per timestep: log-return + realized vol\n",
    "            training_stride=TRAINING_STRIDE,  # Window stride for training (21 = monthly, 1 = daily)\n",
    "        ),\n",
    "        vae=VAEArchitectureConfig(\n",
    "            K=100,                   # Max latent factors the VAE can discover (only AU will be active)\n",
    "            window_length=504,       # Must match data window_length\n",
    "            n_features=2,            # Must match data n_features\n",
    "            r_max=5.0,               # Max param/data ratio (relaxed for small universes, auto-adjusted)\n",
    "        ),\n",
    "        loss=LossConfig(\n",
    "            mode=\"P\",                # \"P\" = full probabilistic ELBO with learned observation noise sigma^2\n",
    "        ),\n",
    "        training=TrainingConfig(\n",
    "            max_epochs=50,           # Hard limit on training (early stopping may trigger sooner)\n",
    "            batch_size=512,          # Windows per gradient update (512 = good balance speed/stability)\n",
    "            learning_rate=1e-4,      # Initial optimizer step size (typical: 1e-4 to 1e-3)\n",
    "            patience=30,             # Stop if validation doesn't improve for 30 epochs\n",
    "            compile_model=COMPILE_MODEL,  # torch.compile on CUDA/MPS\n",
    "        ),\n",
    "        inference=InferenceConfig(),   # Defaults: batch_size=512, au_threshold=0.01, r_min=2\n",
    "        risk_model=RiskModelConfig(),  # Defaults: winsorize=[5,95], cond_threshold=1e6, ridge=1e-6\n",
    "        portfolio=PortfolioConfig(\n",
    "            n_starts=2,              # Multi-start optimizations (2 = fast, 5+ = production)\n",
    "        ),\n",
    "        walk_forward=WalkForwardConfig(\n",
    "            total_years=N_YEARS,             # Total history for walk-forward folds\n",
    "            min_training_years=max(3, N_YEARS // 3),  # Min training window (>=3y for reliability)\n",
    "            holdout_years=max(1, N_YEARS // 5),       # Final holdout excluded from all folds (>=1y)\n",
    "        ),\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    # Single HP config (skip Phase A grid search for speed)\n",
    "    HP_GRID = [{\"mode\": \"P\", \"learning_rate\": 1e-4, \"alpha\": 1.0}]\n",
    "\n",
    "    print(f\"[Quick mode] {N_STOCKS} stocks, {N_YEARS} years, K={config.vae.K}\")\n",
    "    print(f\"  max_epochs={config.training.max_epochs}, patience={config.training.patience}, HP_GRID=1 config, n_starts=2\")\n",
    "    print(f\"  r_max={config.vae.r_max:.0e}, training_stride={config.data.training_stride}, compile={config.training.compile_model}\")\n",
    "    print(f\"  Walk-forward: {config.walk_forward.total_years}y total, \"\n",
    "        f\"{config.walk_forward.min_training_years}y min training, \"\n",
    "        f\"{config.walk_forward.holdout_years}y holdout\")\n",
    "    print(f\"  Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Real Data (Production)\n",
    "\n",
    "Run **all cells below** (through \"ASSEMBLE FULL CONFIG\") for full production configuration. Skip Section 2a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA SOURCE \u2014 Real data\n",
    "# ============================================================\n",
    "# For CSV source:\n",
    "DATA_PATH = \"data/stock_data.csv\"  # <-- Set path to your stock data CSV\n",
    "\n",
    "# For Tiingo source: run download first:\n",
    "#   python scripts/download_tiingo.py --phase all --keys-file keys.txt\n",
    "# Then set DATA_SOURCE = \"tiingo\" in Global cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pipeline Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `n_stocks` | Universe size \u2014 how many stocks to keep (ranked by market cap). More = better diversification, slower training. |\n",
    "| `window_length` | Length of each input window in trading days. 504 \u2248 2 years of daily data. Each stock produces many overlapping windows. |\n",
    "| `n_features` | Features per timestep. 2 = log-return + realized volatility. |\n",
    "| `vol_window` | Lookback (trading days) for trailing volatility computation. 252 \u2248 1 year. |\n",
    "| `vix_lookback_percentile` | VIX percentile above which a day is labeled \"crisis\". Higher = fewer crisis days = less overweighting. |\n",
    "| `min_valid_fraction` | Minimum fraction of non-missing data to keep a stock. 0.80 = stocks missing >20% of their history are dropped. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA PIPELINE (MOD-001)\n",
    "# ============================================================\n",
    "data_cfg = DataPipelineConfig(\n",
    "    n_stocks=N_STOCKS,           # universe cap (same as N_STOCKS above)\n",
    "    window_length=T,             # T: sliding window length (trading days)\n",
    "    n_features=N_FEATURES,       # F: features per timestep (return + realized vol)\n",
    "    vol_window=252,              # trailing vol lookback (days)\n",
    "    vix_lookback_percentile=80.0,# VIX percentile for crisis threshold\n",
    "    min_valid_fraction=0.80,     # Drop stocks missing >20% of their price history\n",
    "    training_stride=TRAINING_STRIDE,  # Window stride for training (21 = monthly, 1 = daily; inference always uses stride=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder depth L=5, Final width C_L=384, D=1008\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# VAE ARCHITECTURE (MOD-002)\n",
    "# ============================================================\n",
    "vae_cfg = VAEArchitectureConfig(\n",
    "    K=K,                                   # Max latent factors the VAE can discover (only \"active units\" AU<=K are used)\n",
    "    sigma_sq_init=1.0,                     # Initial observation noise (1.0 = assume noise = signal, model learns true value)\n",
    "    sigma_sq_min=1e-4,                     # Lower clamp for sigma^2 (prevents overfitting: model can't claim 0 noise)\n",
    "    sigma_sq_max=10.0,                     # Upper clamp for sigma^2 (prevents divergence: model can't give up entirely)\n",
    "    window_length=data_cfg.window_length,  # Must match data_cfg.window_length\n",
    "    n_features=data_cfg.n_features,        # Must match data_cfg.n_features\n",
    "    r_max=5.0,                             # Max model params / data points ratio (safety guard against overfitting)\n",
    "    dropout=DROPOUT,                           # Dropout rate for VAE residual blocks (0.1=standard, 0.2=reinforced for small universes)\n",
    ")\n",
    "\n",
    "print(f\"Encoder depth L={vae_cfg.encoder_depth}, \"\n",
    "      f\"Final width C_L={vae_cfg.final_layer_width}, \"\n",
    "      f\"D={vae_cfg.D}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOSS FUNCTION (MOD-004)\n",
    "# ============================================================\n",
    "loss_cfg = LossConfig(\n",
    "    mode=LOSS_MODE,                        # Loss mode: \"P\"=full ELBO with learned sigma^2, \"F\"=simplified with beta warmup, \"A\"=hybrid\n",
    "    gamma=CRISIS_OVERWEIGHTING,            # Crisis overweighting: crisis windows count 3x more in reconstruction loss\n",
    "    lambda_co_max=MAX_CO_MOVEMENT_WEIGHT,  # Max co-movement weight: how strongly latent distances must match Spearman correlations\n",
    "    beta_fixed=1.0,                        # Fixed KL weight for Mode A (must be 1.0 for Mode P)\n",
    "    warmup_fraction=0.20,                  # Mode F only: fraction of epochs to ramp beta 0->1 (prevents posterior collapse)\n",
    "    max_pairs=2048,                        # Max stock pairs per batch for co-movement loss (limits O(B^2) compute)\n",
    "    delta_sync=21,                         # Max date gap (days) for windows to be \"synchronized\" (21 \u2248 1 month)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING (MOD-005)\n",
    "# ============================================================\n",
    "training_cfg = TrainingConfig(\n",
    "    max_epochs=MAX_EPOCHS,            # Hard limit on training duration (early stopping usually triggers sooner)\n",
    "    batch_size=BATCH_SIZE,            # Windows per gradient step (larger = smoother but more memory)\n",
    "    learning_rate=LEARNING_RATE,      # Initial optimizer step size (too high = diverges, too low = slow)\n",
    "    weight_decay=1e-3,                # L2 penalty on weights to reduce overfitting (1e-5 = mild)\n",
    "    adam_betas=(0.9, 0.999),          # Adam momentum parameters (rarely need tuning)\n",
    "    adam_eps=1e-8,                    # Adam numerical stability constant (rarely need tuning)\n",
    "    patience=EARLY_STOPPING_PATIENCE, # Early stopping: stop after N epochs without sufficient improvement\n",
    "    es_min_delta=ES_MIN_DELTA,        # Min ELBO drop to count as improvement (higher = stops sooner)\n",
    "    lr_patience=LR_PATIENCE,          # Reduce LR if validation stagnates for N epochs (triggers before early stop)\n",
    "    lr_factor=LR_FACTOR,              # LR multiplier when reducing (0.75 = reduce by 25% each time)\n",
    "    n_strata=15,                      # Volatility groups for stratified batching (ensures risk diversity per batch)\n",
    "    curriculum_phase1_frac=0.30,      # Phase 1: 30% of epochs with full co-movement + synchronized batching\n",
    "    curriculum_phase2_frac=0.30,      # Phase 2: 30% of epochs with co-movement linearly decaying to 0\n",
    "                                      # Phase 3: remaining 40% with no co-movement, random batching\n",
    "    compile_model=COMPILE_MODEL,      # torch.compile on CUDA/MPS (faster forward/backward, ~30s warmup)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INFERENCE (MOD-006)\n",
    "# After training, the encoder runs on all windows to extract each stock's\n",
    "# latent profile. These parameters control that extraction pass and\n",
    "# decide how many latent dimensions are kept as \"active\" risk factors.\n",
    "# ============================================================\n",
    "inference_cfg = InferenceConfig(\n",
    "    batch_size=512,                          # Larger than training (no gradients = less memory needed)\n",
    "    au_threshold=AU_THRESHOLD,               # KL > 0.01 nats = dimension is \"active\" (below = unused, posterior \u2248 prior)\n",
    "    r_min=2,                                 # Min data/param ratio \u2014 caps AU_max to prevent more factors than data supports\n",
    "    aggregation_method=AGGREGATION_METHOD,   # Average overlapping window predictions for each stock's final profile\n",
    "    aggregation_half_life=AGGREGATION_HALF_LIFE,  # Exponential decay: 0=uniform, 60=recent windows weighted (~5yr half-life)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RISK MODEL (MOD-007)\n",
    "# Transforms latent exposures (B matrix) into a full covariance\n",
    "# matrix Sigma_assets via cross-sectional regression and\n",
    "# Ledoit-Wolf shrinkage. These parameters handle numerical\n",
    "# edge cases (extreme vol ratios, ill-conditioned matrices).\n",
    "# ============================================================\n",
    "risk_model_cfg = RiskModelConfig(\n",
    "    winsorize_lo=5.0,            # Clip extreme vol ratios below 5th percentile (removes outliers)\n",
    "    winsorize_hi=95.0,           # Clip extreme vol ratios above 95th percentile\n",
    "    d_eps_floor=1e-6,            # Min idiosyncratic variance per stock (prevents division by zero)\n",
    "    conditioning_threshold=1e6,  # Switch to ridge regression if covariance condition number exceeds this\n",
    "    ridge_scale=1e-6,            # Ridge regularization strength (tiny, just enough to stabilize)\n",
    "    sigma_z_eigenvalue_pct=SIGMA_Z_EIGENVALUE_PCT,  # Keep top eigenvalues explaining X% of Sigma_z variance (1.0=all)\n",
    "    b_a_shrinkage_alpha=B_A_SHRINKAGE_ALPHA,  # Shrink B_A towards zero (0.0=off, 0.1=10% shrinkage)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PORTFOLIO OPTIMIZATION (MOD-008)\n",
    "# Constraints identical for VAE and all 6 benchmarks (INV-012)\n",
    "#\n",
    "# Variance (w'\u03a3w) = total portfolio risk. Minimizing it concentrates\n",
    "# weight on low-vol stocks \u2014 but can put 85% of risk on 2-3 latent\n",
    "# factors without noticing (fragile to factor shocks).\n",
    "#\n",
    "# Entropy H(w) = Shannon entropy on each factor's risk contribution.\n",
    "# Maximizing it spreads risk evenly across all active factors \u2014 no\n",
    "# single factor dominates (resilient to individual factor crashes).\n",
    "#\n",
    "# lambda_risk and alpha control the tradeoff: more lambda_risk favors\n",
    "# low total variance; more alpha favors even factor risk distribution.\n",
    "#\n",
    "# lambda_risk guide:\n",
    "#   0.1  \u2192 entropy dominates (pure factor risk parity, higher variance OK)\n",
    "#   0.5  \u2192 entropy favored, soft variance constraint\n",
    "#   1.0  \u2192 balanced (default, let alpha_grid find the right ratio)\n",
    "#   2.0  \u2192 variance favored (more conservative, fewer dominant factors OK)\n",
    "#   5.0+ \u2192 near min-variance (entropy has little influence)\n",
    "#\n",
    "# Penalty tuning guide (defaults calibrated for US mid/large cap):\n",
    "#   Illiquid universe (small caps) \u2192 raise kappa_1 & kappa_2\n",
    "#   Infrequent rebalancing         \u2192 lower kappa_1 & kappa_2\n",
    "#   Allow more concentrated bets   \u2192 lower phi and/or raise w_bar\n",
    "#   Force tighter diversification  \u2192 raise phi and/or lower w_bar\n",
    "#   Smoother trades                \u2192 lower delta_bar (e.g. 0.005)\n",
    "#\n",
    "# Valid ranges from DVT spec:\n",
    "#   phi: [5, 100]  kappa_1: [0.01, 1.0]  kappa_2: [1, 50]  delta_bar: [0.005, 0.03]\n",
    "# ============================================================\n",
    "portfolio_cfg = PortfolioConfig(\n",
    "    lambda_risk=LAMBDA_RISK,     # Risk aversion (higher = more conservative, lower variance portfolio)\n",
    "    w_max=W_MAX,                 # Hard cap per stock (CVXPY constraint: w_i <= w_max)\n",
    "    w_min=W_MIN,                 # Min active weight: below this, stock is eliminated (0 or >= 0.1%)\n",
    "    w_bar=W_BAR,                 # Soft cap: concentration penalty kicks in above this weight\n",
    "    phi=PHI,                     # Concentration penalty strength above w_bar (phi * sum(max(0, w_i - w_bar)^2))\n",
    "    kappa_1=KAPPA_1,             # Linear turnover penalty (penalizes trading costs at rebalance)\n",
    "    kappa_2=KAPPA_2,             # Quadratic turnover penalty (penalizes large trades more than small ones)\n",
    "    delta_bar=DELTA_BAR,         # Turnover below 1% is not penalized (de minimis threshold)\n",
    "    tau_max=MAX_TURNOVER,        # Max 30% portfolio change per rebalance (hard cap on turnover)\n",
    "    n_starts=SCA_N_STARTS,       # Multi-start optimizations (more = better optimum, proportionally slower)\n",
    "    sca_max_iter=100,            # Max iterations for the SCA convex optimizer\n",
    "    sca_tol=1e-8,                # SCA convergence tolerance (stop when improvement < this)\n",
    "    armijo_c=1e-4,               # Line search: sufficient decrease constant\n",
    "    armijo_rho=0.5,              # Line search: backtracking factor (halve step on each retry)\n",
    "    armijo_max_iter=20,          # Line search: max backtracking attempts per SCA step\n",
    "    max_cardinality_elim=100,    # Max rounds of stock elimination to enforce w_min constraint\n",
    "    entropy_eps=1e-30,           # Tiny constant in log() for numerical safety (avoids log(0))\n",
    "    alpha_grid=ALPHA_GRID,       # Risk-entropy tradeoff grid (optimizer picks best alpha)\n",
    "    normalize_entropy_gradient=NORMALIZE_ENTROPY_GRADIENT,  # Balance entropy/risk gradient scales\n",
    "    entropy_budget_mode=ENTROPY_BUDGET_MODE,                # \"proportional\" or \"uniform\" entropy target\n",
    "    momentum_enabled=MOMENTUM_ENABLED,    # Toggle momentum signal (True/False)\n",
    "    momentum_lookback=MOMENTUM_LOOKBACK,  # 12-month lookback (trading days)\n",
    "    momentum_skip=MOMENTUM_SKIP,          # Skip last month (reversal)\n",
    "    momentum_weight=MOMENTUM_WEIGHT,      # Scaling factor for momentum signal\n",
    "    # OOS Rebalancing (DVT \u00a74.2)\n",
    "    rebalancing_frequency_days=REBALANCING_FREQUENCY,  # Days between rebalancing (0=buy-hold)\n",
    "    entropy_trigger_alpha=ENTROPY_TRIGGER_ALPHA,       # Exceptional trigger threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# WALK-FORWARD VALIDATION (MOD-009)\n",
    "# ============================================================\n",
    "walk_forward_cfg = WalkForwardConfig(\n",
    "    total_years=N_YEARS,            # Total history used (more years = more folds = more robust evaluation)\n",
    "    min_training_years=MIN_N_YEARS, # First fold trains on at least MIN_N_YEARS (shorter = unreliable model)\n",
    "    oos_months=6,                   # Test each fold on 6 months out-of-sample, then slide forward\n",
    "    embargo_days=21,                # 21-day gap between training and test (prevents data leakage from windows)\n",
    "    holdout_years=3,                # Last 3 years excluded from all folds (final out-of-sample validation)\n",
    "    val_years=2,                    # 2-year nested validation within training for HP selection (Phase A)\n",
    "    score_lambda_pen=5.0,           # HP scoring: penalty weight for max drawdown (higher = favor low-drawdown configs)\n",
    "    score_lambda_est=2.0,           # HP scoring: penalty weight for poor risk estimation accuracy\n",
    "    score_mdd_threshold=0.20,       # HP scoring: drawdowns above 20% get heavily penalized\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineConfig assembled.\n",
      "  Walk-forward: 40y total, 10y min training, 3y holdout\n",
      "  VAE: K=75, T=504, F=2\n",
      "  Training: 800 max epochs, bs=512, lr=0.001\n",
      "  Loss mode: P, gamma=3.0\n",
      "  Capacity guard r_max: 5.0\n",
      "  Device: mps\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ASSEMBLE FULL CONFIG\n",
    "# ============================================================\n",
    "if DATA_SOURCE != \"synthetic\" and QUICK_MODE == False:\n",
    "      config = PipelineConfig(\n",
    "            data=data_cfg,\n",
    "            vae=vae_cfg,\n",
    "            loss=loss_cfg,\n",
    "            training=training_cfg,\n",
    "            inference=inference_cfg,\n",
    "            risk_model=risk_model_cfg,\n",
    "            portfolio=portfolio_cfg,\n",
    "            walk_forward=walk_forward_cfg,\n",
    "            seed=SEED,\n",
    "      )\n",
    "\n",
    "      print(\"PipelineConfig assembled.\")\n",
    "      print(f\"  Walk-forward: {config.walk_forward.total_years}y total, \"\n",
    "            f\"{config.walk_forward.min_training_years}y min training, \"\n",
    "            f\"{config.walk_forward.holdout_years}y holdout\")\n",
    "      print(f\"  VAE: K={config.vae.K}, T={config.vae.window_length}, F={config.vae.n_features}\")\n",
    "      print(f\"  Training: {config.training.max_epochs} max epochs, \"\n",
    "            f\"bs={config.training.batch_size}, lr={config.training.learning_rate}\")\n",
    "      print(f\"  Loss mode: {config.loss.mode}, gamma={config.loss.gamma}\")\n",
    "      print(f\"  Capacity guard r_max: {config.vae.r_max}\")\n",
    "      print(f\"  Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Tiingo Download (run once)\n",
    "\n",
    "Run this cell **only once** to download Tiingo data. After the first run, the data is saved locally and reused automatically.\n",
    "Set `MAX_TICKERS` to a small number (e.g. 5) for testing, or `None` for the full universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-21 19:47:20,754 [INFO] download_tiingo: Validating 33 API key(s)...\n",
      "2026-02-21 19:47:21,141 [INFO] download_tiingo: Key #1 valid (masked: ...77e2)\n",
      "2026-02-21 19:47:21,458 [INFO] download_tiingo: Key #2 valid (masked: ...1a26)\n",
      "2026-02-21 19:47:21,772 [INFO] download_tiingo: Key #3 valid (masked: ...0fc5)\n",
      "2026-02-21 19:47:22,079 [INFO] download_tiingo: Key #4 valid (masked: ...e157)\n",
      "2026-02-21 19:47:22,387 [INFO] download_tiingo: Key #5 valid (masked: ...2424)\n",
      "2026-02-21 19:47:22,704 [INFO] download_tiingo: Key #6 valid (masked: ...eb4a)\n",
      "2026-02-21 19:47:23,032 [INFO] download_tiingo: Key #7 valid (masked: ...c934)\n",
      "2026-02-21 19:47:23,332 [INFO] download_tiingo: Key #8 valid (masked: ...7c75)\n",
      "2026-02-21 19:47:23,653 [INFO] download_tiingo: Key #9 valid (masked: ...ffdd)\n",
      "2026-02-21 19:47:23,951 [INFO] download_tiingo: Key #10 valid (masked: ...9305)\n",
      "2026-02-21 19:47:24,277 [INFO] download_tiingo: Key #11 valid (masked: ...4561)\n",
      "2026-02-21 19:47:24,600 [INFO] download_tiingo: Key #12 valid (masked: ...7f78)\n",
      "2026-02-21 19:47:24,911 [INFO] download_tiingo: Key #13 valid (masked: ...d5f8)\n",
      "2026-02-21 19:47:25,222 [INFO] download_tiingo: Key #14 valid (masked: ...ce8f)\n",
      "2026-02-21 19:47:25,527 [INFO] download_tiingo: Key #15 valid (masked: ...19d0)\n",
      "2026-02-21 19:47:25,854 [INFO] download_tiingo: Key #16 valid (masked: ...4aa8)\n",
      "2026-02-21 19:47:26,173 [INFO] download_tiingo: Key #17 valid (masked: ...a038)\n",
      "2026-02-21 19:47:26,474 [INFO] download_tiingo: Key #18 valid (masked: ...5a74)\n",
      "2026-02-21 19:47:26,797 [INFO] download_tiingo: Key #19 valid (masked: ...f6ba)\n",
      "2026-02-21 19:47:27,112 [INFO] download_tiingo: Key #20 valid (masked: ...3cda)\n",
      "2026-02-21 19:47:27,454 [INFO] download_tiingo: Key #21 valid (masked: ...38f4)\n",
      "2026-02-21 19:47:27,846 [INFO] download_tiingo: Key #22 valid (masked: ...3e65)\n",
      "2026-02-21 19:47:28,162 [INFO] download_tiingo: Key #23 valid (masked: ...d471)\n",
      "2026-02-21 19:47:28,488 [INFO] download_tiingo: Key #24 valid (masked: ...a88b)\n",
      "2026-02-21 19:47:28,893 [INFO] download_tiingo: Key #25 valid (masked: ...97d0)\n",
      "2026-02-21 19:47:29,198 [INFO] download_tiingo: Key #26 valid (masked: ...b2ac)\n",
      "2026-02-21 19:47:29,526 [INFO] download_tiingo: Key #27 valid (masked: ...202f)\n",
      "2026-02-21 19:47:29,849 [INFO] download_tiingo: Key #28 valid (masked: ...40b8)\n",
      "2026-02-21 19:47:30,152 [INFO] download_tiingo: Key #29 valid (masked: ...4d35)\n",
      "2026-02-21 19:47:30,468 [INFO] download_tiingo: Key #30 valid (masked: ...ec26)\n",
      "2026-02-21 19:47:30,774 [INFO] download_tiingo: Key #31 valid (masked: ...bcce)\n",
      "2026-02-21 19:47:31,109 [INFO] download_tiingo: Key #32 valid (masked: ...2105)\n",
      "2026-02-21 19:47:31,517 [INFO] download_tiingo: Key #33 valid (masked: ...9b39)\n",
      "2026-02-21 19:47:31,520 [INFO] download_tiingo: 33/33 keys valid.\n",
      "2026-02-21 19:47:31,521 [INFO] download_tiingo: === Phase 1: Ticker Discovery ===\n",
      "2026-02-21 19:47:31,522 [INFO] download_tiingo: Downloading supported tickers list from Tiingo...\n",
      "2026-02-21 19:47:32,236 [INFO] download_tiingo: Fetching S&P 500 constituent list from Wikipedia...\n",
      "2026-02-21 19:47:32,418 [INFO] download_tiingo: Found 503 S&P 500 tickers from Wikipedia\n",
      "2026-02-21 19:47:32,423 [INFO] download_tiingo: SP500 priority: 519 SP500 tickers first, then 22265 others\n",
      "2026-02-21 19:47:32,445 [INFO] download_tiingo: Saved 22784 US equity tickers (6899 active, 15885 delisted) to data/tiingo_meta/supported_tickers.csv\n",
      "2026-02-21 19:47:32,462 [INFO] download_tiingo: Assigned permnos to 22784 tickers\n",
      "2026-02-21 19:47:32,462 [INFO] download_tiingo: Found 22784 US equity tickers\n",
      "2026-02-21 19:47:32,462 [INFO] download_tiingo: === Phase 2: Price Download (max_tickers=None) ===\n",
      "2026-02-21 19:47:32,473 [INFO] download_tiingo: Tickers: 13333 to download, 8788 to update, 663 already complete\n",
      "2026-02-21 19:47:32,473 [INFO] download_tiingo: Rate limit budget: 33 keys \u00d7 45 requests/key = 1485 tickers per cycle\n",
      "2026-02-21 19:47:39,183 [INFO] download_tiingo: Progress: 10/22121 tickers processed (GSJ) [33 keys available]\n",
      "2026-02-21 19:47:46,005 [INFO] download_tiingo: Progress: 20/22121 tickers processed (GSL-P-B) [33 keys available]\n",
      "2026-02-21 19:47:59,587 [INFO] download_tiingo: Progress: 40/22121 tickers processed (GSRMR) [33 keys available]\n",
      "2026-02-21 19:48:07,423 [INFO] download_tiingo: Progress: 50/22121 tickers processed (GSTRF) [33 keys available]\n",
      "2026-02-21 19:48:15,591 [INFO] download_tiingo: Progress: 60/22121 tickers processed (GTACW) [33 keys available]\n",
      "2026-02-21 19:48:22,908 [INFO] download_tiingo: Progress: 70/22121 tickers processed (GTENU) [33 keys available]\n",
      "2026-02-21 19:48:39,336 [INFO] download_tiingo: Progress: 90/22121 tickers processed (GTN-A) [33 keys available]\n",
      "2026-02-21 19:48:45,902 [INFO] download_tiingo: Progress: 100/22121 tickers processed (GTPBW) [33 keys available]\n",
      "2026-02-21 19:48:54,054 [INFO] download_tiingo: Progress: 110/22121 tickers processed (GTYHU) [33 keys available]\n",
      "2026-02-21 19:49:01,925 [INFO] download_tiingo: Progress: 120/22121 tickers processed (GUT-P-A) [33 keys available]\n",
      "2026-02-21 19:49:08,676 [INFO] download_tiingo: Progress: 130/22121 tickers processed (GVA) [33 keys available]\n",
      "2026-02-21 19:49:14,900 [INFO] download_tiingo: Progress: 140/22121 tickers processed (GWACU) [33 keys available]\n",
      "2026-02-21 19:49:21,929 [INFO] download_tiingo: Progress: 150/22121 tickers processed (GWIIW) [33 keys available]\n",
      "2026-02-21 19:49:36,451 [INFO] download_tiingo: Progress: 170/22121 tickers processed (GXP) [33 keys available]\n",
      "2026-02-21 19:50:00,532 [INFO] download_tiingo: Progress: 200/22121 tickers processed (HAMPQ) [33 keys available]\n",
      "2026-02-21 19:50:05,948 [INFO] download_tiingo: Progress: 210/22121 tickers processed (HART) [33 keys available]\n",
      "2026-02-21 19:50:20,983 [INFO] download_tiingo: Progress: 230/22121 tickers processed (HBANP) [33 keys available]\n",
      "2026-02-21 19:50:28,780 [INFO] download_tiingo: Progress: 240/22121 tickers processed (HBI) [33 keys available]\n",
      "2026-02-21 19:50:43,332 [INFO] download_tiingo: Progress: 260/22121 tickers processed (HCAC) [33 keys available]\n",
      "2026-02-21 19:50:49,183 [INFO] download_tiingo: Progress: 270/22121 tickers processed (HCAPZ) [33 keys available]\n",
      "2026-02-21 19:50:56,718 [INFO] download_tiingo: Progress: 280/22121 tickers processed (HCC) [33 keys available]\n",
      "2026-02-21 19:51:08,479 [INFO] download_tiingo: Progress: 300/22121 tickers processed (HCICU) [33 keys available]\n",
      "2026-02-21 19:51:15,056 [INFO] download_tiingo: Progress: 310/22121 tickers processed (HCLP) [33 keys available]\n",
      "2026-02-21 19:51:29,142 [INFO] download_tiingo: Progress: 330/22121 tickers processed (HCTI) [33 keys available]\n",
      "2026-02-21 19:51:41,341 [INFO] download_tiingo: Progress: 350/22121 tickers processed (HDRAW) [33 keys available]\n",
      "2026-02-21 19:51:47,936 [INFO] download_tiingo: Progress: 360/22121 tickers processed (HEAT) [33 keys available]\n",
      "2026-02-21 19:51:54,624 [INFO] download_tiingo: Progress: 370/22121 tickers processed (HEI-A) [33 keys available]\n",
      "2026-02-21 19:52:00,004 [INFO] download_tiingo: Progress: 380/22121 tickers processed (HEMI) [33 keys available]\n",
      "2026-02-21 19:52:07,980 [INFO] download_tiingo: Progress: 390/22121 tickers processed (HEROQ) [33 keys available]\n",
      "2026-02-21 19:52:14,540 [INFO] download_tiingo: Progress: 400/22121 tickers processed (HF) [33 keys available]\n",
      "2026-02-21 19:52:21,219 [INFO] download_tiingo: Progress: 410/22121 tickers processed (HFRO-P-A) [33 keys available]\n",
      "2026-02-21 19:52:28,566 [INFO] download_tiingo: Progress: 420/22121 tickers processed (HGH) [33 keys available]\n",
      "2026-02-21 19:52:42,201 [INFO] download_tiingo: Progress: 440/22121 tickers processed (HHGCU) [33 keys available]\n",
      "2026-02-21 19:52:57,672 [INFO] download_tiingo: Progress: 460/22121 tickers processed (HIFS) [33 keys available]\n",
      "2026-02-21 19:53:04,784 [INFO] download_tiingo: Progress: 470/22121 tickers processed (HIHO) [33 keys available]\n",
      "2026-02-21 19:53:13,266 [INFO] download_tiingo: Progress: 480/22121 tickers processed (HIND) [33 keys available]\n",
      "2026-02-21 19:53:20,612 [INFO] download_tiingo: Progress: 490/22121 tickers processed (HITK) [33 keys available]\n",
      "2026-02-21 19:53:28,181 [INFO] download_tiingo: Progress: 500/22121 tickers processed (HJV) [33 keys available]\n",
      "2026-02-21 19:53:36,061 [INFO] download_tiingo: Progress: 510/22121 tickers processed (HL) [33 keys available]\n",
      "2026-02-21 19:53:52,891 [INFO] download_tiingo: Progress: 530/22121 tickers processed (HLMNW) [33 keys available]\n",
      "2026-02-21 19:54:00,993 [INFO] download_tiingo: Progress: 540/22121 tickers processed (HLVX) [33 keys available]\n",
      "2026-02-21 19:54:08,001 [INFO] download_tiingo: Progress: 550/22121 tickers processed (HMACR) [33 keys available]\n",
      "2026-02-21 19:54:15,146 [INFO] download_tiingo: Progress: 560/22121 tickers processed (HMCOU) [33 keys available]\n",
      "2026-02-21 19:54:39,981 [INFO] download_tiingo: Progress: 590/22121 tickers processed (HNBC) [33 keys available]\n",
      "2026-02-21 19:55:07,055 [INFO] download_tiingo: Progress: 620/22121 tickers processed (HOLOW) [33 keys available]\n",
      "2026-02-21 19:55:20,966 [INFO] download_tiingo: Progress: 640/22121 tickers processed (HORIW) [33 keys available]\n",
      "2026-02-21 19:55:37,315 [INFO] download_tiingo: Progress: 660/22121 tickers processed (HOWL) [33 keys available]\n",
      "2026-02-21 19:55:52,329 [INFO] download_tiingo: Progress: 680/22121 tickers processed (HPLTW) [33 keys available]\n",
      "2026-02-21 19:56:00,286 [INFO] download_tiingo: Progress: 690/22121 tickers processed (HPX) [33 keys available]\n",
      "2026-02-21 19:56:09,222 [INFO] download_tiingo: Progress: 700/22121 tickers processed (HR) [33 keys available]\n",
      "2026-02-21 19:56:17,606 [INFO] download_tiingo: Progress: 710/22121 tickers processed (HRG) [33 keys available]\n",
      "2026-02-21 19:56:25,527 [INFO] download_tiingo: Progress: 720/22121 tickers processed (HROWL) [33 keys available]\n",
      "2026-02-21 19:56:39,746 [INFO] download_tiingo: Progress: 740/22121 tickers processed (HSBC) [33 keys available]\n",
      "2026-02-21 19:56:55,273 [INFO] download_tiingo: Progress: 760/22121 tickers processed (HSKA) [33 keys available]\n",
      "2026-02-21 19:57:02,487 [INFO] download_tiingo: Progress: 770/22121 tickers processed (HSPTU) [33 keys available]\n",
      "2026-02-21 19:57:11,577 [INFO] download_tiingo: Progress: 780/22121 tickers processed (HTAQ) [33 keys available]\n",
      "2026-02-21 19:57:26,542 [INFO] download_tiingo: Progress: 800/22121 tickers processed (HTGX-CL) [33 keys available]\n",
      "2026-02-21 19:57:34,724 [INFO] download_tiingo: Progress: 810/22121 tickers processed (HTLF) [33 keys available]\n",
      "2026-02-21 19:57:43,460 [INFO] download_tiingo: Progress: 820/22121 tickers processed (HTPA-U) [33 keys available]\n",
      "2026-02-21 19:57:50,291 [INFO] download_tiingo: Progress: 830/22121 tickers processed (HTZWW) [33 keys available]\n",
      "2026-02-21 19:57:58,238 [INFO] download_tiingo: Progress: 840/22121 tickers processed (HUDAR) [33 keys available]\n",
      "2026-02-21 19:58:04,777 [INFO] download_tiingo: Progress: 850/22121 tickers processed (HUIZ) [33 keys available]\n",
      "2026-02-21 19:58:13,491 [INFO] download_tiingo: Progress: 860/22121 tickers processed (HURN) [33 keys available]\n",
      "2026-02-21 19:58:21,720 [INFO] download_tiingo: Progress: 870/22121 tickers processed (HVII) [33 keys available]\n",
      "2026-02-21 19:58:29,794 [INFO] download_tiingo: Progress: 880/22121 tickers processed (HWAY) [33 keys available]\n",
      "2026-02-21 19:58:38,421 [INFO] download_tiingo: Progress: 890/22121 tickers processed (HWELU) [33 keys available]\n",
      "2026-02-21 19:58:52,705 [INFO] download_tiingo: Progress: 910/22121 tickers processed (HXL) [33 keys available]\n",
      "2026-02-21 19:59:16,458 [INFO] download_tiingo: Progress: 940/22121 tickers processed (HYNE) [33 keys available]\n",
      "2026-02-21 19:59:22,897 [INFO] download_tiingo: Progress: 950/22121 tickers processed (HYZN) [33 keys available]\n",
      "2026-02-21 19:59:31,282 [INFO] download_tiingo: Progress: 960/22121 tickers processed (HZON-U) [33 keys available]\n",
      "2026-02-21 19:59:38,898 [INFO] download_tiingo: Progress: 970/22121 tickers processed (IACA) [33 keys available]\n",
      "2026-02-21 19:59:45,237 [INFO] download_tiingo: Progress: 980/22121 tickers processed (IACOU) [33 keys available]\n",
      "2026-02-21 19:59:51,222 [INFO] download_tiingo: Progress: 990/22121 tickers processed (IAMXU) [33 keys available]\n",
      "2026-02-21 20:00:05,737 [INFO] download_tiingo: Progress: 1010/22121 tickers processed (IBER-WS) [33 keys available]\n",
      "2026-02-21 20:00:13,505 [INFO] download_tiingo: Progress: 1020/22121 tickers processed (IBKCO) [33 keys available]\n",
      "2026-02-21 20:00:25,825 [INFO] download_tiingo: Progress: 1040/22121 tickers processed (IBP) [33 keys available]\n",
      "2026-02-21 20:00:41,868 [INFO] download_tiingo: Progress: 1060/22121 tickers processed (ICFI) [33 keys available]\n",
      "2026-02-21 20:00:47,569 [INFO] download_tiingo: Progress: 1070/22121 tickers processed (ICHR) [33 keys available]\n",
      "2026-02-21 20:00:54,309 [INFO] download_tiingo: Progress: 1080/22121 tickers processed (ICNC) [33 keys available]\n",
      "2026-02-21 20:01:13,611 [INFO] download_tiingo: Progress: 1110/22121 tickers processed (IDAI) [33 keys available]\n",
      "2026-02-21 20:01:33,648 [INFO] download_tiingo: Progress: 1140/22121 tickers processed (IDYN) [33 keys available]\n",
      "2026-02-21 20:01:52,497 [INFO] download_tiingo: Progress: 1170/22121 tickers processed (IFLR) [33 keys available]\n",
      "2026-02-21 20:01:58,467 [INFO] download_tiingo: Progress: 1180/22121 tickers processed (IFS) [33 keys available]\n",
      "2026-02-21 20:02:04,371 [INFO] download_tiingo: Progress: 1190/22121 tickers processed (IGACU) [33 keys available]\n",
      "2026-02-21 20:02:25,103 [INFO] download_tiingo: Progress: 1220/22121 tickers processed (IGTAW) [33 keys available]\n",
      "2026-02-21 20:02:40,197 [INFO] download_tiingo: Progress: 1240/22121 tickers processed (IIAC-WS) [33 keys available]\n",
      "2026-02-21 20:02:49,025 [INFO] download_tiingo: Progress: 1250/22121 tickers processed (IINN) [33 keys available]\n",
      "2026-02-21 20:02:56,468 [INFO] download_tiingo: Progress: 1260/22121 tickers processed (IKGH) [33 keys available]\n",
      "2026-02-21 20:03:18,143 [INFO] download_tiingo: Progress: 1300/22121 tickers processed (IMAQR) [33 keys available]\n",
      "2026-02-21 20:03:33,047 [INFO] download_tiingo: Progress: 1320/22121 tickers processed (IMGO) [33 keys available]\n",
      "2026-02-21 20:03:40,940 [INFO] download_tiingo: Progress: 1330/22121 tickers processed (IMMP) [33 keys available]\n",
      "2026-02-21 20:03:49,377 [INFO] download_tiingo: Progress: 1340/22121 tickers processed (IMO) [33 keys available]\n",
      "2026-02-21 20:04:03,381 [INFO] download_tiingo: Progress: 1360/22121 tickers processed (IMRX) [33 keys available]\n",
      "2026-02-21 20:04:09,155 [INFO] download_tiingo: Progress: 1370/22121 tickers processed (IMTE) [33 keys available]\n",
      "2026-02-21 20:04:16,046 [INFO] download_tiingo: Progress: 1380/22121 tickers processed (IMVTU) [33 keys available]\n",
      "2026-02-21 20:04:29,069 [INFO] download_tiingo: Progress: 1400/22121 tickers processed (INBS) [33 keys available]\n",
      "2026-02-21 20:04:35,462 [INFO] download_tiingo: Progress: 1410/22121 tickers processed (IND) [33 keys available]\n",
      "2026-02-21 20:04:43,347 [INFO] download_tiingo: Progress: 1420/22121 tickers processed (INDT) [33 keys available]\n",
      "2026-02-21 20:04:56,059 [INFO] download_tiingo: Progress: 1440/22121 tickers processed (INFQ-WS) [33 keys available]\n",
      "2026-02-21 20:05:02,832 [INFO] download_tiingo: Progress: 1450/22121 tickers processed (INGM) [33 keys available]\n",
      "2026-02-21 20:05:05,733 [INFO] download_tiingo: Key #1 reached batch limit (45/45 calls, masked: ...77e2)\n",
      "2026-02-21 20:05:06,126 [INFO] download_tiingo: Key #2 reached batch limit (45/45 calls, masked: ...1a26)\n",
      "2026-02-21 20:05:06,801 [INFO] download_tiingo: Key #3 reached batch limit (45/45 calls, masked: ...0fc5)\n",
      "2026-02-21 20:05:07,492 [INFO] download_tiingo: Key #4 reached batch limit (45/45 calls, masked: ...e157)\n",
      "2026-02-21 20:05:07,969 [INFO] download_tiingo: Key #5 reached batch limit (45/45 calls, masked: ...2424)\n",
      "2026-02-21 20:05:08,661 [INFO] download_tiingo: Key #6 reached batch limit (45/45 calls, masked: ...eb4a)\n",
      "2026-02-21 20:05:09,586 [INFO] download_tiingo: Key #7 reached batch limit (45/45 calls, masked: ...c934)\n",
      "2026-02-21 20:05:10,041 [INFO] download_tiingo: Key #8 reached batch limit (45/45 calls, masked: ...7c75)\n",
      "2026-02-21 20:05:10,439 [INFO] download_tiingo: Key #9 reached batch limit (45/45 calls, masked: ...ffdd)\n",
      "2026-02-21 20:05:11,169 [INFO] download_tiingo: Key #10 reached batch limit (45/45 calls, masked: ...9305)\n",
      "2026-02-21 20:05:12,026 [INFO] download_tiingo: Key #11 reached batch limit (45/45 calls, masked: ...4561)\n",
      "2026-02-21 20:05:12,415 [INFO] download_tiingo: Key #12 reached batch limit (45/45 calls, masked: ...7f78)\n",
      "2026-02-21 20:05:13,162 [INFO] download_tiingo: Key #13 reached batch limit (45/45 calls, masked: ...d5f8)\n",
      "2026-02-21 20:05:13,587 [INFO] download_tiingo: Key #14 reached batch limit (45/45 calls, masked: ...ce8f)\n",
      "2026-02-21 20:05:13,968 [INFO] download_tiingo: Key #15 reached batch limit (45/45 calls, masked: ...19d0)\n",
      "2026-02-21 20:05:14,943 [INFO] download_tiingo: Key #16 reached batch limit (45/45 calls, masked: ...4aa8)\n",
      "2026-02-21 20:05:16,099 [INFO] download_tiingo: Key #17 reached batch limit (45/45 calls, masked: ...a038)\n",
      "2026-02-21 20:05:17,383 [INFO] download_tiingo: Key #18 reached batch limit (45/45 calls, masked: ...5a74)\n",
      "2026-02-21 20:05:17,411 [INFO] download_tiingo: Progress: 1470/22121 tickers processed (INM) [15 keys available]\n",
      "2026-02-21 20:05:18,178 [INFO] download_tiingo: Key #19 reached batch limit (45/45 calls, masked: ...f6ba)\n",
      "2026-02-21 20:05:19,129 [INFO] download_tiingo: Key #20 reached batch limit (45/45 calls, masked: ...3cda)\n",
      "2026-02-21 20:05:20,178 [INFO] download_tiingo: Key #21 reached batch limit (45/45 calls, masked: ...38f4)\n",
      "2026-02-21 20:05:20,651 [INFO] download_tiingo: Key #22 reached batch limit (45/45 calls, masked: ...3e65)\n",
      "2026-02-21 20:05:21,066 [INFO] download_tiingo: Key #23 reached batch limit (45/45 calls, masked: ...d471)\n",
      "2026-02-21 20:05:21,472 [INFO] download_tiingo: Key #24 reached batch limit (45/45 calls, masked: ...a88b)\n",
      "2026-02-21 20:05:21,849 [INFO] download_tiingo: Key #25 reached batch limit (45/45 calls, masked: ...97d0)\n",
      "2026-02-21 20:05:23,053 [INFO] download_tiingo: Key #26 reached batch limit (45/45 calls, masked: ...b2ac)\n",
      "2026-02-21 20:05:23,938 [INFO] download_tiingo: Key #27 reached batch limit (45/45 calls, masked: ...202f)\n",
      "2026-02-21 20:05:24,896 [INFO] download_tiingo: Key #28 reached batch limit (45/45 calls, masked: ...40b8)\n",
      "2026-02-21 20:05:24,931 [INFO] download_tiingo: Progress: 1480/22121 tickers processed (INN-P-C) [5 keys available]\n",
      "2026-02-21 20:05:25,846 [INFO] download_tiingo: Key #29 reached batch limit (45/45 calls, masked: ...4d35)\n",
      "2026-02-21 20:05:26,905 [INFO] download_tiingo: Key #30 reached batch limit (45/45 calls, masked: ...ec26)\n",
      "2026-02-21 20:05:27,859 [INFO] download_tiingo: Key #31 reached batch limit (45/45 calls, masked: ...bcce)\n",
      "2026-02-21 20:05:28,617 [INFO] download_tiingo: Key #32 reached batch limit (45/45 calls, masked: ...2105)\n",
      "2026-02-21 20:05:29,046 [INFO] download_tiingo: Key #33 reached batch limit (45/45 calls, masked: ...9b39)\n",
      "2026-02-21 20:05:29,087 [INFO] download_tiingo: All 33 keys exhausted (1485/22121 done). Re-run to resume from where it stopped.\n",
      "2026-02-21 20:05:29,087 [INFO] download_tiingo: Price download complete: 1485 tickers processed\n",
      "2026-02-21 20:05:29,089 [INFO] download_tiingo: === Phase 3: Merge to Pipeline Format ===\n",
      "2026-02-21 20:05:29,362 [INFO] download_tiingo: Merging 7139 ticker files...\n",
      "2026-02-21 20:05:50,099 [INFO] download_tiingo: Penny stock filter (adj_price < $1.00): removed 824840 rows\n",
      "2026-02-21 20:05:50,529 [INFO] download_tiingo: Min history filter (< 504 days): removed 2038 stocks (372525 rows)\n",
      "2026-02-21 20:05:52,061 [INFO] download_tiingo: Merged 15576321 rows for 4608 tickers (1995-01-03 to 2026-02-20) \u2192 data/tiingo_us_equities.parquet (from 16773686 raw rows, 7139 raw tickers)\n",
      "2026-02-21 20:05:52,135 [INFO] download_tiingo: Done! Data saved to data/\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TIINGO DOWNLOAD \u2014 Run once, data is saved locally\n",
    "# Skip this cell if data is already downloaded or DATA_SOURCE != \"tiingo\"\n",
    "# ============================================================\n",
    "\n",
    "if DATA_SOURCE == \"tiingo\":\n",
    "    import importlib.util\n",
    "\n",
    "    _spec = importlib.util.spec_from_file_location(\n",
    "        \"download_tiingo\", str(PROJECT_ROOT / \"scripts\" / \"download_tiingo.py\")\n",
    "    )\n",
    "    _mod = importlib.util.module_from_spec(_spec)\n",
    "    _spec.loader.exec_module(_mod)  # type: ignore[union-attr]\n",
    "\n",
    "    MAX_TICKERS = None  # Set to None for full universe (~22k tickers)\n",
    "\n",
    "    _mod.run_download(\n",
    "        api_keys=TIINGO_API_KEYS,\n",
    "        data_dir=DATA_DIR,\n",
    "        max_tickers=MAX_TICKERS,\n",
    "        sp500_first=True,  # Download SP500 tickers first (priority)\n",
    "    )\n",
    "else:\n",
    "    print(f\"DATA_SOURCE={DATA_SOURCE}, skipping Tiingo download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source: tiingo\n",
      "Stock data shape: (15576321, 8)\n",
      "Date range: 1995-01-03 00:00:00 to 2026-02-20 00:00:00\n",
      "Unique stocks: 4608\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>date</th>\n",
       "      <th>adj_price</th>\n",
       "      <th>volume</th>\n",
       "      <th>exchange_code</th>\n",
       "      <th>share_code</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>delisting_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10002</td>\n",
       "      <td>2023-08-30</td>\n",
       "      <td>10.865612</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>10.865612</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>24120.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>10.865612</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>16080.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10002</td>\n",
       "      <td>2023-09-05</td>\n",
       "      <td>10.865612</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>12060.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10002</td>\n",
       "      <td>2023-09-06</td>\n",
       "      <td>10.865612</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>9648.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   permno       date  adj_price  volume  exchange_code  share_code  \\\n",
       "0   10002 2023-08-30  10.865612     200              3          10   \n",
       "1   10002 2023-08-31  10.865612       1              3          10   \n",
       "2   10002 2023-09-01  10.865612       0              3          10   \n",
       "3   10002 2023-09-05  10.865612       0              3          10   \n",
       "4   10002 2023-09-06  10.865612       0              3          10   \n",
       "\n",
       "   market_cap  delisting_return  \n",
       "0     48000.0               NaN  \n",
       "1     24120.0               NaN  \n",
       "2     16080.0               NaN  \n",
       "3     12060.0               NaN  \n",
       "4      9648.0               NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "\n",
    "stock_data, start_date = load_data_source(\n",
    "    source=DATA_SOURCE,\n",
    "    data_path=DATA_PATH if DATA_SOURCE == \"csv\" else \"\",\n",
    "    data_dir=DATA_DIR,\n",
    "    n_stocks=N_STOCKS,\n",
    "    n_years=N_YEARS,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Data source: {DATA_SOURCE}\")\n",
    "print(f\"Stock data shape: {stock_data.shape}\")\n",
    "print(f\"Date range: {stock_data['date'].min()} to {stock_data['date'].max()}\")\n",
    "print(f\"Unique stocks: {stock_data['permno'].nunique()}\")\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returns: 7879 dates x 4608 stocks\n",
      "Trailing vol: (7879, 4608) (first 251 rows NaN)\n",
      "Returns date range: 1995-01-03 00:00:00 to 2026-02-20 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Compute log-returns and trailing volatility\n",
    "returns = compute_log_returns(stock_data)\n",
    "trailing_vol = compute_trailing_volatility(returns, window=config.data.vol_window)\n",
    "\n",
    "nb_effective_years = returns.shape[0]/252\n",
    "nb_effective_stocks = returns.shape[1]\n",
    "\n",
    "print(f\"Returns: {returns.shape[0]} dates x {returns.shape[1]} stocks\")\n",
    "print(f\"Trailing vol: {trailing_vol.shape} (first {config.data.vol_window-1} rows NaN)\")\n",
    "print(f\"Returns date range: {returns.index[0]} to {returns.index[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Run Pipeline\n",
    "\n",
    "Executes the full walk-forward validation: Phase A (HP selection) + Phase B (deployment) on each fold, then benchmarks, statistical tests, and report generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Value Interpretation\n",
    "\n",
    "The input windows are **z-scored** (mean=0, std=1), so metric values are directly interpretable as fractions of signal variance.\n",
    "\n",
    "#### `reconstruction` (Step/ and Loss/)\n",
    "Raw per-element MSE **before** D/(2\u03c3\u00b2) scaling. Since data is z-scored, L_recon \u2248 fraction of unexplained variance.\n",
    "\n",
    "| L_recon | RMSE | Interpretation |\n",
    "|---|---|---|\n",
    "| **1.0** | 1.0 | No better than predicting the mean. Not learning. |\n",
    "| **0.5** | 0.71 | ~50% variance explained. Poor. |\n",
    "| **0.1** | 0.32 | ~90% variance explained. Decent. |\n",
    "| **0.05** | 0.22 | ~95% explained. Good for financial data. |\n",
    "| **0.01** | 0.10 | ~99% explained. Likely overfitting on noisy data. |\n",
    "\n",
    "Good converged range on real data: **0.05\u20130.20**.\n",
    "\n",
    "#### `kl_divergence` (Step/ and Loss/)\n",
    "KL **summed over K dimensions**, averaged over batch. Raw value scales linearly with K \u2014 divide by K for per-dimension KL (nats).\n",
    "\n",
    "| L_KL (K=100) | KL/dim | Interpretation |\n",
    "|---|---|---|\n",
    "| **0** | 0 | **Posterior collapse.** Encoder ignores input entirely. |\n",
    "| **1** | 0.01 | Right at AU threshold. Quasi-collapsed. |\n",
    "| **10** | 0.1 | ~10 weakly active dimensions. Most collapsed. |\n",
    "| **50** | 0.5 | ~50 active dims, moderate info per dim. Typical early-mid training. |\n",
    "| **100** | 1.0 | ~100 active dims at ~1 nat each. Healthy, full capacity used. |\n",
    "| **250** | 2.5 | Very informative. If AU \u226a 100, a few dims are dominating. |\n",
    "\n",
    "Good converged range (K=100): **25\u2013150**. For K=200, double these values.\n",
    "\n",
    "#### `co_movement` (Step/ and Loss/)\n",
    "MSE between latent cosine distances and Spearman correlation targets: `(1/|P|) \u00d7 \u03a3 (d_cos(\u03bc_i, \u03bc_j) \u2212 (1 \u2212 \u03c1_ij))\u00b2`.\n",
    "\n",
    "| L_co | Avg error | Interpretation |\n",
    "|---|---|---|\n",
    "| **0.01** | 0.10 | Excellent alignment between latent distances and Spearman. |\n",
    "| **0.05** | 0.22 | Good. Reasonable after Phase 1. |\n",
    "| **0.10** | 0.32 | Moderate. Early training territory. |\n",
    "| **0.30** | 0.55 | Poor. Random initialization level. |\n",
    "| **> 0.5** | > 0.7 | Latent space disorganized w.r.t. co-movements. |\n",
    "\n",
    "**Key: this value is only meaningful when \u03bb_co > 0 (Phases 1-2).** In Phase 3, the value is 0.0 because `compute_co_movement_loss` is skipped entirely.\n",
    "\n",
    "#### `sigma_sq` (Step/ and Loss/)\n",
    "Learned observation noise \u03c3\u00b2 = clamp(exp(log_sigma_sq), 1e-4, 10). **At equilibrium in Mode P, \u03c3\u00b2 converges to \u2248 L_recon** (the model's own reconstruction MSE).\n",
    "\n",
    "| \u03c3\u00b2 | Variance explained | Interpretation |\n",
    "|---|---|---|\n",
    "| **10.0** (clamp max) | ~0% | Model giving up on reconstruction. Diverging. |\n",
    "| **1.0** | ~0% | Noise = signal. Initial state / Mode F (frozen). |\n",
    "| **0.3\u20130.5** | 50\u201370% | Early-mid convergence. |\n",
    "| **0.05\u20130.2** | 80\u201395% | Healthy convergence on financial data. |\n",
    "| **0.01** | ~99% | Possibly overfitting. |\n",
    "| **1e-4** (clamp min) | ~100% | Hitting floor. Overfitting. |\n",
    "\n",
    "Mode F: \u03c3\u00b2 is **frozen at 1.0** \u2014 ignore this metric.\n",
    "\n",
    "#### `Validation/ELBO` (val_elbo)\n",
    "**Primary model selection metric** \u2014 used for early stopping and LR scheduling. Combines reconstruction quality, KL regularization, and observation noise into a single comparable score:\n",
    "\n",
    "`L_val = D/(2\u03c3\u00b2) \u00b7 MSE(\u03b3=1) + (D/2)\u00b7ln(\u03c3\u00b2) + KL`\n",
    "\n",
    "where D = T \u00d7 F = 1008 (for T=504, F=2). **Excludes** crisis weighting (\u03b3=1) and co-movement loss (INV-011), so it is comparable across curriculum phases.\n",
    "\n",
    "Since D scales the first two terms, raw values are large. Typical ranges for D=1008:\n",
    "\n",
    "| val_elbo | Regime | Interpretation |\n",
    "|---|---|---|\n",
    "| **> 1000** | Untrained | Random initialization or diverging. D/2 \u00b7 ln(\u03c3\u00b2=1) \u2248 0 + D/2 \u00b7 MSE(\u22481) \u2248 504. Above 1000 = KL also high. |\n",
    "| **500\u20131000** | Early training | Reconstruction improving, \u03c3\u00b2 still near 1.0. |\n",
    "| **200\u2013500** | Mid training | \u03c3\u00b2 dropping, reconstruction sharpening. Healthy convergence territory. |\n",
    "| **50\u2013200** | Good convergence | Strong reconstruction, well-calibrated \u03c3\u00b2, meaningful latent structure. |\n",
    "| **< 50** | Excellent / overfitting | If AU is healthy (>20), excellent. If AU is very low, may indicate posterior collapse with tight \u03c3\u00b2. |\n",
    "\n",
    "**Key: lower is better.** A plateau or increase triggers LR reduction (after `lr_patience` epochs) and eventually early stopping (after `patience` epochs). The best checkpoint (lowest val_elbo) is restored at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LIVE TENSORBOARD (stable in Colab)\n",
    "# Run this cell BEFORE starting training (Section 4).\n",
    "# The dashboard stays live during training \u2014 no need to re-run.\n",
    "#\n",
    "# --reload_interval 30 : reload every 30s instead of default 5s\n",
    "#   \u2192 this is what prevents the crash after a few minutes\n",
    "# ============================================================\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/ --reload_interval 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_FULL_PIPELINE:\n",
    "      print(\"=\" * 60)\n",
    "      print(\"PIPELINE CONFIGURATION SUMMARY\")\n",
    "      print(\"=\" * 60)\n",
    "      print(f\"  Seed: {SEED} | Device: {DEVICE} | Data: {DATA_SOURCE}\")\n",
    "      if CHECKPOINT_PATH:\n",
    "            print(f\"  Pretrained model: {CHECKPOINT_PATH} (skipping VAE training)\")\n",
    "      print()\n",
    "      print(f\"  [Data]      N_STOCKS={N_STOCKS}, N_effective_stocks={nb_effective_stocks}, T={T}, N_FEATURES={N_FEATURES}, N_YEARS={N_YEARS}, N_effective_years={nb_effective_years}\")\n",
    "      print(f\"  [VAE]       K={K}, LOSS_MODE={LOSS_MODE}\")\n",
    "      print(f\"  [Training]  MAX_EPOCHS={MAX_EPOCHS}, BATCH_SIZE={BATCH_SIZE}, LEARNING_RATE={LEARNING_RATE}, EARLY_STOPPING_PATIENCE={EARLY_STOPPING_PATIENCE}\")\n",
    "      print(f\"              TRAINING_STRIDE={TRAINING_STRIDE}, COMPILE_MODEL={COMPILE_MODEL}\")\n",
    "      print(f\"  [Loss]      CRISIS_OVERWEIGHTING={CRISIS_OVERWEIGHTING}, MAX_CO_MOVEMENT_WEIGHT={MAX_CO_MOVEMENT_WEIGHT}\")\n",
    "      print(f\"  [Portfolio]  LAMBDA_RISK={LAMBDA_RISK}, W_MAX={W_MAX}, W_MIN={W_MIN}, SCA_N_STARTS={SCA_N_STARTS}\")\n",
    "      print(f\"  [HP Grid]   {len(HP_GRID) if HP_GRID else '18 (default)'} configs\")\n",
    "      print(\"=\" * 60)\n",
    "\n",
    "      TB_DIR = \"runs/\"  # TensorBoard log directory (set to None to disable)\n",
    "\n",
    "      pipeline = FullPipeline(config, tensorboard_dir=TB_DIR)\n",
    "\n",
    "      skip_phase_a = True\n",
    "\n",
    "      results = pipeline.run(\n",
    "      stock_data=stock_data,\n",
    "      returns=returns,\n",
    "      trailing_vol=trailing_vol,\n",
    "      skip_phase_a=(DATA_SOURCE == \"synthetic\" or QUICK_MODE == True or skip_phase_a),\n",
    "      vix_data=None,\n",
    "      start_date=start_date,\n",
    "      hp_grid=HP_GRID,\n",
    "      device=DEVICE,\n",
    "      pretrained_model=CHECKPOINT_PATH,\n",
    "      )\n",
    "\n",
    "      print(\"Pipeline complete.\")\n",
    "      print(f\"Folds processed: {len(results['vae_results'])}\")\n",
    "      print(f\"Benchmarks: {list(results['benchmark_results'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4b. Direct Training (Skip Walk-Forward)\n",
    "\n",
    "Train the VAE on the entire period minus a holdout, then evaluate on the holdout.\n",
    "**Run EITHER Section 4 (walk-forward) OR Section 4b (direct) -- not both.**\n",
    "\n",
    "Use this mode for:\n",
    "- Quick iteration during development\n",
    "- Single-split evaluation before committing to full walk-forward\n",
    "- Inspecting model internals (B matrix, latent factors, risk model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RUN DIRECT TRAINING\n",
    "# ============================================================\n",
    "if not RUN_FULL_PIPELINE:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PIPELINE CONFIGURATION SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Seed: {SEED} | Device: {DEVICE} | Data: {DATA_SOURCE}\")\n",
    "    if CHECKPOINT_PATH:\n",
    "        print(f\"  Pretrained model: {CHECKPOINT_PATH} (skipping VAE training)\")\n",
    "    print()\n",
    "    print(f\"  [Data]      N_STOCKS={N_STOCKS}, N_effective_stocks={nb_effective_stocks}, T={T}, N_FEATURES={N_FEATURES}, N_YEARS={N_YEARS}, N_effective_years={nb_effective_years}\")\n",
    "    print(f\"  [VAE]       K={K}, LOSS_MODE={LOSS_MODE}\")\n",
    "    print(f\"  [Training]  MAX_EPOCHS={MAX_EPOCHS}, BATCH_SIZE={BATCH_SIZE}, LEARNING_RATE={LEARNING_RATE}, EARLY_STOPPING_PATIENCE={EARLY_STOPPING_PATIENCE}\")\n",
    "    print(f\"              TRAINING_STRIDE={TRAINING_STRIDE}, COMPILE_MODEL={COMPILE_MODEL}\")\n",
    "    print(f\"  [Loss]      CRISIS_OVERWEIGHTING={CRISIS_OVERWEIGHTING}, MAX_CO_MOVEMENT_WEIGHT={MAX_CO_MOVEMENT_WEIGHT}\")\n",
    "    print(f\"  [Portfolio]  LAMBDA_RISK={LAMBDA_RISK}, W_MAX={W_MAX}, W_MIN={W_MIN}, SCA_N_STARTS={SCA_N_STARTS}\")\n",
    "    print(f\"  [HP Grid]   {len(HP_GRID) if HP_GRID else '18 (default)'} configs\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    HOLDOUT_START = None  # Option A: Set an explicit holdout start date (e.g. \"2023-01-03\") \u2014 set to None for automatic split\n",
    "    HOLDOUT_FRACTION = 0.10  # Option B: Fraction of dates reserved for holdout (ignored if HOLDOUT_START is set)\n",
    "    RUN_BENCHMARKS = True # Run benchmarks on the same split for comparison?\n",
    "\n",
    "    TB_DIR = \"runs/\"\n",
    "\n",
    "    pipeline = FullPipeline(config, tensorboard_dir=TB_DIR)\n",
    "\n",
    "    results = pipeline.run_direct(\n",
    "        stock_data=stock_data,\n",
    "        returns=returns,\n",
    "        trailing_vol=trailing_vol,\n",
    "        vix_data=None,\n",
    "        start_date=start_date,\n",
    "        hp_grid=HP_GRID,\n",
    "        device=DEVICE,\n",
    "        holdout_start=HOLDOUT_START,\n",
    "        holdout_fraction=HOLDOUT_FRACTION,\n",
    "        run_benchmarks=RUN_BENCHMARKS,\n",
    "        pretrained_model=CHECKPOINT_PATH,\n",
    "    )\n",
    "\n",
    "    print(\"Direct training complete.\")\n",
    "    if CHECKPOINT_PATH:\n",
    "        print(f\"  (Loaded pretrained encoder from: {CHECKPOINT_PATH})\")\n",
    "    print(f\"  Train: {results['fold_schedule'][0]['train_start']} to {results['train_end']}\")\n",
    "    print(f\"  Test:  {results['oos_start']} to {results['oos_end']}\")\n",
    "    print(f\"  Sharpe: {results['vae_results'][0].get('sharpe', 0.0):.3f}\")\n",
    "    print(f\"  AU: {results['vae_results'][0].get('AU', 0):.0f}\")\n",
    "    print(f\"  E* (best epoch): {results['vae_results'][0].get('e_star', 0):.0f}\")\n",
    "    if \"checkpoint_path\" in results:\n",
    "        print(f\"  Checkpoint saved: {results['checkpoint_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING HISTORY\n",
    "# ============================================================\n",
    "if (\"state\" in results\n",
    "    and results[\"state\"].get(\"fit_result\") is not None\n",
    "    and \"history\" in results[\"state\"][\"fit_result\"]\n",
    "    and not RUN_FULL_PIPELINE):\n",
    "    history = results[\"state\"][\"fit_result\"][\"history\"]\n",
    "    history_df = pd.DataFrame(history)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "    axes[0, 0].plot(history_df[\"train_loss\"], label=\"Train Loss\")\n",
    "    axes[0, 0].plot(history_df[\"val_elbo\"], label=\"Val ELBO\")\n",
    "    axes[0, 0].set_title(\"Loss Curves\")\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0, 1].plot(history_df[\"train_recon\"], label=\"Reconstruction\")\n",
    "    axes[0, 1].plot(history_df[\"train_kl\"], label=\"KL Divergence\")\n",
    "    axes[0, 1].set_title(\"Loss Components\")\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1, 0].plot(history_df[\"AU\"], color=\"#2563eb\")\n",
    "    axes[1, 0].set_title(\"Active Units (AU)\")\n",
    "    axes[1, 0].set_ylabel(\"AU\")\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1, 1].plot(history_df[\"sigma_sq\"], color=\"#dc2626\")\n",
    "    axes[1, 1].set_title(\"Observation Noise (sigma^2)\")\n",
    "    axes[1, 1].set_ylabel(\"sigma^2\")\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "    fig.suptitle(\"Direct Training History\", fontsize=13, fontweight=\"bold\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "elif not RUN_FULL_PIPELINE:\n",
    "    if CHECKPOINT_PATH:\n",
    "        print(\"Training history not available (loaded from checkpoint, training was skipped).\")\n",
    "    else:\n",
    "        print(\"Training history not available (state bag not populated).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL INSPECTION\n",
    "# ============================================================\n",
    "if \"state\" in results and not(RUN_FULL_PIPELINE):\n",
    "    state = results[\"state\"]\n",
    "    B_A = state.get(\"B_A\")\n",
    "    stock_ids = state.get(\"inferred_stock_ids\", [])\n",
    "    AU = state.get(\"AU\", 0)\n",
    "\n",
    "    if B_A is not None and B_A.shape[1] > 0:\n",
    "        n_show = min(20, B_A.shape[1])\n",
    "        fig, ax = plt.subplots(figsize=(12, max(4, len(stock_ids) * 0.15)))\n",
    "        im = ax.imshow(B_A[:, :n_show], aspect=\"auto\", cmap=\"RdBu_r\")\n",
    "        ax.set_xlabel(f\"Active Latent Dimension (showing {n_show}/{AU})\")\n",
    "        ax.set_ylabel(\"Stock\")\n",
    "        ax.set_title(f\"Exposure Matrix B_A ({B_A.shape[0]} stocks x {AU} active dims)\")\n",
    "        fig.colorbar(im, ax=ax)\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        kl = state.get(\"kl_per_dim\")\n",
    "        if kl is not None:\n",
    "            fig, ax = plt.subplots(figsize=(10, 3))\n",
    "            ax.bar(range(len(kl)), np.sort(kl)[::-1], color=\"#2563eb\", alpha=0.7)\n",
    "            ax.axhline(0.01, color=\"#dc2626\", linestyle=\"--\", label=\"AU threshold (0.01)\")\n",
    "            ax.set_xlabel(\"Dimension (sorted by KL)\")\n",
    "            ax.set_ylabel(\"Marginal KL (nats)\")\n",
    "            ax.set_title(f\"Latent Dimension Usage \u2014 AU={AU}/{len(kl)}\")\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"No state available (run Section 4b first).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Results - Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text summary\n",
    "print(format_summary_table(results[\"report\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployment recommendation\n",
    "deployment = results[\"report\"][\"deployment\"]\n",
    "print(f\"Scenario: {deployment['scenario']}\")\n",
    "print(f\"Recommendation: {deployment['recommendation']}\")\n",
    "print()\n",
    "print(\"Per-benchmark wins (VAE vs benchmark on primary metrics):\")\n",
    "for bench, info in deployment[\"per_benchmark\"].items():\n",
    "    print(f\"  {bench:20s}: {info['wins']}/{info['total']} metrics won\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE summary statistics\n",
    "vae_df = aggregate_fold_metrics(results[\"vae_results\"])\n",
    "vae_summary = summary_statistics(vae_df)\n",
    "print(\"VAE Summary Statistics:\")\n",
    "style_summary_table(vae_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark summary statistics\n",
    "for bench_name, bench_metrics in results[\"benchmark_results\"].items():\n",
    "    bench_df = aggregate_fold_metrics(bench_metrics)\n",
    "    bench_summary = summary_statistics(bench_df)\n",
    "    print(f\"\\n{bench_name} Summary:\")\n",
    "    display(style_summary_table(bench_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Results - Per-Fold Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE per-fold metrics\n",
    "print(\"VAE Per-Fold Metrics:\")\n",
    "style_fold_table(vae_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E* distribution\n",
    "e_star_summary = results[\"report\"][\"e_star_summary\"]\n",
    "print(f\"E* epochs: mean={e_star_summary['mean']:.1f}, \"\n",
    "      f\"std={e_star_summary['std']:.1f}, \"\n",
    "      f\"range=[{e_star_summary['min']}, {e_star_summary['max']}]\")\n",
    "\n",
    "plot_e_star_distribution(results[\"e_stars\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fold metrics: VAE vs benchmarks\n",
    "plot_fold_metrics(results[\"vae_results\"], results[\"benchmark_results\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Results - Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise tests heatmap\n",
    "plot_pairwise_heatmap(results[\"report\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed pairwise test results\n",
    "tests = results[\"report\"][\"statistical_tests\"]\n",
    "print(f\"Total comparisons: {tests['n_tests']} (alpha={tests['alpha']})\")\n",
    "print()\n",
    "\n",
    "for bench_name, metrics in tests[\"pairwise\"].items():\n",
    "    print(f\"VAE vs {bench_name}:\")\n",
    "    for metric, result in metrics.items():\n",
    "        if result.get(\"skipped\", False):\n",
    "            print(f\"  {metric}: skipped ({result['reason']})\")\n",
    "            continue\n",
    "        sig = \" *\" if result.get(\"significant_corrected\", False) else \"\"\n",
    "        print(f\"  {metric}: delta={result['median_delta']:+.4f} \"\n",
    "              f\"[{result['ci_lower']:+.4f}, {result['ci_upper']:+.4f}] \"\n",
    "              f\"p={result.get('p_corrected', result['p_value']):.4f}{sig}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"results/\"\n",
    "\n",
    "written = export_results(results, asdict(config), output_dir=OUTPUT_DIR)\n",
    "\n",
    "print(f\"Results saved to {OUTPUT_DIR}\")\n",
    "for path in written:\n",
    "    print(f\"  {os.path.basename(path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Pipeline Diagnostic (E2E)\n",
    "\n",
    "Calls `run_diagnostic()` **in-process** \u2014 tqdm progress bars render as native Jupyter widgets.\n",
    "Uses the same `stock_data`, `returns`, `trailing_vol` and `config` loaded in Sections 2\u20133.\n",
    "The underlying function is shared with `scripts/run_diagnostic.py` (CLI entry point).\n",
    "\n",
    "### Output (`results/diagnostic/`)\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `diagnostic_report.md` | Human-readable Markdown report |\n",
    "| `diagnostic_data.json` | Machine-readable JSON (all metrics) |\n",
    "| `health_checks.csv` | Health check results table |\n",
    "| `training_history.csv` | Per-epoch training metrics |\n",
    "| `strategy_comparison.csv` | VAE vs 6 benchmarks |\n",
    "| `plots/*.png` | 9 diagnostic plots |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9a. RUN DIAGNOSTIC (in-process \u2014 tqdm renders as Jupyter widget)\n",
    "#\n",
    "# Base config = Section 2b (`config`), which inherits ALL parameters:\n",
    "#   sigma_sq_min, dropout, weight_decay, lr, patience, phi, kappa, etc.\n",
    "#\n",
    "# Only speed-sensitive parameters are overridden per profile below.\n",
    "# ============================================================\n",
    "from dataclasses import replace as dc_replace\n",
    "from scripts.run_diagnostic import run_diagnostic\n",
    "from src.integration.pipeline_state import load_run_data\n",
    "\n",
    "# ---------- EDIT THESE PARAMETERS ----------\n",
    "DIAG_PROFILE = \"full\"              # \"quick\" (< 10 min) or \"full\" (production)\n",
    "DIAG_OUTPUT_DIR = \"results/diagnostic\"\n",
    "DIAG_HOLDOUT_FRACTION = 0.2\n",
    "DIAG_HOLDOUT_START = None          # or \"2020-01-01\" to override fraction\n",
    "DIAG_RUN_BENCHMARKS = True\n",
    "DIAG_GENERATE_PLOTS = True\n",
    "\n",
    "# === EXTENDED CHECKPOINTING ===\n",
    "# RUN_DIR: Path to existing run folder for resume/load. \n",
    "#   If None: creates new timestamped folder (e.g., results/diagnostic_runs/2026-02-21_143052/)\n",
    "#   If path: uses existing folder (for resume or load-only)\n",
    "RUN_DIR: str | None = None  # e.g., \"results/diagnostic_runs/2026-02-21_143052\"\n",
    "\n",
    "# LOAD_EXISTING: Skip execution and load from existing run folder\n",
    "#   If True: loads diagnostics from RUN_DIR, skips pipeline execution\n",
    "#   If False: runs the full diagnostic pipeline (default behavior)\n",
    "LOAD_EXISTING: bool = False\n",
    "\n",
    "# VAE_CHECKPOINT_OVERRIDE: Override VAE checkpoint path (optional)\n",
    "VAE_CHECKPOINT_OVERRIDE: str | None = None\n",
    "\n",
    "# Speed overrides per profile (only these affect execution time).\n",
    "# \"full\" = {} means use Section 2b values as-is.\n",
    "DIAG_SPEED_OVERRIDES: dict[str, dict] = {\n",
    "    \"quick\": {\"n_stocks\": 50, \"K\": 30, \"max_epochs\": 15, \"batch_size\": 256},\n",
    "    \"full\":  {},\n",
    "}\n",
    "# -------------------------------------------\n",
    "\n",
    "# Apply speed overrides on top of Section 2b config\n",
    "_overrides = DIAG_SPEED_OVERRIDES.get(DIAG_PROFILE, {})\n",
    "diag_config = dc_replace(\n",
    "    config,\n",
    "    data=dc_replace(config.data,\n",
    "                    n_stocks=_overrides.get(\"n_stocks\", config.data.n_stocks)),\n",
    "    vae=dc_replace(config.vae,\n",
    "                   K=_overrides.get(\"K\", config.vae.K)),\n",
    "    training=dc_replace(config.training,\n",
    "                        max_epochs=_overrides.get(\"max_epochs\", config.training.max_epochs),\n",
    "                        batch_size=_overrides.get(\"batch_size\", config.training.batch_size)),\n",
    ")\n",
    "\n",
    "print(f\"Diagnostic profile: {DIAG_PROFILE} (base = Section 2b config)\")\n",
    "print(f\"  Speed overrides: {_overrides if _overrides else 'none (production values)'}\")\n",
    "print(f\"  K={diag_config.vae.K}, max_epochs={diag_config.training.max_epochs}, \"\n",
    "      f\"batch={diag_config.training.batch_size}, n_stocks={diag_config.data.n_stocks}\")\n",
    "print(f\"  sigma_sq_min={diag_config.vae.sigma_sq_min}, dropout={diag_config.vae.dropout}, \"\n",
    "      f\"weight_decay={diag_config.training.weight_decay}, lr={diag_config.training.learning_rate}, \"\n",
    "      f\"patience={diag_config.training.patience}\")\n",
    "\n",
    "if LOAD_EXISTING and RUN_DIR:\n",
    "    # Load from existing run folder (no pipeline execution)\n",
    "    print(f\"\\n=== LOADING FROM EXISTING RUN: {RUN_DIR} ===\")\n",
    "    _run_data = load_run_data(RUN_DIR)\n",
    "    diagnostics = _run_data.get(\"diagnostics\", {})\n",
    "    _w_loaded = _run_data.get(\"weights\")\n",
    "    _stock_ids_loaded = _run_data.get(\"stock_ids\", [])\n",
    "    \n",
    "    # Expose data for sections 9b-10\n",
    "    if _w_loaded is not None:\n",
    "        diagnostics[\"_raw_weights\"] = _w_loaded\n",
    "    if _stock_ids_loaded:\n",
    "        diagnostics[\"_raw_stock_ids\"] = _stock_ids_loaded\n",
    "    \n",
    "    ACTIVE_RUN_DIR = RUN_DIR\n",
    "    print(f\"  Loaded diagnostics: {len(diagnostics)} top-level keys\")\n",
    "    print(f\"  Weights: {'loaded' if _w_loaded is not None else 'not found'}\")\n",
    "    print(f\"  Stock IDs: {len(_stock_ids_loaded)} stocks\")\n",
    "else:\n",
    "    # Run full diagnostic pipeline\n",
    "    if CHECKPOINT_PATH:\n",
    "        print(f\"  Pretrained model: {CHECKPOINT_PATH} (skipping VAE training)\")\n",
    "    if RUN_DIR:\n",
    "        print(f\"  Resuming from: {RUN_DIR}\")\n",
    "    \n",
    "    _result = run_diagnostic(\n",
    "        stock_data=stock_data,\n",
    "        returns=returns,\n",
    "        trailing_vol=trailing_vol,\n",
    "        config=diag_config,\n",
    "        output_dir=DIAG_OUTPUT_DIR,\n",
    "        device=DEVICE,\n",
    "        holdout_fraction=DIAG_HOLDOUT_FRACTION,\n",
    "        holdout_start=DIAG_HOLDOUT_START,\n",
    "        loss_mode=config.loss.mode,\n",
    "        run_benchmarks=DIAG_RUN_BENCHMARKS,\n",
    "        generate_plots=DIAG_GENERATE_PLOTS,\n",
    "        tensorboard_dir=\"runs/diagnostic\",\n",
    "        profile_name=DIAG_PROFILE,\n",
    "        data_source_label=DATA_SOURCE,\n",
    "        seed=SEED,\n",
    "        pretrained_model=CHECKPOINT_PATH or VAE_CHECKPOINT_OVERRIDE,\n",
    "        run_dir=RUN_DIR,\n",
    "        vae_checkpoint_override=VAE_CHECKPOINT_OVERRIDE,\n",
    "    )\n",
    "    \n",
    "    # Extract diagnostics from result dict (new return format)\n",
    "    diagnostics = _result.get(\"diagnostics\", _result)\n",
    "    ACTIVE_RUN_DIR = _result.get(\"run_dir\", DIAG_OUTPUT_DIR)\n",
    "    \n",
    "    # Also store weights and stock_ids for sections 9c-9d\n",
    "    if \"weights\" in _result:\n",
    "        diagnostics[\"_raw_weights\"] = _result[\"weights\"]\n",
    "    if \"stock_ids\" in _result:\n",
    "        diagnostics[\"_raw_stock_ids\"] = _result[\"stock_ids\"]\n",
    "    \n",
    "    print(f\"\\n=== RUN COMPLETE ===\")\n",
    "    print(f\"  Run directory: {ACTIVE_RUN_DIR}\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9b. DISPLAY DIAGNOSTIC PLOTS (from saved PNGs)\n",
    "# ============================================================\n",
    "from IPython.display import display, Image as IPImage\n",
    "from pathlib import Path\n",
    "\n",
    "_plots_dir = Path(DIAG_OUTPUT_DIR) / \"plots\"\n",
    "\n",
    "if _plots_dir.exists():\n",
    "    _pngs = sorted(_plots_dir.glob(\"*.png\"))\n",
    "    if _pngs:\n",
    "        print(f\"Displaying {len(_pngs)} diagnostic plots from {_plots_dir}/\\n\")\n",
    "        for _p in _pngs:\n",
    "            display(IPImage(filename=str(_p), width=900))\n",
    "    else:\n",
    "        print(f\"No PNG files found in {_plots_dir}/\")\n",
    "else:\n",
    "    print(f\"Plots directory not found: {_plots_dir}/  \u2014 run cell 9a first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9c. DISPLAY MARKDOWN REPORT\n",
    "# ============================================================\n",
    "from IPython.display import display, Markdown\n",
    "from pathlib import Path\n",
    "\n",
    "_report_path = Path(DIAG_OUTPUT_DIR) / \"diagnostic_report.md\"\n",
    "\n",
    "if _report_path.exists():\n",
    "    _md_text = _report_path.read_text(encoding=\"utf-8\")\n",
    "    print(f\"Report loaded from {_report_path} ({len(_md_text)} chars)\\n\")\n",
    "    display(Markdown(_md_text))\n",
    "else:\n",
    "    print(f\"Report not found: {_report_path}  \u2014 run cell 9a first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9c-bis. PORTFOLIO HOLDINGS \u2014 Selected Stocks\n",
    "# ============================================================\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "_stock_ids = diagnostics.get(\"_raw_stock_ids\", [])\n",
    "_w = diagnostics.get(\"_raw_weights\", None)\n",
    "\n",
    "# Fallback: use Section 4b results if diagnostic data unavailable\n",
    "if (_w is None or len(_stock_ids) == 0) and \"results\" in dir():\n",
    "    _stock_ids = results.get(\"state\", {}).get(\"inferred_stock_ids\", [])\n",
    "    _w = results.get(\"weights\", np.zeros(len(_stock_ids)))\n",
    "\n",
    "if _w is not None and len(_stock_ids) > 0:\n",
    "    # --- Build permno -> ticker mapping ---\n",
    "    _meta_dir = Path(DATA_DIR) / \"tiingo_meta\"\n",
    "    _permno_to_ticker: dict[int, str] = {}\n",
    "    _ticker_meta: dict[str, dict] = {}\n",
    "\n",
    "    _json_path = _meta_dir / \"ticker_to_permno.json\"\n",
    "    if _json_path.exists():\n",
    "        with open(_json_path) as f:\n",
    "            _t2p = json.load(f)\n",
    "        _permno_to_ticker = {int(v): k for k, v in _t2p.items()}\n",
    "\n",
    "    _csv_path = _meta_dir / \"supported_tickers.csv\"\n",
    "    if _csv_path.exists():\n",
    "        _meta_df = pd.read_csv(_csv_path)\n",
    "        for _, row in _meta_df.iterrows():\n",
    "            _ticker_meta[str(row[\"ticker\"])] = {\n",
    "                \"exchange\": str(row.get(\"exchange\", \"\")),\n",
    "                \"is_sp500\": bool(row.get(\"is_sp500\", False)),\n",
    "            }\n",
    "\n",
    "    # --- Latest market cap per permno ---\n",
    "    _latest_mcap: dict[int, float] = {}\n",
    "    if \"stock_data\" in dir() and stock_data is not None:\n",
    "        _last_date = stock_data[\"date\"].max()\n",
    "        _latest = stock_data[stock_data[\"date\"] == _last_date]\n",
    "        for _, row in _latest.iterrows():\n",
    "            _latest_mcap[int(row[\"permno\"])] = float(row[\"market_cap\"]) if pd.notna(row.get(\"market_cap\")) else float(\"nan\")\n",
    "\n",
    "    # --- Build holdings table ---\n",
    "    _rows = []\n",
    "    for i, permno in enumerate(_stock_ids):\n",
    "        w_i = float(_w[i])\n",
    "        if w_i < 1e-8:\n",
    "            continue\n",
    "        ticker = _permno_to_ticker.get(permno, f\"ID_{permno}\")\n",
    "        meta = _ticker_meta.get(ticker, {})\n",
    "        mcap = _latest_mcap.get(permno, float(\"nan\"))\n",
    "        _rows.append({\n",
    "            \"Ticker\": ticker,\n",
    "            \"Weight (%)\": round(w_i * 100, 2),\n",
    "            \"Market Cap ($M)\": round(mcap / 1e6, 0) if not np.isnan(mcap) else None,\n",
    "            \"Exchange\": meta.get(\"exchange\", \"\"),\n",
    "            \"S&P 500\": \"Yes\" if meta.get(\"is_sp500\", False) else \"\",\n",
    "            \"Permno\": permno,\n",
    "        })\n",
    "\n",
    "    _holdings = pd.DataFrame(_rows).sort_values(\"Weight (%)\", ascending=False).reset_index(drop=True)\n",
    "    _holdings.index = _holdings.index + 1  # 1-based rank\n",
    "\n",
    "    print(f\"Portfolio Holdings: {len(_holdings)} active positions\")\n",
    "    print(f\"  Total weight: {_holdings['Weight (%)'].sum():.1f}%\")\n",
    "    print(f\"  Weight range: {_holdings['Weight (%)'].min():.2f}% \u2013 {_holdings['Weight (%)'].max():.2f}%\")\n",
    "    _n_sp500 = sum(1 for r in _rows if r[\"S&P 500\"] == \"Yes\")\n",
    "    if _n_sp500 > 0:\n",
    "        print(f\"  S&P 500 members: {_n_sp500}/{len(_holdings)}\")\n",
    "    print()\n",
    "    display(_holdings)\n",
    "else:\n",
    "    print(\"No portfolio weights available \u2014 run Section 9a first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9c-ter. FACTOR EXPOSURE HEATMAP \u2014 Before vs After Optimization\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get exposure matrix and weights from diagnostics or results\n",
    "_state = diagnostics.get(\"state_bag\", results.get(\"state\", {}))\n",
    "_B_A = _state.get(\"B_A\")\n",
    "_stock_ids = _state.get(\"inferred_stock_ids\", diagnostics.get(\"_raw_stock_ids\", []))\n",
    "_w = diagnostics.get(\"_raw_weights\", None)\n",
    "_AU = _state.get(\"AU\", 0)\n",
    "\n",
    "if _B_A is not None and _w is not None and len(_stock_ids) > 0:\n",
    "    # --- Prepare data ---\n",
    "    _w = np.asarray(_w)\n",
    "    _selected_mask = _w > 1e-8  # Stocks selected after optimization\n",
    "    _n_selected = int(np.sum(_selected_mask))\n",
    "\n",
    "    # Limit factors shown for readability\n",
    "    _n_factors_show = min(30, _AU) if _AU > 0 else min(30, _B_A.shape[1])\n",
    "\n",
    "    # BEFORE: Top N stocks by equal-weight (show first N for simplicity, or sort by market cap)\n",
    "    _n_before = min(50, _B_A.shape[0])\n",
    "    _B_before = _B_A[:_n_before, :_n_factors_show]\n",
    "    _ids_before = list(_stock_ids[:_n_before])\n",
    "\n",
    "    # AFTER: Only selected stocks, sorted by weight descending\n",
    "    _B_after_full = _B_A[_selected_mask, :]\n",
    "    _w_selected = _w[_selected_mask]\n",
    "    _ids_selected = [sid for sid, sel in zip(_stock_ids, _selected_mask) if sel]\n",
    "\n",
    "    # Sort by weight descending\n",
    "    _sort_idx = np.argsort(_w_selected)[::-1]\n",
    "    _B_after = _B_after_full[_sort_idx, :_n_factors_show]\n",
    "    _w_sorted = _w_selected[_sort_idx]\n",
    "    _ids_after = [_ids_selected[i] for i in _sort_idx]\n",
    "\n",
    "    # Limit rows for display\n",
    "    _n_after_show = min(50, len(_ids_after))\n",
    "    _B_after = _B_after[:_n_after_show, :]\n",
    "    _ids_after = _ids_after[:_n_after_show]\n",
    "    _w_sorted = _w_sorted[:_n_after_show]\n",
    "\n",
    "    # --- Create figure with two heatmaps ---\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, max(8, _n_after_show * 0.2)))\n",
    "\n",
    "    # Color limits for consistent comparison\n",
    "    _vmax = max(np.abs(_B_before).max(), np.abs(_B_after).max())\n",
    "    _vmin = -_vmax\n",
    "\n",
    "    # LEFT: Before optimization (equal-weight universe)\n",
    "    im1 = axes[0].imshow(_B_before, aspect=\"auto\", cmap=\"RdBu_r\", vmin=_vmin, vmax=_vmax)\n",
    "    axes[0].set_xlabel(f\"Latent Factor (1 to {_n_factors_show})\")\n",
    "    axes[0].set_ylabel(\"Stock (universe order)\")\n",
    "    axes[0].set_title(f\"BEFORE: All Stocks ({_n_before}/{_B_A.shape[0]})\\nEqual-weight exposure\")\n",
    "\n",
    "    # RIGHT: After optimization (selected stocks, weighted)\n",
    "    im2 = axes[1].imshow(_B_after, aspect=\"auto\", cmap=\"RdBu_r\", vmin=_vmin, vmax=_vmax)\n",
    "    axes[1].set_xlabel(f\"Latent Factor (1 to {_n_factors_show})\")\n",
    "    axes[1].set_ylabel(\"Stock (sorted by weight)\")\n",
    "    axes[1].set_title(f\"AFTER: Selected Stocks ({_n_after_show}/{_n_selected})\\nSorted by entropy-optimal weight\")\n",
    "\n",
    "    # Add weight annotations on y-axis for \"after\" plot\n",
    "    _y_labels = [f\"{_w_sorted[i]*100:.1f}%\" for i in range(len(_w_sorted))]\n",
    "    axes[1].set_yticks(range(len(_y_labels)))\n",
    "    axes[1].set_yticklabels(_y_labels, fontsize=7)\n",
    "    axes[1].yaxis.set_label_position(\"right\")\n",
    "    axes[1].yaxis.tick_right()\n",
    "\n",
    "    # Shared colorbar\n",
    "    fig.colorbar(im2, ax=axes, orientation=\"vertical\", fraction=0.02, pad=0.04, label=\"Exposure (z-scored)\")\n",
    "\n",
    "    fig.suptitle(f\"Factor Exposure Heatmap \u2014 Before vs After Shannon Entropy Optimization\\n\"\n",
    "                 f\"AU = {_AU} active dimensions\", fontsize=12, fontweight=\"bold\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Summary statistics\n",
    "    print(f\"\\nExposure Summary:\")\n",
    "    print(f\"  Before: {_B_A.shape[0]} stocks x {_AU} factors (AU)\")\n",
    "    print(f\"  After:  {_n_selected} stocks selected (w > 0)\")\n",
    "    print(f\"  Cardinality reduction: {_B_A.shape[0]} -> {_n_selected} ({100*_n_selected/_B_A.shape[0]:.1f}%)\")\n",
    "else:\n",
    "    print(\"Exposure data not available \u2014 run Section 9a first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9d. EXPORT DIAGNOSTIC FOLDER AS ZIP\n",
    "# ============================================================\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from IPython.display import display, FileLink\n",
    "\n",
    "_diag_dir = Path(DIAG_OUTPUT_DIR)\n",
    "_zip_name = \"diagnostic_export\"\n",
    "_zip_path = Path(_zip_name)  # shutil adds .zip\n",
    "\n",
    "if _diag_dir.exists() and any(_diag_dir.iterdir()):\n",
    "    archive = shutil.make_archive(str(_zip_path), \"zip\", root_dir=_diag_dir.parent, base_dir=_diag_dir.name)\n",
    "    size_mb = Path(archive).stat().st_size / (1024 * 1024)\n",
    "    print(f\"Archive created: {archive}  ({size_mb:.2f} MB)\")\n",
    "    n_files = sum(1 for _ in _diag_dir.rglob(\"*\") if _.is_file())\n",
    "    print(f\"Contains {n_files} files from {_diag_dir}/\")\n",
    "    display(FileLink(archive))\n",
    "else:\n",
    "    print(f\"Directory {_diag_dir}/ is empty or missing \u2014 run cell 9a first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Decision Synthesis\n",
    "\n",
    "This section provides an automated root cause analysis and actionable recommendations\n",
    "based on the diagnostic scores from Section 9. It helps identify the underlying causes\n",
    "of any issues and suggests specific configuration changes.\n",
    "\n",
    "**Components:**\n",
    "- Root Cause Analysis with matched decision rules\n",
    "- Causal chain visualization\n",
    "- Executable configuration recommendations\n",
    "- Validated JSON output for automation\n",
    "\n",
    "**Source modules:**\n",
    "- `src/integration/decision_rules.py` \u2014 10 decision rules + causal graph + 8 metric patterns\n",
    "- `src/integration/action_specs.py` \u2014 35+ config change specifications\n",
    "- `src/integration/diagnostic_schema.py` \u2014 JSON schema validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10a. ROOT CAUSE ANALYSIS\n",
    "# ============================================================\n",
    "from src.integration.decision_rules import (\n",
    "    get_root_cause_analysis,\n",
    "    format_diagnosis_summary,\n",
    ")\n",
    "\n",
    "# Extract composite scores from diagnostic output\n",
    "_comp_scores = diagnostics.get(\"composite_scores\", {})\n",
    "\n",
    "# Build scores dict for decision rule evaluation\n",
    "_scores = {\n",
    "    \"solver_score\": _comp_scores.get(\"solver\", {}).get(\"score\", 100),\n",
    "    \"constraint_score\": _comp_scores.get(\"constraint\", {}).get(\"score\", 100),\n",
    "    \"covariance_score\": _comp_scores.get(\"covariance\", {}).get(\"score\", 100),\n",
    "    \"reconstruction_score\": _comp_scores.get(\"reconstruction\", {}).get(\"score\", 100),\n",
    "    \"vae_health_score\": _comp_scores.get(\"vae_health\", {}).get(\"score\", 100),\n",
    "    \"factor_model_score\": _comp_scores.get(\"factor_model\", {}).get(\"score\", 100),\n",
    "}\n",
    "\n",
    "# Extract raw metrics for pattern detection\n",
    "_raw_metrics = {\n",
    "    \"latent_stability_rho\": diagnostics.get(\"state_bag\", {}).get(\"latent_stability_rho\"),\n",
    "    \"shrinkage_intensity\": diagnostics.get(\"state_bag\", {}).get(\"shrinkage_intensity\"),\n",
    "    \"AU\": diagnostics.get(\"state_bag\", {}).get(\"AU\"),\n",
    "    \"k_bai_ng\": diagnostics.get(\"state_bag\", {}).get(\"k_bai_ng\"),\n",
    "    \"condition_number\": _comp_scores.get(\"covariance\", {}).get(\"details\", {}).get(\"condition_number\"),\n",
    "    \"overfit_ratio\": diagnostics.get(\"training_summary\", {}).get(\"overfit_ratio\"),\n",
    "    \"explanatory_power\": _comp_scores.get(\"covariance\", {}).get(\"details\", {}).get(\"explanatory_power\"),\n",
    "}\n",
    "\n",
    "# Perform root cause analysis\n",
    "_analysis = get_root_cause_analysis(_scores, _raw_metrics)\n",
    "\n",
    "# Display formatted summary\n",
    "print(format_diagnosis_summary(_analysis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10b. MATCHED DECISION RULES TABLE\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Create rules table\n",
    "_rules_data = []\n",
    "for _rule in _analysis.get(\"matching_rules\", []):\n",
    "    _rules_data.append({\n",
    "        \"Rule ID\": _rule[\"rule_id\"],\n",
    "        \"Diagnosis\": _rule[\"diagnosis\"],\n",
    "        \"Confidence\": f\"{_rule['confidence']*100:.0f}%\",\n",
    "        \"Severity\": _rule[\"severity\"].upper(),\n",
    "        \"Root Causes\": \", \".join(_rule[\"root_causes\"][:2]),\n",
    "    })\n",
    "\n",
    "if _rules_data:\n",
    "    _df_rules = pd.DataFrame(_rules_data)\n",
    "    display(HTML(f\"<h4>Matched Decision Rules ({len(_rules_data)})</h4>\"))\n",
    "    display(_df_rules.style.set_properties(**{'text-align': 'left'}))\n",
    "else:\n",
    "    print(\"No issues detected - all systems nominal\")\n",
    "\n",
    "# Show detected patterns\n",
    "_patterns = _analysis.get(\"detected_patterns\", [])\n",
    "if _patterns:\n",
    "    print(f\"\\nDetected Patterns ({len(_patterns)}):\")\n",
    "    for _p in _patterns:\n",
    "        print(f\"  - {_p['name']}: {_p['interpretation']}\")\n",
    "        print(f\"    Recommendation: {_p['recommendation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10c. CAUSAL CHAIN VISUALIZATION\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Extract causal chain\n",
    "_causal = _analysis.get(\"causal_analysis\", {})\n",
    "_metric = _causal.get(\"metric\", \"unknown\")\n",
    "_upstream = _causal.get(\"upstream_causes\", [])\n",
    "_downstream = _causal.get(\"downstream_effects\", [])\n",
    "\n",
    "# Create flow diagram\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 4)\n",
    "ax.axis('off')\n",
    "\n",
    "_weakest = _analysis.get(\"weakest_component\", \"unknown\")\n",
    "_weakest_score = _analysis.get(\"weakest_score\", 0)\n",
    "ax.set_title(f\"Causal Chain for Weakest Component: {_weakest} (score: {_weakest_score:.1f})\",\n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "def draw_box(x, y, text, color, width=1.8):\n",
    "    \"\"\"Draw a rounded rectangle with text.\"\"\"\n",
    "    rect = mpatches.FancyBboxPatch(\n",
    "        (x - width/2, y - 0.35), width, 0.7,\n",
    "        boxstyle=\"round,pad=0.05,rounding_size=0.1\",\n",
    "        facecolor=color, edgecolor='#333333', linewidth=1.5\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "    # Handle long text by wrapping\n",
    "    display_text = text.replace('_', '\\n') if len(text) > 12 else text\n",
    "    ax.text(x, y, display_text, ha='center', va='center', fontsize=9,\n",
    "            fontweight='medium', wrap=True)\n",
    "\n",
    "# Central metric (weakest component)\n",
    "draw_box(6, 2, _metric.replace('_', '\\n'), '#ff9999', width=2.2)\n",
    "\n",
    "# Upstream causes (left side)\n",
    "_n_up = min(4, len(_upstream))\n",
    "for i, cause in enumerate(_upstream[:_n_up]):\n",
    "    x = 1.2 + i * 1.3\n",
    "    draw_box(x, 3.2, cause, '#99ccff', width=1.5)\n",
    "    ax.annotate('', xy=(6 - 1.1, 2.35), xytext=(x + 0.6, 2.85),\n",
    "                arrowprops=dict(arrowstyle='->', color='#3366cc', lw=1.5))\n",
    "\n",
    "# Downstream effects (right side)\n",
    "_n_down = min(4, len(_downstream))\n",
    "for i, effect in enumerate(_downstream[:_n_down]):\n",
    "    x = 7 + i * 1.3\n",
    "    draw_box(x, 0.8, effect, '#99ff99', width=1.5)\n",
    "    ax.annotate('', xy=(x - 0.4, 1.15), xytext=(6 + 1.1, 1.65),\n",
    "                arrowprops=dict(arrowstyle='->', color='#339933', lw=1.5))\n",
    "\n",
    "# Legend\n",
    "_legend_items = [\n",
    "    mpatches.Patch(facecolor='#99ccff', edgecolor='#333', label='Upstream Causes'),\n",
    "    mpatches.Patch(facecolor='#ff9999', edgecolor='#333', label='Weakest Component'),\n",
    "    mpatches.Patch(facecolor='#99ff99', edgecolor='#333', label='Downstream Effects'),\n",
    "]\n",
    "ax.legend(handles=_legend_items, loc='upper right', framealpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10d. CONFIGURATION RECOMMENDATIONS\n",
    "# ============================================================\n",
    "from src.integration.action_specs import get_executable_actions\n",
    "\n",
    "# Get priority actions from overall score\n",
    "_overall_result = _comp_scores.get(\"overall\", {})\n",
    "_priority_actions_raw = _overall_result.get(\"priority_actions\", [])\n",
    "\n",
    "# Fallback: use actions from matched rules if overall unavailable\n",
    "if not _priority_actions_raw:\n",
    "    _priority_actions_raw = []\n",
    "    for _rule in _analysis.get(\"matching_rules\", [])[:3]:\n",
    "        for _action in _rule.get(\"actions\", [])[:2]:\n",
    "            _priority_actions_raw.append({\n",
    "                \"component\": _analysis.get(\"weakest_component\", \"unknown\"),\n",
    "                \"action\": _action,\n",
    "            })\n",
    "\n",
    "# Convert to executable specifications\n",
    "_exec_actions = get_executable_actions(_priority_actions_raw)\n",
    "\n",
    "# Create recommendations table\n",
    "_reco_data = []\n",
    "for _action in _exec_actions:\n",
    "    if _action.get(\"recognized\", False):\n",
    "        _suggested = _action.get(\"suggested_value\")\n",
    "        if _suggested is None:\n",
    "            _sv = _action.get(\"suggested_values\", [\"N/A\"])\n",
    "            _suggested = _sv[0] if _sv else \"N/A\"\n",
    "        _reco_data.append({\n",
    "            \"Priority\": len(_reco_data) + 1,\n",
    "            \"Component\": _action.get(\"component\", \"\"),\n",
    "            \"Config Key\": _action.get(\"config_key\", \"\"),\n",
    "            \"Suggested Value\": _suggested,\n",
    "            \"Rationale\": _action.get(\"rationale\", \"\"),\n",
    "        })\n",
    "    else:\n",
    "        _reco_data.append({\n",
    "            \"Priority\": len(_reco_data) + 1,\n",
    "            \"Component\": _action.get(\"component\", \"\"),\n",
    "            \"Config Key\": \"(manual review)\",\n",
    "            \"Suggested Value\": \"-\",\n",
    "            \"Rationale\": _action.get(\"original_action\", \"\")[:80],\n",
    "        })\n",
    "\n",
    "if _reco_data:\n",
    "    _df_reco = pd.DataFrame(_reco_data)\n",
    "    display(HTML(\"<h4>Configuration Recommendations</h4>\"))\n",
    "    display(_df_reco.style.set_properties(**{'text-align': 'left'}))\n",
    "else:\n",
    "    print(\"No configuration changes recommended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10e. VALIDATED JSON EXPORT\n",
    "# ============================================================\n",
    "from src.integration.diagnostic_schema import (\n",
    "    validate_diagnostic_output,\n",
    "    severity_from_score,\n",
    "    create_minimal_output,\n",
    ")\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Create structured output\n",
    "_overall_score = _comp_scores.get(\"overall\", {}).get(\"score\", 50.0)\n",
    "_severity = severity_from_score(_overall_score)\n",
    "_verdict = (\n",
    "    _analysis[\"matching_rules\"][0][\"diagnosis\"]\n",
    "    if _analysis.get(\"matching_rules\")\n",
    "    else \"All systems nominal\"\n",
    ")\n",
    "\n",
    "_output = create_minimal_output(_overall_score, _severity, _verdict)\n",
    "\n",
    "# Add component scores\n",
    "_output[\"component_scores\"] = {}\n",
    "for _comp in [\"solver\", \"constraint\", \"covariance\", \"reconstruction\", \"vae_health\", \"factor_model\"]:\n",
    "    if _comp in _comp_scores:\n",
    "        _output[\"component_scores\"][_comp] = {\n",
    "            \"score\": _comp_scores[_comp].get(\"score\", 0),\n",
    "            \"grade\": _comp_scores[_comp].get(\"grade\", \"F\"),\n",
    "            \"available\": True,\n",
    "        }\n",
    "\n",
    "# Add priority actions\n",
    "_output[\"priority_actions\"] = [\n",
    "    {\"component\": a.get(\"component\"), \"action\": a.get(\"original_action\", a.get(\"action\", \"\"))}\n",
    "    for a in _exec_actions[:5]\n",
    "]\n",
    "\n",
    "# Add key findings from analysis\n",
    "_output[\"key_findings\"] = [\n",
    "    {\n",
    "        \"metric\": \"weakest_component\",\n",
    "        \"value\": _analysis.get(\"weakest_score\"),\n",
    "        \"interpretation\": f\"{_analysis.get('weakest_component')} has lowest score\",\n",
    "    }\n",
    "]\n",
    "for _p in _analysis.get(\"detected_patterns\", [])[:3]:\n",
    "    _output[\"key_findings\"].append({\n",
    "        \"metric\": _p[\"pattern_id\"],\n",
    "        \"interpretation\": _p[\"interpretation\"],\n",
    "    })\n",
    "\n",
    "# Add metadata\n",
    "_output[\"metadata\"] = {\n",
    "    \"schema_version\": \"1.0.0\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "# Validate\n",
    "_is_valid, _errors = validate_diagnostic_output(_output)\n",
    "\n",
    "print(f\"Schema Validation: {'VALID' if _is_valid else 'INVALID'}\")\n",
    "if _errors:\n",
    "    for _e in _errors:\n",
    "        print(f\"  - {_e}\")\n",
    "\n",
    "# Display JSON\n",
    "print(\"\\n--- decision_synthesis.json ---\")\n",
    "print(json.dumps(_output, indent=2))\n",
    "\n",
    "# Save to file\n",
    "_out_dir = Path(DIAG_OUTPUT_DIR)\n",
    "_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "with open(_out_dir / \"decision_synthesis.json\", \"w\") as f:\n",
    "    json.dump(_output, f, indent=2)\n",
    "print(f\"\\nSaved to {_out_dir / 'decision_synthesis.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Reference: Decision Rules\n",
    "\n",
    "| Rule ID | Condition | Diagnosis |\n",
    "|---------|-----------|----------|\n",
    "| PURE_OPTIMIZATION | solver<60, constraint>80 | Optimization problem, not constraint-related |\n",
    "| CONSTRAINT_DOMINATED | solver>70, constraint<50 | Constraints limiting optimization freedom |\n",
    "| COVARIANCE_DEGRADATION | covariance<55 | Risk model miscalibrated |\n",
    "| VAE_COLLAPSE | reconstruction<50, vae_health<50 | VAE posterior collapse detected |\n",
    "| FACTOR_MODEL_WEAK | factor_model<50 | Factor model specification issues |\n",
    "| OVERALL_DEGRADATION | solver<60, covariance<60, recon<60 | Systemic pipeline degradation |\n",
    "| HEALTHY_PIPELINE | solver>75, constraint>60, cov>70, recon>70 | Pipeline operating normally |\n",
    "| RECONSTRUCTION_IMBALANCE | recon 40-70, vae_health>60 | Feature reconstruction imbalanced |\n",
    "| OVERFITTING_DETECTED | reconstruction<55 + overfit_ratio>1.3 | VAE overfitting to training data |\n",
    "| SOLVER_CONSTRAINT_CONFLICT | solver<60, constraint<60 | Infeasible optimization problem |\n",
    "\n",
    "**Action Categories:**\n",
    "- `solver`: sca_max_iter, sca_tol, n_starts, armijo_rho\n",
    "- `constraints`: w_max, w_min, tau_max, kappa_1, kappa_2\n",
    "- `vae`: dropout, K\n",
    "- `training`: max_epochs, learning_rate, patience, batch_size\n",
    "- `loss`: beta_fixed, gamma, lambda_co_max\n",
    "- `risk_model`: ridge_scale, d_eps_floor, sigma_z_shrinkage\n",
    "\n",
    "**Reference:** `docs/diagnostic.md` for full interpretation guide."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Latent_risk_factor (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}