{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cloner le repo\n",
    "# !git clone https://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@github.com/Mathis-Royer/Latent_risk_factor.git\n",
    "\n",
    "# 2. Se placer dans le dossier\n",
    "# %cd Latent_risk_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /content/Latent_risk_factor\n",
    "# %run notebooks/dashboard.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Latent Risk Factor - Pipeline Dashboard\n",
    "\n",
    "Central configuration and execution notebook for the full walk-forward validation pipeline.\n",
    "\n",
    "**Workflow:**\n",
    "1. Configure all parameters (Sections 1-2)\n",
    "2. Load data (Section 3)\n",
    "3. Run pipeline (Section 4)\n",
    "4. Inspect results (Sections 5-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from dataclasses import replace, asdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project root: go up from notebooks/ to project root\n",
    "_NB_DIR = Path(os.path.abspath(\"\")).resolve()\n",
    "PROJECT_ROOT = (_NB_DIR / \"..\").resolve() if _NB_DIR.name == \"notebooks\" else _NB_DIR\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.config import (\n",
    "    PipelineConfig,\n",
    "    DataPipelineConfig,\n",
    "    VAEArchitectureConfig,\n",
    "    LossConfig,\n",
    "    TrainingConfig,\n",
    "    InferenceConfig,\n",
    "    RiskModelConfig,\n",
    "    PortfolioConfig,\n",
    "    WalkForwardConfig,\n",
    ")\n",
    "from src.data_pipeline.data_loader import load_data_source\n",
    "from src.data_pipeline.returns import compute_log_returns\n",
    "from src.data_pipeline.features import compute_trailing_volatility\n",
    "from src.integration.pipeline import FullPipeline\n",
    "from src.integration.reporting import export_results, format_summary_table\n",
    "from src.integration.visualization import (\n",
    "    plot_fold_metrics,\n",
    "    plot_e_star_distribution,\n",
    "    plot_pairwise_heatmap,\n",
    "    style_summary_table,\n",
    "    style_fold_table,\n",
    ")\n",
    "from src.utils import get_optimal_device\n",
    "from src.walk_forward.selection import aggregate_fold_metrics, summary_statistics\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\")\n",
    "logger = logging.getLogger(\"dashboard\")\n",
    "\n",
    "print(f\"PyTorch {torch.__version__} | Device: {get_optimal_device()}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration\n",
    "\n",
    "Two configuration profiles are available. **Run ONLY one section:**\n",
    "- **Section 2a** — Synthetic data: minimal parameters for quick end-to-end testing\n",
    "- **Section 2b** — Real data: full production configuration\n",
    "\n",
    "Always run the **Global** cell (below) first, then choose ONE section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GLOBAL\n",
    "# ============================================================\n",
    "SEED = 42\n",
    "DEVICE = str(get_optimal_device())\n",
    "\n",
    "# Data source: \"synthetic\", \"tiingo\", or \"csv\"\n",
    "DATA_SOURCE = \"tiingo\"\n",
    "QUICK_MODE = False          # Set True for minimal config even with real data\n",
    "RUN_FULL_PIPELINE = False   # Set False to skip training and inference (useful for quick visualization)\n",
    "\n",
    "# Tiingo API keys (used when DATA_SOURCE = \"tiingo\")\n",
    "TIINGO_API_KEYS = [\n",
    "    \"9ba6e57788deaac3b3c38ed47047cabbbd6077e2\",\n",
    "    \"9aad315d49275c400687f41dd26b22328d8b1a26\",\n",
    "]\n",
    "DATA_DIR = \"data/\"          # Directory for Tiingo downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STOCKS = 500                   #[⭐️] Top N stocks by median market cap (50=fast, 200=realistic, 0=all)\n",
    "N_YEARS = 15                     #[⭐️] Years of history to keep\n",
    "MIN_N_YEARS = 10                 # First fold trains on at least MIN_N_YEARS (shorter = unreliable model)\n",
    "T = 504                          #[⭐️] Window length (trading days, ~2 years)\n",
    "N_FEATURES = 2                   #[⭐️] Features per timestep (return + realized vol)\n",
    "K = 100                          #[⭐️] Max latent factors the VAE can discover (only \"active units\" AU<=K are used)\n",
    "LOSS_MODE = \"P\"                  # Loss mode: \"P\"=full ELBO with learned sigma^2, \"F\"=simplified with beta warmup, \"A\"=hybrid\n",
    "CRISIS_OVERWEIGHTING = 3.0       #[⭐️] Crisis overweighting: crisis windows count 3x more in reconstruction loss\n",
    "MAX_CO_MOVEMENT_WEIGHT = 0.5     # Max co-movement weight: how strongly latent distances must match Spearman correlations\n",
    "\n",
    "MAX_EPOCHS = 30                  #[⭐️] Max training epochs (early stopping may halt training sooner)\n",
    "BATCH_SIZE = 512                 # Training batch size (windows)\n",
    "LEARNING_RATE = 5e-4             # Adam learning rate\n",
    "EARLY_STOPPING_PATIENCE = 10     # Early stopping patience (epochs)\n",
    "\n",
    "AU_THRESHOLD = 0.01              # KL > 0.01 nats = dimension is \"active\" (below = unused, posterior ≈ prior)\n",
    "AGGREGATION_METHOD = \"mean\"      # Method to aggregate overlapping window predictions (\"mean\", \"median\", etc.)\n",
    "\n",
    "HP_GRID = None                   # HP GRID for Phase A (set to None for default 18-config grid : 3 loss modes × 2 learning rates × 3 alphas = 18 configs)\n",
    "                                 # Phase A tries all configs on nested validation to find the best HP set per fold\n",
    "                \n",
    "# Uncomment to define a custom grid (faster, but less thorough HP search):\n",
    "# HP_GRID = [\n",
    "#     {\"mode\": \"P\", \"learning_rate\": 5e-4, \"alpha\": 1.0},   # mode: loss formulation, alpha: risk-entropy tradeoff\n",
    "#     {\"mode\": \"F\", \"learning_rate\": 1e-3, \"alpha\": 0.5},\n",
    "#     {\"mode\": \"A\", \"learning_rate\": 1e-3, \"alpha\": 2.0},\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Variance (w'Σw) = total portfolio risk. Minimizing it concentrates\n",
    "# weight on low-vol stocks — but can put 85% of risk on 2-3 latent\n",
    "# factors without noticing (fragile to factor shocks).\n",
    "#\n",
    "# Entropy H(w) = Shannon entropy on each factor's risk contribution.\n",
    "# Maximizing it spreads risk evenly across all active factors — no\n",
    "# single factor dominates (resilient to individual factor crashes).\n",
    "#\n",
    "# lambda_risk and alpha control the tradeoff: more lambda_risk favors\n",
    "# low total variance; more alpha favors even factor risk distribution.\n",
    "#\n",
    "# lambda_risk guide:\n",
    "#   0.1  → entropy dominates (pure factor risk parity, higher variance OK)\n",
    "#   0.5  → entropy favored, soft variance constraint\n",
    "#   1.0  → balanced (default, let alpha_grid find the right ratio)\n",
    "#   2.0  → variance favored (more conservative, fewer dominant factors OK)\n",
    "#   5.0+ → near min-variance (entropy has little influence)\n",
    "#\n",
    "# Penalty tuning guide (defaults calibrated for US mid/large cap):\n",
    "#   Illiquid universe (small caps) → raise kappa_1 & kappa_2\n",
    "#   Infrequent rebalancing         → lower kappa_1 & kappa_2\n",
    "#   Allow more concentrated bets   → lower phi and/or raise w_bar\n",
    "#   Force tighter diversification  → raise phi and/or lower w_bar\n",
    "#   Smoother trades                → lower delta_bar (e.g. 0.005)\n",
    "#\n",
    "# Valid ranges from DVT spec:\n",
    "#   phi: [5, 100]  kappa_1: [0.01, 1.0]  kappa_2: [1, 50]  delta_bar: [0.005, 0.03]\n",
    "# ============================================================\n",
    "\n",
    "LAMBDA_RISK = 1.0                #[⭐️] Risk aversion (higher = more conservative, lower variance portfolio)\n",
    "\n",
    "W_MAX = 0.5                      # Max 5% per stock (hard cap, prevents single-stock concentration)\n",
    "W_MIN = 0.001                    # Min active weight: below this, stock is eliminated (0 or >= 0.1%)\n",
    "W_BAR = 0.03                     # Concentration penalty kicks in above 3% weight per stock\n",
    "\n",
    "PHI = 25.0                       # Concentration penalty strength above w_bar (higher = stronger diversification pressure)\n",
    "KAPPA_1 = 0.1                    # Linear turnover penalty (penalizes trading costs at rebalance)\n",
    "KAPPA_2 = 7.5                    # Quadratic turnover penalty (penalizes large trades more than small ones)\n",
    "DELTA_BAR = 0.01                 # Turnover below 1% is not penalized (de minimis threshold)\n",
    "MAX_TURNOVER = 0.30              #[⭐️] Max 30% turnover per rebalance (prevents excessive trading costs)\n",
    "\n",
    "SCA_N_STARTS = 10                # Multi-start optimizations (more = better optimum, proportionally slower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pipeline Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `n_stocks` | Universe size — how many stocks to keep (ranked by market cap). More = better diversification, slower training. |\n",
    "| `window_length` | Length of each input window in trading days. 504 ≈ 2 years of daily data. Each stock produces many overlapping windows. |\n",
    "| `n_features` | Features per timestep. 2 = log-return + realized volatility. |\n",
    "| `vol_window` | Lookback (trading days) for trailing volatility computation. 252 ≈ 1 year. |\n",
    "| `vix_lookback_percentile` | VIX percentile above which a day is labeled \"crisis\". Higher = fewer crisis days = less overweighting. |\n",
    "| `min_valid_fraction` | Minimum fraction of non-missing data to keep a stock. 0.80 = stocks missing >20% of their history are dropped. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAE Architecture Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `K` | Maximum latent capacity — how many risk factors the VAE *can* learn. Only \"active units\" (AU ≤ K) are actually used. Typical: 100-200. |\n",
    "| `sigma_sq_init` | Starting value of learned observation noise σ². 1.0 = assume noise equals signal at first. The model learns the true value during training (Mode P/A). |\n",
    "| `sigma_sq_min` / `sigma_sq_max` | Clamp bounds for σ². Prevents extreme values: too small = overfitting (model claims perfect reconstruction), too large = underfitting (model gives up). |\n",
    "| `window_length` / `n_features` | Must match DataPipelineConfig — determines input tensor shape (T×F). |\n",
    "| `r_max` | Maximum ratio of model parameters to data points. Safety guard — if the CNN has more parameters than the data can support, training is rejected. Relaxed automatically for small universes. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `mode` | Loss formulation. **P** = full probabilistic ELBO with learned σ² (recommended). **F** = simplified with β warmup, σ² frozen at 1.0 (use if Mode P diverges). **A** = hybrid with tunable KL weight β. |\n",
    "| `gamma` | Crisis overweighting factor. 3.0 = windows falling in crisis periods (high VIX) count 3× more in reconstruction loss. Forces the model to learn crisis dynamics well. |\n",
    "| `lambda_co_max` | Maximum co-movement loss weight. Controls how strongly latent distances must match stock Spearman correlations. Active during Phases 1-2, decays to 0 in Phase 3. |\n",
    "| `beta_fixed` | Fixed KL weight for Mode A (must be 1.0 for Mode P). Values <1 reduce regularization pressure, giving more freedom to reconstruction. |\n",
    "| `warmup_fraction` | Fraction of training where β ramps 0→1 (Mode F only). Prevents posterior collapse by letting the model learn to reconstruct before enforcing KL regularization. |\n",
    "| `max_pairs` | Max stock pairs sampled per batch for co-movement loss. Limits compute cost — full pairwise is O(B²). |\n",
    "| `delta_sync` | Maximum date gap (calendar days) for windows to be \"synchronized\" in the same time block. 21 ≈ 1 month. Ensures co-movement comparisons are temporally valid. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `max_epochs` | Hard limit on training duration. Training may stop earlier via early stopping. |\n",
    "| `batch_size` | Windows per gradient update. Larger = smoother gradients but more memory. 256 is a good default. |\n",
    "| `learning_rate` | Initial optimizer step size (η₀). Too high → loss diverges. Too low → converges very slowly. Typical: 1e-4 to 1e-3. |\n",
    "| `weight_decay` | L2 regularization on model weights. Penalizes large weights to reduce overfitting. 1e-5 is mild. |\n",
    "| `adam_betas` / `adam_eps` | Adam optimizer internals (momentum and numerical stability). Rarely need tuning. |\n",
    "| `patience` | **Early stopping** — if validation ELBO doesn't improve for this many consecutive epochs, stop training and restore the best checkpoint. |\n",
    "| `lr_patience` | **LR reduction** — if validation stagnates for this many epochs, multiply LR by `lr_factor`. Triggers before early stopping. |\n",
    "| `lr_factor` | LR reduction multiplier. 0.5 = halve the learning rate each time it triggers. |\n",
    "| `n_strata` | Number of volatility-based groups for stratified batching (Phases 1-2). Ensures each batch contains stocks from all risk profiles, not just one cluster. |\n",
    "| `curriculum_phase1_frac` | Fraction of epochs for Phase 1 (co-movement at full strength + synchronized batching). |\n",
    "| `curriculum_phase2_frac` | Fraction of epochs for Phase 2 (co-movement linearly decaying). Phase 3 = remainder (free refinement, random batching). |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `batch_size` | Batch size for inference pass. Can be larger than training (no gradients stored = less memory). |\n",
    "| `au_threshold` | KL threshold in nats to consider a latent dimension \"active\". 0.01 is standard — dimensions with KL < 0.01 are effectively unused (posterior ≈ prior). |\n",
    "| `r_min` | Minimum observations-per-parameter ratio. Caps AU_max = ⌊√(2·N_obs/r_min)⌋ to prevent more active factors than the data can reliably estimate. |\n",
    "| `aggregation_method` | How to combine predictions from overlapping windows for the same stock. \"mean\" averages all windows. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Risk Model Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `winsorize_lo` / `winsorize_hi` | Percentile bounds for clipping extreme volatility ratios during rescaling. [5, 95] = trim the 5% most extreme values on each side. |\n",
    "| `d_eps_floor` | Minimum idiosyncratic (stock-specific) variance. Prevents division by zero when a stock has near-zero residual risk. |\n",
    "| `conditioning_threshold` | If the covariance matrix condition number exceeds this, the factor regression switches to ridge regression for numerical stability. |\n",
    "| `ridge_scale` | Ridge regularization strength when the fallback activates. Small value (1e-6) = minimal regularization, just enough to stabilize. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Portfolio Optimization Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `lambda_risk` | Risk aversion. Higher = portfolio avoids variance more aggressively, at the cost of lower expected return. |\n",
    "| `w_max` | Hard cap per stock. 0.05 = no stock can exceed 5% of the portfolio. |\n",
    "| `w_min` | Minimum active weight. Stocks allocated below this are eliminated entirely (semi-continuous: either 0 or ≥ w_min). |\n",
    "| `w_bar` | Concentration penalty threshold. Stocks above this weight get penalized to encourage diversification. |\n",
    "| `phi` | Concentration penalty strength. Higher = more aggressively pushes weights below w_bar. |\n",
    "| `kappa_1` / `kappa_2` | Linear and quadratic turnover penalties. Penalize trading costs when rebalancing. Higher = more stable portfolio across rebalances. |\n",
    "| `delta_bar` | Turnover penalty threshold — small weight changes below this are not penalized. |\n",
    "| `tau_max` | Maximum one-way turnover per rebalance. 0.30 = at most 30% of the portfolio can change in a single rebalance. |\n",
    "| `n_starts` | Multi-start initializations for the SCA optimizer. More starts = higher chance of finding the global optimum, but proportionally slower. |\n",
    "| `sca_max_iter` / `sca_tol` | SCA (Sequential Convex Approximation) iteration limit and convergence tolerance. |\n",
    "| `armijo_*` | Line search parameters (sufficient decrease, backtracking factor, max steps). Controls step size selection within SCA. |\n",
    "| `max_cardinality_elim` | Maximum rounds of sequential stock elimination to enforce the minimum weight constraint. |\n",
    "| `entropy_eps` | Tiny constant (1e-30) added inside log() to avoid log(0). Pure numerical safety. |\n",
    "| `alpha_grid` | Grid of α values for the variance-entropy frontier. The optimizer tries each α and picks the best tradeoff between risk and factor diversification. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Walk-Forward Validation Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `total_years` | Total history length used for the walk-forward. More years = more folds = more robust evaluation, but requires more data. |\n",
    "| `min_training_years` | Minimum training window. The first fold starts with this many years of training data. Too small = unreliable model, too large = few folds. |\n",
    "| `oos_months` | Out-of-sample test period per fold. After training, the portfolio is tested on this many months, then the window slides forward. |\n",
    "| `embargo_days` | Gap (trading days) between training end and OOS start. Prevents information leakage from overlapping windows near the boundary. 21 ≈ 1 month. |\n",
    "| `holdout_years` | Final holdout period excluded from all training/testing. Reserved for ultimate out-of-sample validation. |\n",
    "| `val_years` | Nested validation window within training for Phase A hyperparameter selection. Used to score HP configs without touching OOS data. |\n",
    "| `score_lambda_pen` | Weight of maximum drawdown penalty in the composite HP scoring function. Higher = favor configs with lower drawdowns. |\n",
    "| `score_lambda_est` | Weight of estimation quality penalty (variance ratio) in scoring. Higher = favor configs with more accurate risk predictions. |\n",
    "| `score_mdd_threshold` | Maximum drawdown threshold. Drawdowns beyond this are heavily penalized in the composite score. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Quick Mode\n",
    "\n",
    "Run **only this cell** to configure the pipeline for a minimal end-to-end test. Skip Section 2b entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# QUICK MODE — Minimal config for end-to-end testing\n",
    "# Run ONLY this cell, then jump to Section 3\n",
    "# ============================================================\n",
    "\n",
    "if QUICK_MODE == True or DATA_SOURCE == \"synthetic\":\n",
    "    DATA_PATH = \"\"\n",
    "    N_STOCKS = 50   # How many stocks in the universe (ranked by market cap)\n",
    "    N_YEARS = 20    # Years of history to use\n",
    "\n",
    "    config = PipelineConfig(\n",
    "        data=DataPipelineConfig(\n",
    "            n_stocks=N_STOCKS,       # Universe size (top N by market cap)\n",
    "            window_length=504,       # Input window = 504 trading days (~2 years)\n",
    "            n_features=2,            # 2 features per timestep: log-return + realized vol\n",
    "        ),\n",
    "        vae=VAEArchitectureConfig(\n",
    "            K=100,                   # Max latent factors the VAE can discover (only AU will be active)\n",
    "            window_length=504,       # Must match data window_length\n",
    "            n_features=2,            # Must match data n_features\n",
    "            r_max=5.0,               # Max param/data ratio (relaxed for small universes, auto-adjusted)\n",
    "        ),\n",
    "        loss=LossConfig(\n",
    "            mode=\"P\",                # \"P\" = full probabilistic ELBO with learned observation noise sigma^2\n",
    "        ),\n",
    "        training=TrainingConfig(\n",
    "            max_epochs=50,           # Hard limit on training (early stopping may trigger sooner)\n",
    "            batch_size=256,          # Windows per gradient update (256 = good balance speed/stability)\n",
    "            learning_rate=1e-4,      # Initial optimizer step size (typical: 1e-4 to 1e-3)\n",
    "            patience=30,             # Stop if validation doesn't improve for 30 epochs\n",
    "        ),\n",
    "        inference=InferenceConfig(),   # Defaults: batch_size=512, au_threshold=0.01, r_min=2\n",
    "        risk_model=RiskModelConfig(),  # Defaults: winsorize=[5,95], cond_threshold=1e6, ridge=1e-6\n",
    "        portfolio=PortfolioConfig(\n",
    "            n_starts=2,              # Multi-start optimizations (2 = fast, 5+ = production)\n",
    "        ),\n",
    "        walk_forward=WalkForwardConfig(\n",
    "            total_years=N_YEARS,             # Total history for walk-forward folds\n",
    "            min_training_years=max(3, N_YEARS // 3),  # Min training window (>=3y for reliability)\n",
    "            holdout_years=max(1, N_YEARS // 5),       # Final holdout excluded from all folds (>=1y)\n",
    "        ),\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    # Single HP config (skip Phase A grid search for speed)\n",
    "    HP_GRID = [{\"mode\": \"P\", \"learning_rate\": 1e-4, \"alpha\": 1.0}]\n",
    "\n",
    "    print(f\"[Quick mode] {N_STOCKS} stocks, {N_YEARS} years, K={config.vae.K}\")\n",
    "    print(f\"  max_epochs={config.training.max_epochs}, patience={config.training.patience}, HP_GRID=1 config, n_starts=2\")\n",
    "    print(f\"  r_max={config.vae.r_max:.0e}\")\n",
    "    print(f\"  Walk-forward: {config.walk_forward.total_years}y total, \"\n",
    "        f\"{config.walk_forward.min_training_years}y min training, \"\n",
    "        f\"{config.walk_forward.holdout_years}y holdout\")\n",
    "    print(f\"  Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Real Data (Production)\n",
    "\n",
    "Run **all cells below** (through \"ASSEMBLE FULL CONFIG\") for full production configuration. Skip Section 2a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA SOURCE — Real data\n",
    "# ============================================================\n",
    "# For CSV source:\n",
    "DATA_PATH = \"data/stock_data.csv\"  # <-- Set path to your stock data CSV\n",
    "\n",
    "# For Tiingo source: run download first:\n",
    "#   python scripts/download_tiingo.py --phase all --keys-file keys.txt\n",
    "# Then set DATA_SOURCE = \"tiingo\" in Global cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pipeline Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `n_stocks` | Universe size — how many stocks to keep (ranked by market cap). More = better diversification, slower training. |\n",
    "| `window_length` | Length of each input window in trading days. 504 ≈ 2 years of daily data. Each stock produces many overlapping windows. |\n",
    "| `n_features` | Features per timestep. 2 = log-return + realized volatility. |\n",
    "| `vol_window` | Lookback (trading days) for trailing volatility computation. 252 ≈ 1 year. |\n",
    "| `vix_lookback_percentile` | VIX percentile above which a day is labeled \"crisis\". Higher = fewer crisis days = less overweighting. |\n",
    "| `min_valid_fraction` | Minimum fraction of non-missing data to keep a stock. 0.80 = stocks missing >20% of their history are dropped. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA PIPELINE (MOD-001)\n",
    "# ============================================================\n",
    "data_cfg = DataPipelineConfig(\n",
    "    n_stocks=N_STOCKS,           # universe cap (same as N_STOCKS above)\n",
    "    window_length=T,             # T: sliding window length (trading days)\n",
    "    n_features=N_FEATURES,       # F: features per timestep (return + realized vol)\n",
    "    vol_window=252,              # trailing vol lookback (days)\n",
    "    vix_lookback_percentile=80.0,# VIX percentile for crisis threshold\n",
    "    min_valid_fraction=0.80,     # Drop stocks missing >20% of their price history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VAE ARCHITECTURE (MOD-002)\n",
    "# ============================================================\n",
    "vae_cfg = VAEArchitectureConfig(\n",
    "    K=K,                                   # Max latent factors the VAE can discover (only \"active units\" AU<=K are used)\n",
    "    sigma_sq_init=1.0,                     # Initial observation noise (1.0 = assume noise = signal, model learns true value)\n",
    "    sigma_sq_min=1e-4,                     # Lower clamp for sigma^2 (prevents overfitting: model can't claim 0 noise)\n",
    "    sigma_sq_max=10.0,                     # Upper clamp for sigma^2 (prevents divergence: model can't give up entirely)\n",
    "    window_length=data_cfg.window_length,  # Must match data_cfg.window_length\n",
    "    n_features=data_cfg.n_features,        # Must match data_cfg.n_features\n",
    "    r_max=5.0,                             # Max model params / data points ratio (safety guard against overfitting)\n",
    ")\n",
    "\n",
    "print(f\"Encoder depth L={vae_cfg.encoder_depth}, \"\n",
    "      f\"Final width C_L={vae_cfg.final_layer_width}, \"\n",
    "      f\"D={vae_cfg.D}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOSS FUNCTION (MOD-004)\n",
    "# ============================================================\n",
    "loss_cfg = LossConfig(\n",
    "    mode=LOSS_MODE,                        # Loss mode: \"P\"=full ELBO with learned sigma^2, \"F\"=simplified with beta warmup, \"A\"=hybrid\n",
    "    gamma=CRISIS_OVERWEIGHTING,            # Crisis overweighting: crisis windows count 3x more in reconstruction loss\n",
    "    lambda_co_max=MAX_CO_MOVEMENT_WEIGHT,  # Max co-movement weight: how strongly latent distances must match Spearman correlations\n",
    "    beta_fixed=1.0,                        # Fixed KL weight for Mode A (must be 1.0 for Mode P)\n",
    "    warmup_fraction=0.20,                  # Mode F only: fraction of epochs to ramp beta 0->1 (prevents posterior collapse)\n",
    "    max_pairs=2048,                        # Max stock pairs per batch for co-movement loss (limits O(B^2) compute)\n",
    "    delta_sync=21,                         # Max date gap (days) for windows to be \"synchronized\" (21 ≈ 1 month)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING (MOD-005)\n",
    "# ============================================================\n",
    "training_cfg = TrainingConfig(\n",
    "    max_epochs=MAX_EPOCHS,            # Hard limit on training duration (early stopping usually triggers sooner)\n",
    "    batch_size=BATCH_SIZE,            # Windows per gradient step (larger = smoother but more memory)\n",
    "    learning_rate=LEARNING_RATE,      # Initial optimizer step size (too high = diverges, too low = slow)\n",
    "    weight_decay=1e-5,                # L2 penalty on weights to reduce overfitting (1e-5 = mild)\n",
    "    adam_betas=(0.9, 0.999),          # Adam momentum parameters (rarely need tuning)\n",
    "    adam_eps=1e-8,                    # Adam numerical stability constant (rarely need tuning)\n",
    "    patience=EARLY_STOPPING_PATIENCE, # Early stopping: stop after 10 epochs without validation improvement\n",
    "    lr_patience=5,                    # Reduce LR if validation stagnates for 5 epochs (triggers before early stop)\n",
    "    lr_factor=0.5,                    # LR multiplier when reducing (0.5 = halve each time)\n",
    "    n_strata=15,                      # Volatility groups for stratified batching (ensures risk diversity per batch)\n",
    "    curriculum_phase1_frac=0.30,      # Phase 1: 30% of epochs with full co-movement + synchronized batching\n",
    "    curriculum_phase2_frac=0.30,      # Phase 2: 30% of epochs with co-movement linearly decaying to 0\n",
    "                                      # Phase 3: remaining 40% with no co-movement, random batching\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INFERENCE (MOD-006)\n",
    "# After training, the encoder runs on all windows to extract each stock's\n",
    "# latent profile. These parameters control that extraction pass and\n",
    "# decide how many latent dimensions are kept as \"active\" risk factors.\n",
    "# ============================================================\n",
    "inference_cfg = InferenceConfig(\n",
    "    batch_size=512,                          # Larger than training (no gradients = less memory needed)\n",
    "    au_threshold=AU_THRESHOLD,               # KL > 0.01 nats = dimension is \"active\" (below = unused, posterior ≈ prior)\n",
    "    r_min=2,                                 # Min data/param ratio — caps AU_max to prevent more factors than data supports\n",
    "    aggregation_method=AGGREGATION_METHOD,   # Average overlapping window predictions for each stock's final profile\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RISK MODEL (MOD-007)\n",
    "# Transforms latent exposures (B matrix) into a full covariance\n",
    "# matrix Sigma_assets via cross-sectional regression and\n",
    "# Ledoit-Wolf shrinkage. These parameters handle numerical\n",
    "# edge cases (extreme vol ratios, ill-conditioned matrices).\n",
    "# ============================================================\n",
    "risk_model_cfg = RiskModelConfig(\n",
    "    winsorize_lo=5.0,            # Clip extreme vol ratios below 5th percentile (removes outliers)\n",
    "    winsorize_hi=95.0,           # Clip extreme vol ratios above 95th percentile\n",
    "    d_eps_floor=1e-6,            # Min idiosyncratic variance per stock (prevents division by zero)\n",
    "    conditioning_threshold=1e6,  # Switch to ridge regression if covariance condition number exceeds this\n",
    "    ridge_scale=1e-6,            # Ridge regularization strength (tiny, just enough to stabilize)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PORTFOLIO OPTIMIZATION (MOD-008)\n",
    "# Constraints identical for VAE and all 6 benchmarks (INV-012)\n",
    "#\n",
    "# Variance (w'Σw) = total portfolio risk. Minimizing it concentrates\n",
    "# weight on low-vol stocks — but can put 85% of risk on 2-3 latent\n",
    "# factors without noticing (fragile to factor shocks).\n",
    "#\n",
    "# Entropy H(w) = Shannon entropy on each factor's risk contribution.\n",
    "# Maximizing it spreads risk evenly across all active factors — no\n",
    "# single factor dominates (resilient to individual factor crashes).\n",
    "#\n",
    "# lambda_risk and alpha control the tradeoff: more lambda_risk favors\n",
    "# low total variance; more alpha favors even factor risk distribution.\n",
    "#\n",
    "# lambda_risk guide:\n",
    "#   0.1  → entropy dominates (pure factor risk parity, higher variance OK)\n",
    "#   0.5  → entropy favored, soft variance constraint\n",
    "#   1.0  → balanced (default, let alpha_grid find the right ratio)\n",
    "#   2.0  → variance favored (more conservative, fewer dominant factors OK)\n",
    "#   5.0+ → near min-variance (entropy has little influence)\n",
    "#\n",
    "# Penalty tuning guide (defaults calibrated for US mid/large cap):\n",
    "#   Illiquid universe (small caps) → raise kappa_1 & kappa_2\n",
    "#   Infrequent rebalancing         → lower kappa_1 & kappa_2\n",
    "#   Allow more concentrated bets   → lower phi and/or raise w_bar\n",
    "#   Force tighter diversification  → raise phi and/or lower w_bar\n",
    "#   Smoother trades                → lower delta_bar (e.g. 0.005)\n",
    "#\n",
    "# Valid ranges from DVT spec:\n",
    "#   phi: [5, 100]  kappa_1: [0.01, 1.0]  kappa_2: [1, 50]  delta_bar: [0.005, 0.03]\n",
    "# ============================================================\n",
    "portfolio_cfg = PortfolioConfig(\n",
    "    lambda_risk=LAMBDA_RISK,     # Risk aversion (higher = more conservative, lower variance portfolio)\n",
    "    w_max=W_MAX,                 # Max 5% per stock (hard cap, prevents single-stock concentration)\n",
    "    w_min=W_MIN,                 # Min active weight: below this, stock is eliminated (0 or >= 0.1%)\n",
    "    w_bar=W_BAR,                 # Concentration penalty kicks in above 3% weight per stock\n",
    "    phi=PHI,                     # Concentration penalty strength above w_bar (higher = stronger diversification pressure)\n",
    "    kappa_1=KAPPA_1,             # Linear turnover penalty (penalizes trading costs at rebalance)\n",
    "    kappa_2=KAPPA_2,             # Quadratic turnover penalty (penalizes large trades more than small ones)\n",
    "    delta_bar=DELTA_BAR,         # Turnover below 1% is not penalized (de minimis threshold)\n",
    "    tau_max=MAX_TURNOVER,        # Max 30% portfolio change per rebalance (hard cap on turnover)\n",
    "    n_starts=SCA_N_STARTS,       # Multi-start optimizations (more = better optimum, proportionally slower)\n",
    "    sca_max_iter=100,            # Max iterations for the SCA convex optimizer\n",
    "    sca_tol=1e-8,                # SCA convergence tolerance (stop when improvement < this)\n",
    "    armijo_c=1e-4,               # Line search: sufficient decrease constant\n",
    "    armijo_rho=0.5,              # Line search: backtracking factor (halve step on each retry)\n",
    "    armijo_max_iter=20,          # Line search: max backtracking attempts per SCA step\n",
    "    max_cardinality_elim=100,    # Max rounds of stock elimination to enforce w_min constraint\n",
    "    entropy_eps=1e-30,           # Tiny constant in log() for numerical safety (avoids log(0))\n",
    "    alpha_grid=[0.0, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0],  # Risk-entropy tradeoff grid (optimizer picks best alpha)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# WALK-FORWARD VALIDATION (MOD-009)\n",
    "# ============================================================\n",
    "walk_forward_cfg = WalkForwardConfig(\n",
    "    total_years=N_YEARS,            # Total history used (more years = more folds = more robust evaluation)\n",
    "    min_training_years=MIN_N_YEARS, # First fold trains on at least MIN_N_YEARS (shorter = unreliable model)\n",
    "    oos_months=6,                   # Test each fold on 6 months out-of-sample, then slide forward\n",
    "    embargo_days=21,                # 21-day gap between training and test (prevents data leakage from windows)\n",
    "    holdout_years=3,                # Last 3 years excluded from all folds (final out-of-sample validation)\n",
    "    val_years=2,                    # 2-year nested validation within training for HP selection (Phase A)\n",
    "    score_lambda_pen=5.0,           # HP scoring: penalty weight for max drawdown (higher = favor low-drawdown configs)\n",
    "    score_lambda_est=2.0,           # HP scoring: penalty weight for poor risk estimation accuracy\n",
    "    score_mdd_threshold=0.20,       # HP scoring: drawdowns above 20% get heavily penalized\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ASSEMBLE FULL CONFIG\n",
    "# ============================================================\n",
    "if DATA_SOURCE != \"synthetic\" and QUICK_MODE == False:\n",
    "      config = PipelineConfig(\n",
    "            data=data_cfg,\n",
    "            vae=vae_cfg,\n",
    "            loss=loss_cfg,\n",
    "            training=training_cfg,\n",
    "            inference=inference_cfg,\n",
    "            risk_model=risk_model_cfg,\n",
    "            portfolio=portfolio_cfg,\n",
    "            walk_forward=walk_forward_cfg,\n",
    "            seed=SEED,\n",
    "      )\n",
    "\n",
    "      print(\"PipelineConfig assembled.\")\n",
    "      print(f\"  Walk-forward: {config.walk_forward.total_years}y total, \"\n",
    "            f\"{config.walk_forward.min_training_years}y min training, \"\n",
    "            f\"{config.walk_forward.holdout_years}y holdout\")\n",
    "      print(f\"  VAE: K={config.vae.K}, T={config.vae.window_length}, F={config.vae.n_features}\")\n",
    "      print(f\"  Training: {config.training.max_epochs} max epochs, \"\n",
    "            f\"bs={config.training.batch_size}, lr={config.training.learning_rate}\")\n",
    "      print(f\"  Loss mode: {config.loss.mode}, gamma={config.loss.gamma}\")\n",
    "      print(f\"  Capacity guard r_max: {config.vae.r_max}\")\n",
    "      print(f\"  Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Tiingo Download (run once)\n",
    "\n",
    "Run this cell **only once** to download Tiingo data. After the first run, the data is saved locally and reused automatically.\n",
    "Set `MAX_TICKERS` to a small number (e.g. 5) for testing, or `None` for the full universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TIINGO DOWNLOAD — Run once, data is saved locally\n",
    "# Skip this cell if data is already downloaded or DATA_SOURCE != \"tiingo\"\n",
    "# ============================================================\n",
    "\n",
    "if DATA_SOURCE == \"tiingo\":\n",
    "    import importlib.util\n",
    "\n",
    "    _spec = importlib.util.spec_from_file_location(\n",
    "        \"download_tiingo\", str(PROJECT_ROOT / \"scripts\" / \"download_tiingo.py\")\n",
    "    )\n",
    "    _mod = importlib.util.module_from_spec(_spec)\n",
    "    _spec.loader.exec_module(_mod)  # type: ignore[union-attr]\n",
    "\n",
    "    MAX_TICKERS = None  # Set to None for full universe (~22k tickers)\n",
    "\n",
    "    _mod.run_download(\n",
    "        api_keys=TIINGO_API_KEYS,\n",
    "        data_dir=DATA_DIR,\n",
    "        max_tickers=MAX_TICKERS,\n",
    "        sp500_first=True,  # Download SP500 tickers first (priority)\n",
    "    )\n",
    "else:\n",
    "    print(f\"DATA_SOURCE={DATA_SOURCE}, skipping Tiingo download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "\n",
    "stock_data, start_date = load_data_source(\n",
    "    source=DATA_SOURCE,\n",
    "    data_path=DATA_PATH if DATA_SOURCE == \"csv\" else \"\",\n",
    "    data_dir=DATA_DIR,\n",
    "    n_stocks=N_STOCKS,\n",
    "    n_years=N_YEARS,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Data source: {DATA_SOURCE}\")\n",
    "print(f\"Stock data shape: {stock_data.shape}\")\n",
    "print(f\"Date range: {stock_data['date'].min()} to {stock_data['date'].max()}\")\n",
    "print(f\"Unique stocks: {stock_data['permno'].nunique()}\")\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log-returns and trailing volatility\n",
    "returns = compute_log_returns(stock_data)\n",
    "trailing_vol = compute_trailing_volatility(returns, window=config.data.vol_window)\n",
    "\n",
    "print(f\"Returns: {returns.shape[0]} dates x {returns.shape[1]} stocks\")\n",
    "print(f\"Trailing vol: {trailing_vol.shape} (first {config.data.vol_window-1} rows NaN)\")\n",
    "print(f\"Returns date range: {returns.index[0]} to {returns.index[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Run Pipeline\n",
    "\n",
    "Executes the full walk-forward validation: Phase A (HP selection) + Phase B (deployment) on each fold, then benchmarks, statistical tests, and report generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard Monitoring\n",
    "\n",
    "Launch TensorBoard in a terminal to monitor training in real-time:\n",
    "\n",
    "```bash\n",
    ".venv/bin/tensorboard --logdir runs/\n",
    "```\n",
    "\n",
    "Logs are organized as `runs/fold_XX/phase_a/config_YY_mode_M_lr_L/` and `runs/fold_XX/phase_b/`.\n",
    "\n",
    "### Metric Value Interpretation\n",
    "\n",
    "The input windows are **z-scored** (mean=0, std=1), so metric values are directly interpretable as fractions of signal variance.\n",
    "\n",
    "#### `reconstruction` (Step/ and Loss/)\n",
    "Raw per-element MSE **before** D/(2σ²) scaling. Since data is z-scored, L_recon ≈ fraction of unexplained variance.\n",
    "\n",
    "| L_recon | RMSE | Interpretation |\n",
    "|---|---|---|\n",
    "| **1.0** | 1.0 | No better than predicting the mean. Not learning. |\n",
    "| **0.5** | 0.71 | ~50% variance explained. Poor. |\n",
    "| **0.1** | 0.32 | ~90% variance explained. Decent. |\n",
    "| **0.05** | 0.22 | ~95% explained. Good for financial data. |\n",
    "| **0.01** | 0.10 | ~99% explained. Likely overfitting on noisy data. |\n",
    "\n",
    "Good converged range on real data: **0.05–0.20**.\n",
    "\n",
    "#### `kl_divergence` (Step/ and Loss/)\n",
    "KL **summed over K dimensions**, averaged over batch. Raw value scales linearly with K — divide by K for per-dimension KL (nats).\n",
    "\n",
    "| L_KL (K=100) | KL/dim | Interpretation |\n",
    "|---|---|---|\n",
    "| **0** | 0 | **Posterior collapse.** Encoder ignores input entirely. |\n",
    "| **1** | 0.01 | Right at AU threshold. Quasi-collapsed. |\n",
    "| **10** | 0.1 | ~10 weakly active dimensions. Most collapsed. |\n",
    "| **50** | 0.5 | ~50 active dims, moderate info per dim. Typical early-mid training. |\n",
    "| **100** | 1.0 | ~100 active dims at ~1 nat each. Healthy, full capacity used. |\n",
    "| **250** | 2.5 | Very informative. If AU ≪ 100, a few dims are dominating. |\n",
    "\n",
    "Good converged range (K=100): **25–150**. For K=200, double these values.\n",
    "\n",
    "#### `co_movement` (Step/ and Loss/)\n",
    "MSE between latent cosine distances and Spearman correlation targets: `(1/|P|) × Σ (d_cos(μ_i, μ_j) − (1 − ρ_ij))²`.\n",
    "\n",
    "| L_co | Avg error | Interpretation |\n",
    "|---|---|---|\n",
    "| **0.01** | 0.10 | Excellent alignment between latent distances and Spearman. |\n",
    "| **0.05** | 0.22 | Good. Reasonable after Phase 1. |\n",
    "| **0.10** | 0.32 | Moderate. Early training territory. |\n",
    "| **0.30** | 0.55 | Poor. Random initialization level. |\n",
    "| **> 0.5** | > 0.7 | Latent space disorganized w.r.t. co-movements. |\n",
    "\n",
    "**Key: this value is only meaningful when λ_co > 0 (Phases 1-2).** In Phase 3, the value is 0.0 because `compute_co_movement_loss` is skipped entirely.\n",
    "\n",
    "#### `sigma_sq` (Step/ and Loss/)\n",
    "Learned observation noise σ² = clamp(exp(log_sigma_sq), 1e-4, 10). **At equilibrium in Mode P, σ² converges to ≈ L_recon** (the model's own reconstruction MSE).\n",
    "\n",
    "| σ² | Variance explained | Interpretation |\n",
    "|---|---|---|\n",
    "| **10.0** (clamp max) | ~0% | Model giving up on reconstruction. Diverging. |\n",
    "| **1.0** | ~0% | Noise = signal. Initial state / Mode F (frozen). |\n",
    "| **0.3–0.5** | 50–70% | Early-mid convergence. |\n",
    "| **0.05–0.2** | 80–95% | Healthy convergence on financial data. |\n",
    "| **0.01** | ~99% | Possibly overfitting. |\n",
    "| **1e-4** (clamp min) | ~100% | Hitting floor. Overfitting. |\n",
    "\n",
    "Mode F: σ² is **frozen at 1.0** — ignore this metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/Latent_risk_factor/runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_FULL_PIPELINE:\n",
    "      print(\"=\" * 60)\n",
    "      print(\"PIPELINE CONFIGURATION SUMMARY\")\n",
    "      print(\"=\" * 60)\n",
    "      print(f\"  Seed: {SEED} | Device: {DEVICE} | Data: {DATA_SOURCE}\")\n",
    "      print()\n",
    "      print(f\"  [Data]      N_STOCKS={N_STOCKS}, T={T}, N_FEATURES={N_FEATURES}, N_YEARS={N_YEARS}\")\n",
    "      print(f\"  [VAE]       K={K}, LOSS_MODE={LOSS_MODE}\")\n",
    "      print(f\"  [Training]  MAX_EPOCHS={MAX_EPOCHS}, BATCH_SIZE={BATCH_SIZE}, LEARNING_RATE={LEARNING_RATE}, EARLY_STOPPING_PATIENCE={EARLY_STOPPING_PATIENCE}\")\n",
    "      print(f\"  [Loss]      CRISIS_OVERWEIGHTING={CRISIS_OVERWEIGHTING}, MAX_CO_MOVEMENT_WEIGHT={MAX_CO_MOVEMENT_WEIGHT}\")\n",
    "      print(f\"  [Portfolio]  LAMBDA_RISK={LAMBDA_RISK}, W_MAX={W_MAX}, W_MIN={W_MIN}, SCA_N_STARTS={SCA_N_STARTS}\")\n",
    "      print(f\"  [HP Grid]   {len(HP_GRID) if HP_GRID else '18 (default)'} configs\")\n",
    "      print(\"=\" * 60)\n",
    "\n",
    "      TB_DIR = \"runs/\"  # TensorBoard log directory (set to None to disable)\n",
    "\n",
    "      pipeline = FullPipeline(config, tensorboard_dir=TB_DIR)\n",
    "\n",
    "      skip_phase_a = True\n",
    "\n",
    "      results = pipeline.run(\n",
    "      stock_data=stock_data,\n",
    "      returns=returns,\n",
    "      trailing_vol=trailing_vol,\n",
    "      skip_phase_a=(DATA_SOURCE == \"synthetic\" or QUICK_MODE == True or skip_phase_a),\n",
    "      vix_data=None,\n",
    "      start_date=start_date,\n",
    "      hp_grid=HP_GRID,\n",
    "      device=DEVICE,\n",
    "      )\n",
    "\n",
    "      print(\"Pipeline complete.\")\n",
    "      print(f\"Folds processed: {len(results['vae_results'])}\")\n",
    "      print(f\"Benchmarks: {list(results['benchmark_results'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4b. Direct Training (Skip Walk-Forward)\n",
    "\n",
    "Train the VAE on the entire period minus a holdout, then evaluate on the holdout.\n",
    "**Run EITHER Section 4 (walk-forward) OR Section 4b (direct) -- not both.**\n",
    "\n",
    "Use this mode for:\n",
    "- Quick iteration during development\n",
    "- Single-split evaluation before committing to full walk-forward\n",
    "- Inspecting model internals (B matrix, latent factors, risk model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RUN DIRECT TRAINING\n",
    "# ============================================================\n",
    "if not RUN_FULL_PIPELINE:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PIPELINE CONFIGURATION SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Seed: {SEED} | Device: {DEVICE} | Data: {DATA_SOURCE}\")\n",
    "    print()\n",
    "    print(f\"  [Data]      N_STOCKS={N_STOCKS}, T={T}, N_FEATURES={N_FEATURES}, N_YEARS={N_YEARS}\")\n",
    "    print(f\"  [VAE]       K={K}, LOSS_MODE={LOSS_MODE}\")\n",
    "    print(f\"  [Training]  MAX_EPOCHS={MAX_EPOCHS}, BATCH_SIZE={BATCH_SIZE}, LEARNING_RATE={LEARNING_RATE}, EARLY_STOPPING_PATIENCE={EARLY_STOPPING_PATIENCE}\")\n",
    "    print(f\"  [Loss]      CRISIS_OVERWEIGHTING={CRISIS_OVERWEIGHTING}, MAX_CO_MOVEMENT_WEIGHT={MAX_CO_MOVEMENT_WEIGHT}\")\n",
    "    print(f\"  [Portfolio]  LAMBDA_RISK={LAMBDA_RISK}, W_MAX={W_MAX}, W_MIN={W_MIN}, SCA_N_STARTS={SCA_N_STARTS}\")\n",
    "    print(f\"  [HP Grid]   {len(HP_GRID) if HP_GRID else '18 (default)'} configs\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    HOLDOUT_START = None  # Option A: Set an explicit holdout start date (e.g. \"2023-01-03\") — set to None for automatic split\n",
    "    HOLDOUT_FRACTION = 0.10  # Option B: Fraction of dates reserved for holdout (ignored if HOLDOUT_START is set)\n",
    "    RUN_BENCHMARKS = True # Run benchmarks on the same split for comparison?\n",
    "\n",
    "    TB_DIR = \"runs/\"\n",
    "\n",
    "    pipeline = FullPipeline(config, tensorboard_dir=TB_DIR)\n",
    "\n",
    "    results = pipeline.run_direct(\n",
    "        stock_data=stock_data,\n",
    "        returns=returns,\n",
    "        trailing_vol=trailing_vol,\n",
    "        vix_data=None,\n",
    "        start_date=start_date,\n",
    "        hp_grid=HP_GRID,\n",
    "        device=DEVICE,\n",
    "        holdout_start=HOLDOUT_START,\n",
    "        holdout_fraction=HOLDOUT_FRACTION,\n",
    "        run_benchmarks=RUN_BENCHMARKS,\n",
    "    )\n",
    "\n",
    "    print(\"Direct training complete.\")\n",
    "    print(f\"  Train: {results['fold_schedule'][0]['train_start']} to {results['train_end']}\")\n",
    "    print(f\"  Test:  {results['oos_start']} to {results['oos_end']}\")\n",
    "    print(f\"  Sharpe: {results['vae_results'][0].get('sharpe', 0.0):.3f}\")\n",
    "    print(f\"  AU: {results['vae_results'][0].get('AU', 0):.0f}\")\n",
    "    print(f\"  E* (best epoch): {results['vae_results'][0].get('e_star', 0):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING HISTORY\n",
    "# ============================================================\n",
    "if \"state\" in results and \"fit_result\" in results[\"state\"] and not(RUN_FULL_PIPELINE):\n",
    "    history = results[\"state\"][\"fit_result\"][\"history\"]\n",
    "    history_df = pd.DataFrame(history)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "    axes[0, 0].plot(history_df[\"train_loss\"], label=\"Train Loss\")\n",
    "    axes[0, 0].plot(history_df[\"val_elbo\"], label=\"Val ELBO\")\n",
    "    axes[0, 0].set_title(\"Loss Curves\")\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0, 1].plot(history_df[\"train_recon\"], label=\"Reconstruction\")\n",
    "    axes[0, 1].plot(history_df[\"train_kl\"], label=\"KL Divergence\")\n",
    "    axes[0, 1].set_title(\"Loss Components\")\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1, 0].plot(history_df[\"AU\"], color=\"#2563eb\")\n",
    "    axes[1, 0].set_title(\"Active Units (AU)\")\n",
    "    axes[1, 0].set_ylabel(\"AU\")\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1, 1].plot(history_df[\"sigma_sq\"], color=\"#dc2626\")\n",
    "    axes[1, 1].set_title(\"Observation Noise (sigma^2)\")\n",
    "    axes[1, 1].set_ylabel(\"sigma^2\")\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "    fig.suptitle(\"Direct Training History\", fontsize=13, fontweight=\"bold\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Training history not available (state bag not populated).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL INSPECTION\n",
    "# ============================================================\n",
    "if \"state\" in results and not(RUN_FULL_PIPELINE):\n",
    "    state = results[\"state\"]\n",
    "    B_A = state.get(\"B_A\")\n",
    "    stock_ids = state.get(\"inferred_stock_ids\", [])\n",
    "    AU = state.get(\"AU\", 0)\n",
    "\n",
    "    if B_A is not None and B_A.shape[1] > 0:\n",
    "        n_show = min(20, B_A.shape[1])\n",
    "        fig, ax = plt.subplots(figsize=(12, max(4, len(stock_ids) * 0.15)))\n",
    "        im = ax.imshow(B_A[:, :n_show], aspect=\"auto\", cmap=\"RdBu_r\")\n",
    "        ax.set_xlabel(f\"Active Latent Dimension (showing {n_show}/{AU})\")\n",
    "        ax.set_ylabel(\"Stock\")\n",
    "        ax.set_title(f\"Exposure Matrix B_A ({B_A.shape[0]} stocks x {AU} active dims)\")\n",
    "        fig.colorbar(im, ax=ax)\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        kl = state.get(\"kl_per_dim\")\n",
    "        if kl is not None:\n",
    "            fig, ax = plt.subplots(figsize=(10, 3))\n",
    "            ax.bar(range(len(kl)), np.sort(kl)[::-1], color=\"#2563eb\", alpha=0.7)\n",
    "            ax.axhline(0.01, color=\"#dc2626\", linestyle=\"--\", label=\"AU threshold (0.01)\")\n",
    "            ax.set_xlabel(\"Dimension (sorted by KL)\")\n",
    "            ax.set_ylabel(\"Marginal KL (nats)\")\n",
    "            ax.set_title(f\"Latent Dimension Usage — AU={AU}/{len(kl)}\")\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"No state available (run Section 4b first).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Results - Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text summary\n",
    "print(format_summary_table(results[\"report\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployment recommendation\n",
    "deployment = results[\"report\"][\"deployment\"]\n",
    "print(f\"Scenario: {deployment['scenario']}\")\n",
    "print(f\"Recommendation: {deployment['recommendation']}\")\n",
    "print()\n",
    "print(\"Per-benchmark wins (VAE vs benchmark on primary metrics):\")\n",
    "for bench, info in deployment[\"per_benchmark\"].items():\n",
    "    print(f\"  {bench:20s}: {info['wins']}/{info['total']} metrics won\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE summary statistics\n",
    "vae_df = aggregate_fold_metrics(results[\"vae_results\"])\n",
    "vae_summary = summary_statistics(vae_df)\n",
    "print(\"VAE Summary Statistics:\")\n",
    "style_summary_table(vae_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark summary statistics\n",
    "for bench_name, bench_metrics in results[\"benchmark_results\"].items():\n",
    "    bench_df = aggregate_fold_metrics(bench_metrics)\n",
    "    bench_summary = summary_statistics(bench_df)\n",
    "    print(f\"\\n{bench_name} Summary:\")\n",
    "    display(style_summary_table(bench_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Results - Per-Fold Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE per-fold metrics\n",
    "print(\"VAE Per-Fold Metrics:\")\n",
    "style_fold_table(vae_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E* distribution\n",
    "e_star_summary = results[\"report\"][\"e_star_summary\"]\n",
    "print(f\"E* epochs: mean={e_star_summary['mean']:.1f}, \"\n",
    "      f\"std={e_star_summary['std']:.1f}, \"\n",
    "      f\"range=[{e_star_summary['min']}, {e_star_summary['max']}]\")\n",
    "\n",
    "plot_e_star_distribution(results[\"e_stars\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fold metrics: VAE vs benchmarks\n",
    "plot_fold_metrics(results[\"vae_results\"], results[\"benchmark_results\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Results - Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise tests heatmap\n",
    "plot_pairwise_heatmap(results[\"report\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed pairwise test results\n",
    "tests = results[\"report\"][\"statistical_tests\"]\n",
    "print(f\"Total comparisons: {tests['n_tests']} (alpha={tests['alpha']})\")\n",
    "print()\n",
    "\n",
    "for bench_name, metrics in tests[\"pairwise\"].items():\n",
    "    print(f\"VAE vs {bench_name}:\")\n",
    "    for metric, result in metrics.items():\n",
    "        if result.get(\"skipped\", False):\n",
    "            print(f\"  {metric}: skipped ({result['reason']})\")\n",
    "            continue\n",
    "        sig = \" *\" if result.get(\"significant_corrected\", False) else \"\"\n",
    "        print(f\"  {metric}: delta={result['median_delta']:+.4f} \"\n",
    "              f\"[{result['ci_lower']:+.4f}, {result['ci_upper']:+.4f}] \"\n",
    "              f\"p={result.get('p_corrected', result['p_value']):.4f}{sig}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"results/\"\n",
    "\n",
    "written = export_results(results, asdict(config), output_dir=OUTPUT_DIR)\n",
    "\n",
    "print(f\"Results saved to {OUTPUT_DIR}\")\n",
    "for path in written:\n",
    "    print(f\"  {os.path.basename(path)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Latent_risk_factor (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
