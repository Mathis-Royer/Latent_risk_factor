{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Latent Risk Factor - Pipeline Dashboard\n",
    "\n",
    "Central configuration and execution notebook for the full walk-forward validation pipeline.\n",
    "\n",
    "**Workflow:**\n",
    "1. Configure all parameters (Sections 1-2)\n",
    "2. Load data (Section 3)\n",
    "3. Run pipeline (Section 4)\n",
    "4. Inspect results (Sections 5-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathis/Latent_risk_factor/Latent_risk_factor/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-02-08 01:23:52,630 [INFO] src.utils: Device auto-detected: MPS (Apple Silicon)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 2.10.0 | Device: mps\n",
      "Working directory: /Users/mathis/Latent_risk_factor/Latent_risk_factor\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from dataclasses import replace, asdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project root: go up from notebooks/ to project root\n",
    "_NB_DIR = Path(os.path.abspath(\"\")).resolve()\n",
    "PROJECT_ROOT = (_NB_DIR / \"..\").resolve() if _NB_DIR.name == \"notebooks\" else _NB_DIR\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.config import (\n",
    "    PipelineConfig,\n",
    "    DataPipelineConfig,\n",
    "    VAEArchitectureConfig,\n",
    "    LossConfig,\n",
    "    TrainingConfig,\n",
    "    InferenceConfig,\n",
    "    RiskModelConfig,\n",
    "    PortfolioConfig,\n",
    "    WalkForwardConfig,\n",
    ")\n",
    "from src.data_pipeline.data_loader import load_data_source\n",
    "from src.data_pipeline.returns import compute_log_returns\n",
    "from src.data_pipeline.features import compute_trailing_volatility\n",
    "from src.integration.pipeline import FullPipeline\n",
    "from src.integration.reporting import export_results, format_summary_table\n",
    "from src.integration.visualization import (\n",
    "    plot_fold_metrics,\n",
    "    plot_e_star_distribution,\n",
    "    plot_pairwise_heatmap,\n",
    "    style_summary_table,\n",
    "    style_fold_table,\n",
    ")\n",
    "from src.utils import get_optimal_device\n",
    "from src.walk_forward.selection import aggregate_fold_metrics, summary_statistics\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\")\n",
    "logger = logging.getLogger(\"dashboard\")\n",
    "\n",
    "print(f\"PyTorch {torch.__version__} | Device: {get_optimal_device()}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration\n",
    "\n",
    "Two configuration profiles are available. **Run ONLY one section:**\n",
    "- **Section 2a** — Synthetic data: minimal parameters for quick end-to-end testing\n",
    "- **Section 2b** — Real data: full production configuration\n",
    "\n",
    "Always run the **Global** cell (below) first, then choose ONE section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 01:23:52,636 [INFO] src.utils: Device auto-detected: MPS (Apple Silicon)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# GLOBAL\n",
    "# ============================================================\n",
    "SEED = 42\n",
    "DEVICE = str(get_optimal_device())\n",
    "\n",
    "# Data source: \"synthetic\", \"tiingo\", or \"csv\"\n",
    "DATA_SOURCE = \"tiingo\"\n",
    "QUICK_MODE = True  # Set True for minimal config even with real data\n",
    "\n",
    "# Tiingo API keys (used when DATA_SOURCE = \"tiingo\")\n",
    "TIINGO_API_KEYS = [\n",
    "    \"9ba6e57788deaac3b3c38ed47047cabbbd6077e2\",\n",
    "    \"9aad315d49275c400687f41dd26b22328d8b1a26\",\n",
    "]\n",
    "DATA_DIR = \"data/\"  # Directory for Tiingo downloaded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Quick Mode\n",
    "\n",
    "Run **only this cell** to configure the pipeline for a minimal end-to-end test. Skip Section 2b entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# QUICK MODE — Minimal config for end-to-end testing\n# Run ONLY this cell, then jump to Section 3\n# ============================================================\n\nif QUICK_MODE == True or DATA_SOURCE == \"synthetic\":\n    DATA_PATH = \"\"\n    N_STOCKS = 50   # How many stocks in the universe (ranked by market cap)\n    N_YEARS = 20    # Years of history to use\n\n    config = PipelineConfig(\n        data=DataPipelineConfig(\n            n_stocks=N_STOCKS,       # Universe size (top N by market cap)\n            window_length=504,       # Input window = 504 trading days (~2 years)\n            n_features=2,            # 2 features per timestep: log-return + realized vol\n        ),\n        vae=VAEArchitectureConfig(\n            K=100,                   # Max latent factors the VAE can discover (only AU will be active)\n            window_length=504,       # Must match data window_length\n            n_features=2,            # Must match data n_features\n            r_max=5.0,               # Max param/data ratio (relaxed for small universes, auto-adjusted)\n        ),\n        loss=LossConfig(\n            mode=\"P\",                # \"P\" = full probabilistic ELBO with learned observation noise sigma^2\n        ),\n        training=TrainingConfig(\n            max_epochs=50,           # Hard limit on training (early stopping may trigger sooner)\n            batch_size=256,          # Windows per gradient update (256 = good balance speed/stability)\n            learning_rate=1e-4,      # Initial optimizer step size (typical: 1e-4 to 1e-3)\n            patience=30,             # Stop if validation doesn't improve for 30 epochs\n        ),\n        inference=InferenceConfig(),   # Defaults: batch_size=512, au_threshold=0.01, r_min=2\n        risk_model=RiskModelConfig(),  # Defaults: winsorize=[5,95], cond_threshold=1e6, ridge=1e-6\n        portfolio=PortfolioConfig(\n            n_starts=2,              # Multi-start optimizations (2 = fast, 5+ = production)\n        ),\n        walk_forward=WalkForwardConfig(\n            total_years=N_YEARS,             # Total history for walk-forward folds\n            min_training_years=max(3, N_YEARS // 3),  # Min training window (>=3y for reliability)\n            holdout_years=max(1, N_YEARS // 5),       # Final holdout excluded from all folds (>=1y)\n        ),\n        seed=SEED,\n    )\n\n    # Single HP config (skip Phase A grid search for speed)\n    HP_GRID = [{\"mode\": \"P\", \"learning_rate\": 1e-4, \"alpha\": 1.0}]\n\n    print(f\"[Quick mode] {N_STOCKS} stocks, {N_YEARS} years, K={config.vae.K}\")\n    print(f\"  max_epochs={config.training.max_epochs}, patience={config.training.patience}, HP_GRID=1 config, n_starts=2\")\n    print(f\"  r_max={config.vae.r_max:.0e}\")\n    print(f\"  Walk-forward: {config.walk_forward.total_years}y total, \"\n        f\"{config.walk_forward.min_training_years}y min training, \"\n        f\"{config.walk_forward.holdout_years}y holdout\")\n    print(f\"  Device: {DEVICE}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Real Data (Production)\n",
    "\n",
    "Run **all cells below** (through \"ASSEMBLE FULL CONFIG\") for full production configuration. Skip Section 2a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# DATA SOURCE — Real data\n# ============================================================\n# For CSV source:\nDATA_PATH = \"data/stock_data.csv\"  # Path to your stock data CSV file\n\n# For Tiingo source: run download first:\n#   python scripts/download_tiingo.py --phase all --keys-file keys.txt\n# Then set DATA_SOURCE = \"tiingo\" in Global cell above.\n\n# Universe and history parameters (overridden by quick mode cell if active)\nN_STOCKS = 50    # How many stocks to keep, ranked by median market cap (50=fast, 200=realistic, 0=all)\nN_YEARS = 14     # How many years of history to use (0=all available)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pipeline Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `n_stocks` | Universe size — how many stocks to keep (ranked by market cap). More = better diversification, slower training. |\n",
    "| `window_length` | Length of each input window in trading days. 504 ≈ 2 years of daily data. Each stock produces many overlapping windows. |\n",
    "| `n_features` | Features per timestep. 2 = log-return + realized volatility. |\n",
    "| `vol_window` | Lookback (trading days) for trailing volatility computation. 252 ≈ 1 year. |\n",
    "| `vix_lookback_percentile` | VIX percentile above which a day is labeled \"crisis\". Higher = fewer crisis days = less overweighting. |\n",
    "| `min_valid_fraction` | Minimum fraction of non-missing data to keep a stock. 0.80 = stocks missing >20% of their history are dropped. |"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "#### VAE Architecture Parameters\n\n| Parameter | What it controls |\n|---|---|\n| `K` | Maximum latent capacity — how many risk factors the VAE *can* learn. Only \"active units\" (AU ≤ K) are actually used. Typical: 100-200. |\n| `sigma_sq_init` | Starting value of learned observation noise σ². 1.0 = assume noise equals signal at first. The model learns the true value during training (Mode P/A). |\n| `sigma_sq_min` / `sigma_sq_max` | Clamp bounds for σ². Prevents extreme values: too small = overfitting (model claims perfect reconstruction), too large = underfitting (model gives up). |\n| `window_length` / `n_features` | Must match DataPipelineConfig — determines input tensor shape (T×F). |\n| `r_max` | Maximum ratio of model parameters to data points. Safety guard — if the CNN has more parameters than the data can support, training is rejected. Relaxed automatically for small universes. |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Loss Function Parameters\n\n| Parameter | What it controls |\n|---|---|\n| `mode` | Loss formulation. **P** = full probabilistic ELBO with learned σ² (recommended). **F** = simplified with β warmup, σ² frozen at 1.0 (use if Mode P diverges). **A** = hybrid with tunable KL weight β. |\n| `gamma` | Crisis overweighting factor. 3.0 = windows falling in crisis periods (high VIX) count 3× more in reconstruction loss. Forces the model to learn crisis dynamics well. |\n| `lambda_co_max` | Maximum co-movement loss weight. Controls how strongly latent distances must match stock Spearman correlations. Active during Phases 1-2, decays to 0 in Phase 3. |\n| `beta_fixed` | Fixed KL weight for Mode A (must be 1.0 for Mode P). Values <1 reduce regularization pressure, giving more freedom to reconstruction. |\n| `warmup_fraction` | Fraction of training where β ramps 0→1 (Mode F only). Prevents posterior collapse by letting the model learn to reconstruct before enforcing KL regularization. |\n| `max_pairs` | Max stock pairs sampled per batch for co-movement loss. Limits compute cost — full pairwise is O(B²). |\n| `delta_sync` | Maximum date gap (calendar days) for windows to be \"synchronized\" in the same time block. 21 ≈ 1 month. Ensures co-movement comparisons are temporally valid. |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Training Parameters\n\n| Parameter | What it controls |\n|---|---|\n| `max_epochs` | Hard limit on training duration. Training may stop earlier via early stopping. |\n| `batch_size` | Windows per gradient update. Larger = smoother gradients but more memory. 256 is a good default. |\n| `learning_rate` | Initial optimizer step size (η₀). Too high → loss diverges. Too low → converges very slowly. Typical: 1e-4 to 1e-3. |\n| `weight_decay` | L2 regularization on model weights. Penalizes large weights to reduce overfitting. 1e-5 is mild. |\n| `adam_betas` / `adam_eps` | Adam optimizer internals (momentum and numerical stability). Rarely need tuning. |\n| `patience` | **Early stopping** — if validation ELBO doesn't improve for this many consecutive epochs, stop training and restore the best checkpoint. |\n| `lr_patience` | **LR reduction** — if validation stagnates for this many epochs, multiply LR by `lr_factor`. Triggers before early stopping. |\n| `lr_factor` | LR reduction multiplier. 0.5 = halve the learning rate each time it triggers. |\n| `n_strata` | Number of volatility-based groups for stratified batching (Phases 1-2). Ensures each batch contains stocks from all risk profiles, not just one cluster. |\n| `curriculum_phase1_frac` | Fraction of epochs for Phase 1 (co-movement at full strength + synchronized batching). |\n| `curriculum_phase2_frac` | Fraction of epochs for Phase 2 (co-movement linearly decaying). Phase 3 = remainder (free refinement, random batching). |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Inference Parameters\n\n| Parameter | What it controls |\n|---|---|\n| `batch_size` | Batch size for inference pass. Can be larger than training (no gradients stored = less memory). |\n| `au_threshold` | KL threshold in nats to consider a latent dimension \"active\". 0.01 is standard — dimensions with KL < 0.01 are effectively unused (posterior ≈ prior). |\n| `r_min` | Minimum observations-per-parameter ratio. Caps AU_max = ⌊√(2·N_obs/r_min)⌋ to prevent more active factors than the data can reliably estimate. |\n| `aggregation_method` | How to combine predictions from overlapping windows for the same stock. \"mean\" averages all windows. |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Risk Model Parameters\n\n| Parameter | What it controls |\n|---|---|\n| `winsorize_lo` / `winsorize_hi` | Percentile bounds for clipping extreme volatility ratios during rescaling. [5, 95] = trim the 5% most extreme values on each side. |\n| `d_eps_floor` | Minimum idiosyncratic (stock-specific) variance. Prevents division by zero when a stock has near-zero residual risk. |\n| `conditioning_threshold` | If the covariance matrix condition number exceeds this, the factor regression switches to ridge regression for numerical stability. |\n| `ridge_scale` | Ridge regularization strength when the fallback activates. Small value (1e-6) = minimal regularization, just enough to stabilize. |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Portfolio Optimization Parameters\n\n| Parameter | What it controls |\n|---|---|\n| `lambda_risk` | Risk aversion. Higher = portfolio avoids variance more aggressively, at the cost of lower expected return. |\n| `w_max` | Hard cap per stock. 0.05 = no stock can exceed 5% of the portfolio. |\n| `w_min` | Minimum active weight. Stocks allocated below this are eliminated entirely (semi-continuous: either 0 or ≥ w_min). |\n| `w_bar` | Concentration penalty threshold. Stocks above this weight get penalized to encourage diversification. |\n| `phi` | Concentration penalty strength. Higher = more aggressively pushes weights below w_bar. |\n| `kappa_1` / `kappa_2` | Linear and quadratic turnover penalties. Penalize trading costs when rebalancing. Higher = more stable portfolio across rebalances. |\n| `delta_bar` | Turnover penalty threshold — small weight changes below this are not penalized. |\n| `tau_max` | Maximum one-way turnover per rebalance. 0.30 = at most 30% of the portfolio can change in a single rebalance. |\n| `n_starts` | Multi-start initializations for the SCA optimizer. More starts = higher chance of finding the global optimum, but proportionally slower. |\n| `sca_max_iter` / `sca_tol` | SCA (Sequential Convex Approximation) iteration limit and convergence tolerance. |\n| `armijo_*` | Line search parameters (sufficient decrease, backtracking factor, max steps). Controls step size selection within SCA. |\n| `max_cardinality_elim` | Maximum rounds of sequential stock elimination to enforce the minimum weight constraint. |\n| `entropy_eps` | Tiny constant (1e-30) added inside log() to avoid log(0). Pure numerical safety. |\n| `alpha_grid` | Grid of α values for the variance-entropy frontier. The optimizer tries each α and picks the best tradeoff between risk and factor diversification. |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Walk-Forward Validation Parameters\n\n| Parameter | What it controls |\n|---|---|\n| `total_years` | Total history length used for the walk-forward. More years = more folds = more robust evaluation, but requires more data. |\n| `min_training_years` | Minimum training window. The first fold starts with this many years of training data. Too small = unreliable model, too large = few folds. |\n| `oos_months` | Out-of-sample test period per fold. After training, the portfolio is tested on this many months, then the window slides forward. |\n| `embargo_days` | Gap (trading days) between training end and OOS start. Prevents information leakage from overlapping windows near the boundary. 21 ≈ 1 month. |\n| `holdout_years` | Final holdout period excluded from all training/testing. Reserved for ultimate out-of-sample validation. |\n| `val_years` | Nested validation window within training for Phase A hyperparameter selection. Used to score HP configs without touching OOS data. |\n| `score_lambda_pen` | Weight of maximum drawdown penalty in the composite HP scoring function. Higher = favor configs with lower drawdowns. |\n| `score_lambda_est` | Weight of estimation quality penalty (variance ratio) in scoring. Higher = favor configs with more accurate risk predictions. |\n| `score_mdd_threshold` | Maximum drawdown threshold. Drawdowns beyond this are heavily penalized in the composite score. |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# DATA PIPELINE (MOD-001)\n# ============================================================\ndata_cfg = DataPipelineConfig(\n    n_stocks=N_STOCKS,           # Universe size: top N stocks by market cap (more = better diversification, slower)\n    window_length=504,           # Input window length in trading days (504 ≈ 2 years of daily data)\n    n_features=2,                # Features per timestep: log-return + realized volatility\n    vol_window=252,              # Lookback for trailing volatility (252 ≈ 1 year)\n    vix_lookback_percentile=80.0,# VIX percentile to label \"crisis\" days (80 = top 20% VIX days are crises)\n    min_valid_fraction=0.80,     # Drop stocks missing >20% of their price history\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# VAE ARCHITECTURE (MOD-002)\n# ============================================================\nvae_cfg = VAEArchitectureConfig(\n    K=200,                                 # Max latent factors the VAE can discover (only \"active units\" AU<=K are used)\n    sigma_sq_init=1.0,                     # Initial observation noise (1.0 = assume noise = signal, model learns true value)\n    sigma_sq_min=1e-4,                     # Lower clamp for sigma^2 (prevents overfitting: model can't claim 0 noise)\n    sigma_sq_max=10.0,                     # Upper clamp for sigma^2 (prevents divergence: model can't give up entirely)\n    window_length=data_cfg.window_length,  # Must match data_cfg.window_length\n    n_features=data_cfg.n_features,        # Must match data_cfg.n_features\n    r_max=5.0,                             # Max model params / data points ratio (safety guard against overfitting)\n)\n\nprint(f\"Encoder depth L={vae_cfg.encoder_depth}, \"\n      f\"Final width C_L={vae_cfg.final_layer_width}, \"\n      f\"D={vae_cfg.D}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# LOSS FUNCTION (MOD-004)\n# ============================================================\nloss_cfg = LossConfig(\n    mode=\"P\",                    # Loss mode: \"P\"=full ELBO with learned sigma^2, \"F\"=simplified with beta warmup, \"A\"=hybrid\n    gamma=3.0,                   # Crisis overweighting: crisis windows count 3x more in reconstruction loss\n    lambda_co_max=0.5,           # Max co-movement weight: how strongly latent distances must match Spearman correlations\n    beta_fixed=1.0,              # Fixed KL weight for Mode A (must be 1.0 for Mode P)\n    warmup_fraction=0.20,        # Mode F only: fraction of epochs to ramp beta 0->1 (prevents posterior collapse)\n    max_pairs=2048,              # Max stock pairs per batch for co-movement loss (limits O(B^2) compute)\n    delta_sync=21,               # Max date gap (days) for windows to be \"synchronized\" (21 ≈ 1 month)\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# TRAINING (MOD-005)\n# ============================================================\ntraining_cfg = TrainingConfig(\n    max_epochs=100,              # Hard limit on training duration (early stopping usually triggers sooner)\n    batch_size=256,              # Windows per gradient step (larger = smoother but more memory)\n    learning_rate=1e-4,          # Initial optimizer step size (too high = diverges, too low = slow)\n    weight_decay=1e-5,           # L2 penalty on weights to reduce overfitting (1e-5 = mild)\n    adam_betas=(0.9, 0.999),     # Adam momentum parameters (rarely need tuning)\n    adam_eps=1e-8,               # Adam numerical stability constant (rarely need tuning)\n    patience=10,                 # Early stopping: stop after 10 epochs without validation improvement\n    lr_patience=5,               # Reduce LR if validation stagnates for 5 epochs (triggers before early stop)\n    lr_factor=0.5,               # LR multiplier when reducing (0.5 = halve each time)\n    n_strata=15,                 # Volatility groups for stratified batching (ensures risk diversity per batch)\n    curriculum_phase1_frac=0.30, # Phase 1: 30% of epochs with full co-movement + synchronized batching\n    curriculum_phase2_frac=0.30, # Phase 2: 30% of epochs with co-movement linearly decaying to 0\n                                 # Phase 3: remaining 40% with no co-movement, random batching\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# INFERENCE (MOD-006)\n# After training, the encoder runs on all windows to extract each stock's\n# latent profile. These parameters control that extraction pass and\n# decide how many latent dimensions are kept as \"active\" risk factors.\n# ============================================================\ninference_cfg = InferenceConfig(\n    batch_size=512,              # Larger than training (no gradients = less memory needed)\n    au_threshold=0.01,           # KL > 0.01 nats = dimension is \"active\" (below = unused, posterior ≈ prior)\n    r_min=2,                     # Min data/param ratio — caps AU_max to prevent more factors than data supports\n    aggregation_method=\"mean\",   # Average overlapping window predictions for each stock's final profile\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# RISK MODEL (MOD-007)\n# Transforms latent exposures (B matrix) into a full covariance\n# matrix Sigma_assets via cross-sectional regression and\n# Ledoit-Wolf shrinkage. These parameters handle numerical\n# edge cases (extreme vol ratios, ill-conditioned matrices).\n# ============================================================\nrisk_model_cfg = RiskModelConfig(\n    winsorize_lo=5.0,            # Clip extreme vol ratios below 5th percentile (removes outliers)\n    winsorize_hi=95.0,           # Clip extreme vol ratios above 95th percentile\n    d_eps_floor=1e-6,            # Min idiosyncratic variance per stock (prevents division by zero)\n    conditioning_threshold=1e6,  # Switch to ridge regression if covariance condition number exceeds this\n    ridge_scale=1e-6,            # Ridge regularization strength (tiny, just enough to stabilize)\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# PORTFOLIO OPTIMIZATION (MOD-008)\n# Constraints identical for VAE and all 6 benchmarks (INV-012)\n#\n# Objective: f(w) = -lambda_risk * w'Σw  +  alpha * H(w)  -  penalties\n#   Low variance (w'Σw) ≠ diversified: min-var can put 85% of risk on\n#   one latent factor. Entropy H(w) ensures risk is spread across all\n#   factors. lambda_risk and alpha control the tradeoff between the two.\n# ============================================================\nportfolio_cfg = PortfolioConfig(\n    lambda_risk=1.0,             # Variance penalty weight (higher = lower total variance, more conservative)\n    w_max=0.05,                  # Max 5% per stock (hard cap, prevents single-stock concentration)\n    w_min=0.001,                 # Min active weight: below this, stock is eliminated (0 or >= 0.1%)\n    w_bar=0.03,                  # Concentration penalty kicks in above 3% weight per stock\n    phi=25.0,                    # Concentration penalty strength (higher = stronger diversification pressure)\n    kappa_1=0.1,                 # Linear turnover penalty (penalizes trading costs at rebalance)\n    kappa_2=7.5,                 # Quadratic turnover penalty (penalizes large trades more than small ones)\n    delta_bar=0.01,              # Turnover below 1% is not penalized (de minimis threshold)\n    tau_max=0.30,                # Max 30% portfolio change per rebalance (hard cap on turnover)\n    n_starts=5,                  # Multi-start optimizations (more = better optimum, proportionally slower)\n    sca_max_iter=100,            # Max iterations for the SCA convex optimizer\n    sca_tol=1e-8,                # SCA convergence tolerance (stop when improvement < this)\n    armijo_c=1e-4,               # Line search: sufficient decrease constant\n    armijo_rho=0.5,              # Line search: backtracking factor (halve step on each retry)\n    armijo_max_iter=20,          # Line search: max backtracking attempts per SCA step\n    max_cardinality_elim=100,    # Max rounds of stock elimination to enforce w_min constraint\n    entropy_eps=1e-30,           # Tiny constant in log() for numerical safety (avoids log(0))\n    alpha_grid=[0.0, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0],  # Risk-entropy tradeoff grid (optimizer picks best alpha)\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# WALK-FORWARD VALIDATION (MOD-009)\n# ============================================================\nwalk_forward_cfg = WalkForwardConfig(\n    total_years=N_YEARS,         # Total history used (more years = more folds = more robust evaluation)\n    min_training_years=10,       # First fold trains on at least 10 years (shorter = unreliable model)\n    oos_months=6,                # Test each fold on 6 months out-of-sample, then slide forward\n    embargo_days=21,             # 21-day gap between training and test (prevents data leakage from windows)\n    holdout_years=3,             # Last 3 years excluded from all folds (final out-of-sample validation)\n    val_years=2,                 # 2-year nested validation within training for HP selection (Phase A)\n    score_lambda_pen=5.0,        # HP scoring: penalty weight for max drawdown (higher = favor low-drawdown configs)\n    score_lambda_est=2.0,        # HP scoring: penalty weight for poor risk estimation accuracy\n    score_mdd_threshold=0.20,    # HP scoring: drawdowns above 20% get heavily penalized\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# HP GRID for Phase A (set to None for default 18-config grid)\n# ============================================================\nHP_GRID = None  # None = default grid: 3 loss modes × 2 learning rates × 3 alphas = 18 configs\n                # Phase A tries all configs on nested validation to find the best HP set per fold\n\n# Uncomment to define a custom grid (faster, but less thorough HP search):\n# HP_GRID = [\n#     {\"mode\": \"P\", \"learning_rate\": 5e-4, \"alpha\": 1.0},   # mode: loss formulation, alpha: risk-entropy tradeoff\n#     {\"mode\": \"F\", \"learning_rate\": 1e-3, \"alpha\": 0.5},\n#     {\"mode\": \"A\", \"learning_rate\": 1e-3, \"alpha\": 2.0},\n# ]"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ASSEMBLE FULL CONFIG\n",
    "# ============================================================\n",
    "if DATA_SOURCE != \"synthetic\" and QUICK_MODE == False:\n",
    "      config = PipelineConfig(\n",
    "            data=data_cfg,\n",
    "            vae=vae_cfg,\n",
    "            loss=loss_cfg,\n",
    "            training=training_cfg,\n",
    "            inference=inference_cfg,\n",
    "            risk_model=risk_model_cfg,\n",
    "            portfolio=portfolio_cfg,\n",
    "            walk_forward=walk_forward_cfg,\n",
    "            seed=SEED,\n",
    "      )\n",
    "\n",
    "      print(\"PipelineConfig assembled.\")\n",
    "      print(f\"  Walk-forward: {config.walk_forward.total_years}y total, \"\n",
    "            f\"{config.walk_forward.min_training_years}y min training, \"\n",
    "            f\"{config.walk_forward.holdout_years}y holdout\")\n",
    "      print(f\"  VAE: K={config.vae.K}, T={config.vae.window_length}, F={config.vae.n_features}\")\n",
    "      print(f\"  Training: {config.training.max_epochs} max epochs, \"\n",
    "            f\"bs={config.training.batch_size}, lr={config.training.learning_rate}\")\n",
    "      print(f\"  Loss mode: {config.loss.mode}, gamma={config.loss.gamma}\")\n",
    "      print(f\"  Capacity guard r_max: {config.vae.r_max}\")\n",
    "      print(f\"  Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Tiingo Download (run once)\n",
    "\n",
    "Run this cell **only once** to download Tiingo data. After the first run, the data is saved locally and reused automatically.\n",
    "Set `MAX_TICKERS` to a small number (e.g. 5) for testing, or `None` for the full universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 01:23:52,794 [INFO] download_tiingo: Validating 2 API key(s)...\n",
      "2026-02-08 01:23:53,429 [INFO] download_tiingo: Key #1 valid (masked: ...77e2)\n",
      "2026-02-08 01:23:53,956 [INFO] download_tiingo: Key #2 valid (masked: ...1a26)\n",
      "2026-02-08 01:23:53,958 [INFO] download_tiingo: 2/2 keys valid.\n",
      "2026-02-08 01:23:53,959 [INFO] download_tiingo: === Phase 1: Ticker Discovery ===\n",
      "2026-02-08 01:23:53,960 [INFO] download_tiingo: Downloading supported tickers list from Tiingo...\n",
      "2026-02-08 01:23:55,507 [INFO] download_tiingo: Fetching S&P 500 constituent list from Wikipedia...\n",
      "2026-02-08 01:23:56,327 [INFO] download_tiingo: Found 503 S&P 500 tickers from Wikipedia\n",
      "2026-02-08 01:23:56,334 [INFO] download_tiingo: SP500 priority: 519 SP500 tickers first, then 21856 others\n",
      "2026-02-08 01:23:56,369 [INFO] download_tiingo: Saved 22375 US equity tickers (6687 active, 15688 delisted) to data/tiingo_meta/supported_tickers.csv\n",
      "2026-02-08 01:23:56,397 [INFO] download_tiingo: Assigned permnos to 22375 tickers\n",
      "2026-02-08 01:23:56,397 [INFO] download_tiingo: Found 22375 US equity tickers\n",
      "2026-02-08 01:23:56,397 [INFO] download_tiingo: === Phase 2: Price Download (max_tickers=None) ===\n",
      "2026-02-08 01:23:56,413 [INFO] download_tiingo: Tickers: 21642 to download, 728 to update, 5 already complete\n",
      "2026-02-08 01:23:56,413 [INFO] download_tiingo: Rate limit budget: 2 keys × 45 requests/key = 90 tickers per cycle\n",
      "2026-02-08 01:23:56,829 [WARNING] download_tiingo: Rate limited on ticker ACERW (key ...77e2)\n",
      "2026-02-08 01:23:56,831 [INFO] download_tiingo: Key #1 rate-limited by server (masked: ...77e2)\n",
      "2026-02-08 01:23:57,233 [WARNING] download_tiingo: Rate limited on ticker ACERW (key ...1a26)\n",
      "2026-02-08 01:23:57,241 [INFO] download_tiingo: Key #2 rate-limited by server (masked: ...1a26)\n",
      "2026-02-08 01:23:57,241 [INFO] download_tiingo: All 2 keys exhausted (1/22370 done). Re-run to resume from where it stopped.\n",
      "2026-02-08 01:23:57,242 [INFO] download_tiingo: Price download complete: 0 tickers processed\n",
      "2026-02-08 01:23:57,244 [INFO] download_tiingo: === Phase 3: Merge to Pipeline Format ===\n",
      "2026-02-08 01:23:57,662 [INFO] download_tiingo: Merging 649 ticker files...\n",
      "2026-02-08 01:24:02,442 [INFO] download_tiingo: Penny stock filter (adj_price < $1.00): removed 58331 rows\n",
      "2026-02-08 01:24:02,559 [INFO] download_tiingo: Min history filter (< 504 days): removed 45 stocks (9562 rows)\n",
      "2026-02-08 01:24:03,083 [INFO] download_tiingo: Merged 3445308 rows for 592 tickers (1995-01-03 to 2026-02-06) → data/tiingo_us_equities.parquet (from 3513201 raw rows, 649 raw tickers)\n",
      "2026-02-08 01:24:03,087 [INFO] download_tiingo: Done! Data saved to data/\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TIINGO DOWNLOAD — Run once, data is saved locally\n",
    "# Skip this cell if data is already downloaded or DATA_SOURCE != \"tiingo\"\n",
    "# ============================================================\n",
    "\n",
    "if DATA_SOURCE == \"tiingo\":\n",
    "    import importlib.util\n",
    "\n",
    "    _spec = importlib.util.spec_from_file_location(\n",
    "        \"download_tiingo\", str(PROJECT_ROOT / \"scripts\" / \"download_tiingo.py\")\n",
    "    )\n",
    "    _mod = importlib.util.module_from_spec(_spec)\n",
    "    _spec.loader.exec_module(_mod)  # type: ignore[union-attr]\n",
    "\n",
    "    MAX_TICKERS = None  # Set to None for full universe (~22k tickers)\n",
    "\n",
    "    _mod.run_download(\n",
    "        api_keys=TIINGO_API_KEYS,\n",
    "        data_dir=DATA_DIR,\n",
    "        max_tickers=MAX_TICKERS,\n",
    "        sp500_first=True,  # Download SP500 tickers first (priority)\n",
    "    )\n",
    "else:\n",
    "    print(f\"DATA_SOURCE={DATA_SOURCE}, skipping Tiingo download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source: tiingo\n",
      "Stock data shape: (161887, 8)\n",
      "Date range: 2012-02-06 00:00:00 to 2026-02-06 00:00:00\n",
      "Unique stocks: 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>date</th>\n",
       "      <th>adj_price</th>\n",
       "      <th>volume</th>\n",
       "      <th>exchange_code</th>\n",
       "      <th>share_code</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>delisting_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10061</td>\n",
       "      <td>2012-02-06</td>\n",
       "      <td>13.914680</td>\n",
       "      <td>8907600</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1.147953e+11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10061</td>\n",
       "      <td>2012-02-07</td>\n",
       "      <td>14.060434</td>\n",
       "      <td>11293700</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1.160722e+11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10061</td>\n",
       "      <td>2012-02-08</td>\n",
       "      <td>14.295859</td>\n",
       "      <td>14567500</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1.187594e+11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10061</td>\n",
       "      <td>2012-02-09</td>\n",
       "      <td>14.790402</td>\n",
       "      <td>31579100</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1.255727e+11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10061</td>\n",
       "      <td>2012-02-10</td>\n",
       "      <td>14.797900</td>\n",
       "      <td>22546500</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1.260426e+11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   permno       date  adj_price    volume  exchange_code  share_code  \\\n",
       "0   10061 2012-02-06  13.914680   8907600              3          10   \n",
       "1   10061 2012-02-07  14.060434  11293700              3          10   \n",
       "2   10061 2012-02-08  14.295859  14567500              3          10   \n",
       "3   10061 2012-02-09  14.790402  31579100              3          10   \n",
       "4   10061 2012-02-10  14.797900  22546500              3          10   \n",
       "\n",
       "     market_cap  delisting_return  \n",
       "0  1.147953e+11               NaN  \n",
       "1  1.160722e+11               NaN  \n",
       "2  1.187594e+11               NaN  \n",
       "3  1.255727e+11               NaN  \n",
       "4  1.260426e+11               NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "\n",
    "stock_data, start_date = load_data_source(\n",
    "    source=DATA_SOURCE,\n",
    "    data_path=DATA_PATH if DATA_SOURCE == \"csv\" else \"\",\n",
    "    data_dir=DATA_DIR,\n",
    "    n_stocks=N_STOCKS,\n",
    "    n_years=N_YEARS,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Data source: {DATA_SOURCE}\")\n",
    "print(f\"Stock data shape: {stock_data.shape}\")\n",
    "print(f\"Date range: {stock_data['date'].min()} to {stock_data['date'].max()}\")\n",
    "print(f\"Unique stocks: {stock_data['permno'].nunique()}\")\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returns: 3522 dates x 50 stocks\n",
      "Trailing vol: (3522, 50) (first 251 rows NaN)\n",
      "Returns date range: 2012-02-06 00:00:00 to 2026-02-06 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Compute log-returns and trailing volatility\n",
    "returns = compute_log_returns(stock_data)\n",
    "trailing_vol = compute_trailing_volatility(returns, window=config.data.vol_window)\n",
    "\n",
    "print(f\"Returns: {returns.shape[0]} dates x {returns.shape[1]} stocks\")\n",
    "print(f\"Trailing vol: {trailing_vol.shape} (first {config.data.vol_window-1} rows NaN)\")\n",
    "print(f\"Returns date range: {returns.index[0]} to {returns.index[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Run Pipeline\n",
    "\n",
    "Executes the full walk-forward validation: Phase A (HP selection) + Phase B (deployment) on each fold, then benchmarks, statistical tests, and report generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard Monitoring\n",
    "\n",
    "Launch TensorBoard in a terminal to monitor training in real-time:\n",
    "\n",
    "```bash\n",
    ".venv/bin/tensorboard --logdir runs/\n",
    "```\n",
    "\n",
    "Logs are organized as `runs/fold_XX/phase_a/config_YY_mode_M_lr_L/` and `runs/fold_XX/phase_b/`.\n",
    "\n",
    "### Metric Value Interpretation\n",
    "\n",
    "The input windows are **z-scored** (mean=0, std=1), so metric values are directly interpretable as fractions of signal variance.\n",
    "\n",
    "#### `reconstruction` (Step/ and Loss/)\n",
    "Raw per-element MSE **before** D/(2σ²) scaling. Since data is z-scored, L_recon ≈ fraction of unexplained variance.\n",
    "\n",
    "| L_recon | RMSE | Interpretation |\n",
    "|---|---|---|\n",
    "| **1.0** | 1.0 | No better than predicting the mean. Not learning. |\n",
    "| **0.5** | 0.71 | ~50% variance explained. Poor. |\n",
    "| **0.1** | 0.32 | ~90% variance explained. Decent. |\n",
    "| **0.05** | 0.22 | ~95% explained. Good for financial data. |\n",
    "| **0.01** | 0.10 | ~99% explained. Likely overfitting on noisy data. |\n",
    "\n",
    "Good converged range on real data: **0.05–0.20**.\n",
    "\n",
    "#### `kl_divergence` (Step/ and Loss/)\n",
    "KL **summed over K dimensions**, averaged over batch. Raw value scales linearly with K — divide by K for per-dimension KL (nats).\n",
    "\n",
    "| L_KL (K=100) | KL/dim | Interpretation |\n",
    "|---|---|---|\n",
    "| **0** | 0 | **Posterior collapse.** Encoder ignores input entirely. |\n",
    "| **1** | 0.01 | Right at AU threshold. Quasi-collapsed. |\n",
    "| **10** | 0.1 | ~10 weakly active dimensions. Most collapsed. |\n",
    "| **50** | 0.5 | ~50 active dims, moderate info per dim. Typical early-mid training. |\n",
    "| **100** | 1.0 | ~100 active dims at ~1 nat each. Healthy, full capacity used. |\n",
    "| **250** | 2.5 | Very informative. If AU ≪ 100, a few dims are dominating. |\n",
    "\n",
    "Good converged range (K=100): **25–150**. For K=200, double these values.\n",
    "\n",
    "#### `co_movement` (Step/ and Loss/)\n",
    "MSE between latent cosine distances and Spearman correlation targets: `(1/|P|) × Σ (d_cos(μ_i, μ_j) − (1 − ρ_ij))²`.\n",
    "\n",
    "| L_co | Avg error | Interpretation |\n",
    "|---|---|---|\n",
    "| **0.01** | 0.10 | Excellent alignment between latent distances and Spearman. |\n",
    "| **0.05** | 0.22 | Good. Reasonable after Phase 1. |\n",
    "| **0.10** | 0.32 | Moderate. Early training territory. |\n",
    "| **0.30** | 0.55 | Poor. Random initialization level. |\n",
    "| **> 0.5** | > 0.7 | Latent space disorganized w.r.t. co-movements. |\n",
    "\n",
    "**Key: this value is only meaningful when λ_co > 0 (Phases 1-2).** In Phase 3, the value is 0.0 because `compute_co_movement_loss` is skipped entirely.\n",
    "\n",
    "#### `sigma_sq` (Step/ and Loss/)\n",
    "Learned observation noise σ² = clamp(exp(log_sigma_sq), 1e-4, 10). **At equilibrium in Mode P, σ² converges to ≈ L_recon** (the model's own reconstruction MSE).\n",
    "\n",
    "| σ² | Variance explained | Interpretation |\n",
    "|---|---|---|\n",
    "| **10.0** (clamp max) | ~0% | Model giving up on reconstruction. Diverging. |\n",
    "| **1.0** | ~0% | Noise = signal. Initial state / Mode F (frozen). |\n",
    "| **0.3–0.5** | 50–70% | Early-mid convergence. |\n",
    "| **0.05–0.2** | 80–95% | Healthy convergence on financial data. |\n",
    "| **0.01** | ~99% | Possibly overfitting. |\n",
    "| **1e-4** (clamp min) | ~100% | Hitting floor. Overfitting. |\n",
    "\n",
    "Mode F: σ² is **frozen at 1.0** — ignore this metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 01:24:03,520 [INFO] src.integration.pipeline: Starting walk-forward: 20 folds + holdout\n",
      "2026-02-08 01:24:03,520 [INFO] src.integration.pipeline: [Fold 0/20] Phase A skipped, using default config\n",
      "2026-02-08 01:24:03,521 [INFO] src.integration.pipeline: [Fold 0/20] Phase B: deployment training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PIPELINE CONFIGURATION SUMMARY\n",
      "============================================================\n",
      "  Seed: 42 | Device: mps | Data: tiingo\n",
      "\n",
      "  [Data]         n_stocks=50, T=504, F=2, vol_window=252\n",
      "  [VAE]          K=100, L=5, C_L=384, r_max=5.0\n",
      "  [Loss]         mode=P, gamma=3.0, lambda_co_max=0.5, beta_fixed=1.0\n",
      "  [Training]     max_epochs=50, bs=256, lr=0.0001, wd=1e-05\n",
      "                 patience=30, lr_patience=5, lr_factor=0.5\n",
      "                 curriculum: phase1=0.3, phase2=0.3, strata=15\n",
      "  [Inference]    bs=512, AU_threshold=0.01, r_min=2\n",
      "  [Risk Model]   winsorize=[5.0, 95.0], cond_threshold=1e+06\n",
      "  [Portfolio]    w_max=0.05, w_min=0.001, tau_max=0.3, n_starts=2\n",
      "  [Walk-Forward] 20y total, 6y min train, 6mo OOS, 4y holdout, embargo=21d\n",
      "  [HP Grid]      18 (default) configs\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 01:24:04,446 [INFO] src.integration.pipeline:   [Fold 0] Windowing: 41534 windows from 50 stocks, 1511 train days\n",
      "2026-02-08 01:24:04,447 [WARNING] src.integration.pipeline: Small universe adaptation: n=50, T_annee=5, K=100->70, c_min=384->144, r=108.52 > r_max=5.0. Relaxing r_max to 119.37 with reinforced regularization (dropout=0.2).\n",
      "    Training: 7320step [4:31:03,  2.22s/step, AU=68, epoch=50/50, loss=286.8337, lr=6.3e-06, val=387.7550]                          \n",
      "2026-02-08 05:55:09,459 [INFO] src.integration.pipeline:   [Fold 0] Training: 50 epochs in 16265.9s (best_epoch=30)\n",
      "2026-02-08 05:55:09,460 [INFO] src.integration.pipeline:   [Fold 0] Inference: extracting latent profiles...\n",
      "    Inference: 100%|██████████| 82/82 [00:31<00:00,  2.62batch/s]\n",
      "2026-02-08 05:56:10,916 [INFO] src.integration.pipeline:   [Fold 0] AU=38 active units (max=38)\n",
      "2026-02-08 05:56:13,342 [INFO] src.integration.pipeline:   [Fold 0] Dual rescaling done\n",
      "2026-02-08 05:56:14,091 [INFO] src.integration.pipeline:   [Fold 0] Risk model: AU=38, B_A(44×38), Sigma(44×44)\n",
      "2026-02-08 05:56:14,091 [INFO] src.integration.pipeline:   [Fold 0] Frontier: 7 alphas × 2 starts...\n",
      "2026-02-08 05:56:14,091 [INFO] src.portfolio.frontier:     Frontier alpha 1/7 (alpha=0.000)...\n",
      "2026-02-08 05:56:14,210 [INFO] src.portfolio.frontier:     Frontier alpha 2/7 (alpha=0.010)...\n",
      "2026-02-08 05:56:18,076 [INFO] src.portfolio.frontier:     Frontier alpha 3/7 (alpha=0.050)...\n",
      "2026-02-08 05:56:22,504 [INFO] src.portfolio.frontier:     Frontier alpha 4/7 (alpha=0.100)...\n",
      "2026-02-08 05:56:26,950 [INFO] src.portfolio.frontier:     Frontier alpha 5/7 (alpha=0.500)...\n",
      "2026-02-08 05:56:31,452 [INFO] src.portfolio.frontier:     Frontier alpha 6/7 (alpha=1.000)...\n",
      "2026-02-08 05:56:36,000 [INFO] src.portfolio.frontier:     Frontier alpha 7/7 (alpha=5.000)...\n",
      "2026-02-08 05:56:40,710 [INFO] src.integration.pipeline:   [Fold 0] Frontier done (26.6s), alpha*=5.000. Final SCA (2 starts)...\n",
      "2026-02-08 05:56:45,395 [INFO] src.integration.pipeline:   [Fold 0] SCA done. Cardinality enforcement...\n",
      "2026-02-08 05:57:15,369 [INFO] src.integration.pipeline:   [Fold 0] Portfolio: alpha=5.00, n_active=28 | total 16391.8s\n",
      "/Users/mathis/Latent_risk_factor/Latent_risk_factor/src/benchmarks/erc.py:67: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  prob.solve(solver=cp.ECOS, max_iters=500)\n",
      "2026-02-08 05:57:16,714 [INFO] src.integration.pipeline: [Fold 0/20] E*=50, AU=38.0, H=0.880, Sharpe=1.046\n",
      "2026-02-08 05:57:16,715 [INFO] src.integration.pipeline: [Fold 1/20] Phase A skipped, using default config\n",
      "2026-02-08 05:57:16,715 [INFO] src.integration.pipeline: [Fold 1/20] Phase B: deployment training\n",
      "2026-02-08 05:57:17,815 [INFO] src.integration.pipeline:   [Fold 1] Windowing: 47097 windows from 50 stocks, 1636 train days\n",
      "2026-02-08 05:57:17,816 [WARNING] src.integration.pipeline: Small universe adaptation: n=50, T_annee=6, K=100->76, c_min=384->144, r=90.04 > r_max=5.0. Relaxing r_max to 99.05 with reinforced regularization (dropout=0.2).\n",
      "    Training:  96%|█████████▌| 7905/8250 [5:07:49<12:50,  2.23s/step, epoch=48/50, loss=273.5763]                                    "
     ]
    }
   ],
   "source": [
    "# Print training configuration summary\n",
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE CONFIGURATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Seed: {config.seed} | Device: {DEVICE} | Data: {DATA_SOURCE}\")\n",
    "print()\n",
    "print(f\"  [Data]         n_stocks={config.data.n_stocks}, T={config.data.window_length}, \"\n",
    "      f\"F={config.data.n_features}, vol_window={config.data.vol_window}\")\n",
    "print(f\"  [VAE]          K={config.vae.K}, L={config.vae.encoder_depth}, \"\n",
    "      f\"C_L={config.vae.final_layer_width}, r_max={config.vae.r_max}\")\n",
    "print(f\"  [Loss]         mode={config.loss.mode}, gamma={config.loss.gamma}, \"\n",
    "      f\"lambda_co_max={config.loss.lambda_co_max}, beta_fixed={config.loss.beta_fixed}\")\n",
    "print(f\"  [Training]     max_epochs={config.training.max_epochs}, bs={config.training.batch_size}, \"\n",
    "      f\"lr={config.training.learning_rate}, wd={config.training.weight_decay}\")\n",
    "print(f\"                 patience={config.training.patience}, lr_patience={config.training.lr_patience}, \"\n",
    "      f\"lr_factor={config.training.lr_factor}\")\n",
    "print(f\"                 curriculum: phase1={config.training.curriculum_phase1_frac}, \"\n",
    "      f\"phase2={config.training.curriculum_phase2_frac}, strata={config.training.n_strata}\")\n",
    "print(f\"  [Inference]    bs={config.inference.batch_size}, AU_threshold={config.inference.au_threshold}, \"\n",
    "      f\"r_min={config.inference.r_min}\")\n",
    "print(f\"  [Risk Model]   winsorize=[{config.risk_model.winsorize_lo}, {config.risk_model.winsorize_hi}], \"\n",
    "      f\"cond_threshold={config.risk_model.conditioning_threshold:.0e}\")\n",
    "print(f\"  [Portfolio]    w_max={config.portfolio.w_max}, w_min={config.portfolio.w_min}, \"\n",
    "      f\"tau_max={config.portfolio.tau_max}, n_starts={config.portfolio.n_starts}\")\n",
    "print(f\"  [Walk-Forward] {config.walk_forward.total_years}y total, \"\n",
    "      f\"{config.walk_forward.min_training_years}y min train, \"\n",
    "      f\"{config.walk_forward.oos_months}mo OOS, \"\n",
    "      f\"{config.walk_forward.holdout_years}y holdout, \"\n",
    "      f\"embargo={config.walk_forward.embargo_days}d\")\n",
    "n_hp = len(HP_GRID) if HP_GRID else \"18 (default)\"\n",
    "print(f\"  [HP Grid]      {n_hp} configs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "TB_DIR = \"runs/\"  # TensorBoard log directory (set to None to disable)\n",
    "\n",
    "pipeline = FullPipeline(config, tensorboard_dir=TB_DIR)\n",
    "\n",
    "results = pipeline.run(\n",
    "    stock_data=stock_data,\n",
    "    returns=returns,\n",
    "    trailing_vol=trailing_vol,\n",
    "    skip_phase_a=(DATA_SOURCE == \"synthetic\" or QUICK_MODE == True),\n",
    "    vix_data=None,\n",
    "    start_date=start_date,\n",
    "    hp_grid=HP_GRID,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "print(\"Pipeline complete.\")\n",
    "print(f\"Folds processed: {len(results['vae_results'])}\")\n",
    "print(f\"Benchmarks: {list(results['benchmark_results'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Results - Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text summary\n",
    "print(format_summary_table(results[\"report\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployment recommendation\n",
    "deployment = results[\"report\"][\"deployment\"]\n",
    "print(f\"Scenario: {deployment['scenario']}\")\n",
    "print(f\"Recommendation: {deployment['recommendation']}\")\n",
    "print()\n",
    "print(\"Per-benchmark wins (VAE vs benchmark on primary metrics):\")\n",
    "for bench, info in deployment[\"per_benchmark\"].items():\n",
    "    print(f\"  {bench:20s}: {info['wins']}/{info['total']} metrics won\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE summary statistics\n",
    "vae_df = aggregate_fold_metrics(results[\"vae_results\"])\n",
    "vae_summary = summary_statistics(vae_df)\n",
    "print(\"VAE Summary Statistics:\")\n",
    "style_summary_table(vae_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark summary statistics\n",
    "for bench_name, bench_metrics in results[\"benchmark_results\"].items():\n",
    "    bench_df = aggregate_fold_metrics(bench_metrics)\n",
    "    bench_summary = summary_statistics(bench_df)\n",
    "    print(f\"\\n{bench_name} Summary:\")\n",
    "    display(style_summary_table(bench_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Results - Per-Fold Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE per-fold metrics\n",
    "print(\"VAE Per-Fold Metrics:\")\n",
    "style_fold_table(vae_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E* distribution\n",
    "e_star_summary = results[\"report\"][\"e_star_summary\"]\n",
    "print(f\"E* epochs: mean={e_star_summary['mean']:.1f}, \"\n",
    "      f\"std={e_star_summary['std']:.1f}, \"\n",
    "      f\"range=[{e_star_summary['min']}, {e_star_summary['max']}]\")\n",
    "\n",
    "plot_e_star_distribution(results[\"e_stars\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fold metrics: VAE vs benchmarks\n",
    "plot_fold_metrics(results[\"vae_results\"], results[\"benchmark_results\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Results - Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise tests heatmap\n",
    "plot_pairwise_heatmap(results[\"report\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed pairwise test results\n",
    "tests = results[\"report\"][\"statistical_tests\"]\n",
    "print(f\"Total comparisons: {tests['n_tests']} (alpha={tests['alpha']})\")\n",
    "print()\n",
    "\n",
    "for bench_name, metrics in tests[\"pairwise\"].items():\n",
    "    print(f\"VAE vs {bench_name}:\")\n",
    "    for metric, result in metrics.items():\n",
    "        if result.get(\"skipped\", False):\n",
    "            print(f\"  {metric}: skipped ({result['reason']})\")\n",
    "            continue\n",
    "        sig = \" *\" if result.get(\"significant_corrected\", False) else \"\"\n",
    "        print(f\"  {metric}: delta={result['median_delta']:+.4f} \"\n",
    "              f\"[{result['ci_lower']:+.4f}, {result['ci_upper']:+.4f}] \"\n",
    "              f\"p={result.get('p_corrected', result['p_value']):.4f}{sig}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"results/\"\n",
    "\n",
    "written = export_results(results, asdict(config), output_dir=OUTPUT_DIR)\n",
    "\n",
    "print(f\"Results saved to {OUTPUT_DIR}\")\n",
    "for path in written:\n",
    "    print(f\"  {os.path.basename(path)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Latent_risk_factor (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}