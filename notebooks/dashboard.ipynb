{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GOOGLE DRIVE PERSISTENCE (Colab only)\n",
    "# ============================================================\n",
    "# This cell MUST run first to redirect results/ to Drive\n",
    "# before any other code writes to disk.\n",
    "# On local machines, this is a no-op.\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()))\n",
    "\n",
    "from src.integration.colab_drive import is_colab, setup_drive_persistence\n",
    "\n",
    "# === Configuration ===\n",
    "USE_DRIVE = True       # Set False to use local storage only\n",
    "KEEP_N_RUNS = 5        # Number of old runs to keep (rotation)\n",
    "\n",
    "# === Setup ===\n",
    "if is_colab() and USE_DRIVE:\n",
    "    DRIVE_PATH = setup_drive_persistence(\n",
    "        local_results_dir=Path(\"results/diagnostic_runs\"),\n",
    "        keep_n_runs=KEEP_N_RUNS\n",
    "    )\n",
    "    print(f\"Drive persistence enabled\")\n",
    "    print(f\"  Results folder: {DRIVE_PATH}\")\n",
    "    print(f\"  Old runs kept: {KEEP_N_RUNS}\")\n",
    "else:\n",
    "    DRIVE_PATH = None\n",
    "    print(\"Local storage mode (results saved locally)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESUME AFTER DISCONNECT (run this after reconnecting)\n",
    "# ============================================================\n",
    "# Skip this cell for normal execution.\n",
    "# Only use when resuming after a Colab runtime disconnection.\n",
    "\n",
    "# from src.integration.colab_drive import mount_drive, list_runs, get_latest_run\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Remount Drive after disconnect\n",
    "# mount_drive()\n",
    "\n",
    "# # List available runs\n",
    "# runs_dir = Path(\"/content/drive/MyDrive/latent_risk_factor/results/diagnostic_runs\")\n",
    "# print(\"Available runs:\")\n",
    "# for run in list_runs(runs_dir):\n",
    "#     print(f\"  {run['name']} - {run['size_mb']:.1f} MB - Stage: {run['last_stage']}\")\n",
    "\n",
    "# # Get latest run for resume\n",
    "# latest = get_latest_run(runs_dir)\n",
    "# if latest:\n",
    "#     print(f\"\\nTo resume, set RUN_DIR = '{latest}'\")\n",
    "# else:\n",
    "#     print(\"No runs found on Drive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install .\n",
    "\n",
    "# MOSEK license setup (MOSEK is installed via pip install .)\n",
    "# Upload your mosek.lic file to Colab, then copy it:\n",
    "!mkdir -p ~/mosek && cp mosek.lic ~/mosek/mosek.lic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Latent Risk Factor - Pipeline Dashboard\n",
    "\n",
    "Central configuration and execution notebook for the full walk-forward validation pipeline.\n",
    "\n",
    "**Workflow:**\n",
    "1. Configure all parameters (Sections 1-2)\n",
    "2. Load data (Section 3)\n",
    "3. Run pipeline (Section 4)\n",
    "4. Inspect results (Sections 5-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from dataclasses import replace, asdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project root: go up from notebooks/ to project root\n",
    "_NB_DIR = Path(os.path.abspath(\"\")).resolve()\n",
    "PROJECT_ROOT = (_NB_DIR / \"..\").resolve() if _NB_DIR.name == \"notebooks\" else _NB_DIR\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.config import (\n",
    "    PipelineConfig,\n",
    "    DataPipelineConfig,\n",
    "    VAEArchitectureConfig,\n",
    "    LossConfig,\n",
    "    TrainingConfig,\n",
    "    InferenceConfig,\n",
    "    RiskModelConfig,\n",
    "    PortfolioConfig,\n",
    "    WalkForwardConfig,\n",
    ")\n",
    "from src.data_pipeline.data_loader import load_data_source\n",
    "from src.data_pipeline.returns import compute_log_returns\n",
    "from src.data_pipeline.features import compute_trailing_volatility\n",
    "from src.integration.pipeline import FullPipeline\n",
    "from src.integration.reporting import export_results, format_summary_table\n",
    "from src.integration.visualization import (\n",
    "    plot_fold_metrics,\n",
    "    plot_e_star_distribution,\n",
    "    plot_pairwise_heatmap,\n",
    "    style_summary_table,\n",
    "    style_fold_table,\n",
    ")\n",
    "from src.utils import get_optimal_device\n",
    "from src.walk_forward.selection import aggregate_fold_metrics, summary_statistics\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "# force=True required in Colab/Jupyter where root logger is pre-configured\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\", force=True)\n",
    "logger = logging.getLogger(\"dashboard\")\n",
    "\n",
    "print(f\"PyTorch {torch.__version__} | Device: {get_optimal_device()}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration\n",
    "\n",
    "Two configuration profiles are available. **Run ONLY one section:**\n",
    "- **Section 2a** — Synthetic data: minimal parameters for quick end-to-end testing\n",
    "- **Section 2b** — Real data: full production configuration\n",
    "\n",
    "Always run the **Global** cell (below) first, then choose ONE section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GLOBAL\n",
    "# ============================================================\n",
    "SEED = 42\n",
    "DEVICE = str(get_optimal_device())\n",
    "\n",
    "# Data source: \"synthetic\", \"tiingo\", or \"csv\"\n",
    "DATA_SOURCE = \"tiingo\"\n",
    "QUICK_MODE = False          # Set True for minimal config even with real data\n",
    "END_DATE = \"2025-12-31\"       # Latest date for data download and loading (None = today)\n",
    "RUN_FULL_PIPELINE = False   # Set False to skip training and inference (useful for quick visualization)\n",
    "\n",
    "# Pretrained model: set to a checkpoint path to skip VAE training and re-run only portfolio optimization\n",
    "# Set to None to train from scratch\n",
    "CHECKPOINT_PATH = None  # e.g. \"checkpoints/checkpoint_direct_N200_Y10_T504_K200_F2.pt\"\n",
    "\n",
    "# Tiingo API keys (used when DATA_SOURCE = \"tiingo\")\n",
    "TIINGO_API_KEYS = [\n",
    "    \"9ba6e57788deaac3b3c38ed47047cabbbd6077e2\",\n",
    "    \"9aad315d49275c400687f41dd26b22328d8b1a26\",\n",
    "    \"5d0dd679cc88f2a55c833b6e4ffd8d8d56fa0fc5\",\n",
    "    \"66134d425322719f2465e1a021fa43c8054fe157\",\n",
    "    \"1bc7a5bcd30f0d4e32137e92dc74f162da8d2424\",\n",
    "    \"643fbd2e772679625edda7916feb0aaac7d5eb4a\",\n",
    "    \"1d4af3a9a1ce6092fd6dd08ed95c702b6c63c934\",\n",
    "    \"b9a4b731ee5ba3dd02bc09614613ee22e52a7c75\",\n",
    "    \"3b2337b53147ba06aafe33122887ee143bd9ffdd\",\n",
    "    \"561d65cdc9a7dddfd9d891522250bf40f25e9305\",\n",
    "    \"cea76c53d7bafad468b98d32ea600eec2e104561\",\n",
    "    \"e07dce0affdccd565f803a4152364515d2df7f78\",\n",
    "    \"04b97f43d5ef1387395207f5d4388e8cf28fd5f8\",\n",
    "    \"289f4878471f82cee0a533f49aa44bae080dce8f\",\n",
    "    \"1b4e3c553ac4d6078395bdd55cd2f292222919d0\",\n",
    "    \"0a4dd0e82339e617fff7fbb6d8e2affe11194aa8\",\n",
    "    \"43455b994add9f54e6dc629399f851b007f2a038\",\n",
    "    \"a27f7790680f379499ddfe1302ec68a3c9a75a74\",\n",
    "    \"707f2f47875bb820471cff943a7462d91dcbf6ba\",\n",
    "    \"4745d791cd7d08912015bf65edb12fbe256b3cda\",\n",
    "    \"2a2fb9bc6a5d12c3097499a42bbb2afd76f538f4\",\n",
    "    \"15627485874843399ce75d8e984a8c3473e83e65\",\n",
    "    \"c9793a615c339d204454dbac332680fe25c5d471\",\n",
    "    \"669f1bfa6a4b1b51042304af7daf6288f53ea88b\",\n",
    "    \"ffa58b3d42c7b34547574e9318090067621c97d0\",\n",
    "    \"2d18ed83dc9c9b5ea2faa284cdaf6188819cb2ac\",\n",
    "    \"8bfe595273b49c7bafac292261c0e5670002202f\",\n",
    "    \"7f5c957a4129d6bdc8644fc4832e8148828740b8\",\n",
    "    \"fe353f2c950b8e03b4e83e569a0b129ede8e4d35\",\n",
    "    \"3b1ba45faa5f00e1c6fce3beb905c043caefec26\",\n",
    "    \"552b2ca3c474c9694dc417f95b5113175dfdbcce\",\n",
    "    \"6460458e258d2a7f0fc0bb03bc94c2047ba82105\",\n",
    "    \"3062157e1ace50ba07b2a4af9ee4bac16a369b39\"\n",
    "]\n",
    "DATA_DIR = \"data/\"          # Directory for Tiingo downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STOCKS = 4500                  #[⭐️] Top N stocks by median market cap (50=fast, 200=realistic, 0=all)\n",
    "N_YEARS = 40                     #[⭐️] Years of history to keep\n",
    "MIN_N_YEARS = 10                 # First fold trains on at least MIN_N_YEARS (shorter = unreliable model)\n",
    "T = 504                          #[⭐️] Window length (trading days, ~2 years)\n",
    "N_FEATURES = 2                   #[⭐️] Features per timestep (return + realized vol)\n",
    "K = 75                           #[⭐️] Max latent factors the VAE can discover (only \"active units\" AU<=K are used)\n",
    "LOSS_MODE = \"P\"                  # Loss mode: \"P\"=full ELBO with learned sigma^2, \"F\"=simplified with beta warmup, \"A\"=hybrid\n",
    "CRISIS_OVERWEIGHTING = 3.0       #[⭐️] Crisis overweighting: crisis windows count 3x more in reconstruction loss\n",
    "MAX_CO_MOVEMENT_WEIGHT = 0.1     #[⭐️] Max co-movement weight (0.1 = moderate co-movement signal (original))\n",
    "LAMBDA_CS = 0.0                  #[⭐️] Cross-sectional R² loss weight (DISABLED — confirmed harmful in V2 (corrupts encoder))\n",
    "CS_N_SAMPLE_DATES = 20           # Number of time offsets sampled per batch for cross-sectional loss\n",
    "FEATURE_WEIGHTS = [2.0, 0.5]     #[⭐️] Per-feature reconstruction weights: [returns, vol]. Emphasizes returns over volatility.\n",
    "\n",
    "MAX_EPOCHS = 500                 #[⭐️] Max training epochs (early stopping may halt training sooner)\n",
    "BATCH_SIZE = 512                 # Training batch size (windows)\n",
    "LEARNING_RATE = 5e-3             # Adam learning rate\n",
    "EARLY_STOPPING_PATIENCE = 20     #[⭐️] Early stopping patience (epochs without improvement)\n",
    "ES_MIN_DELTA = 0.0               #[⭐️] Min ELBO improvement to count as progress (0=any improvement resets counter)\n",
    "LR_PATIENCE = 30                 # Reduce LR after this many stagnant epochs (triggers before early stop)\n",
    "LR_FACTOR = 0.75                 # LR reduction factor when plateau detected\n",
    "DROPOUT = 0.2\n",
    "TRAINING_STRIDE = 21             # Training window stride (21 = one window per month per stock, faster training)\n",
    "COMPILE_MODEL = True             # torch.compile on CUDA/MPS (faster forward/backward, ~30s warmup)\n",
    "\n",
    "AU_THRESHOLD = 0.01              # KL > 0.01 nats = dimension is \"active\" (below = unused, posterior ≈ prior)\n",
    "AGGREGATION_METHOD = \"mean\"      # Method to aggregate overlapping window predictions (\"mean\", \"median\", etc.)\n",
    "AGGREGATION_HALF_LIFE = 60       #[⭐️] Exponential decay half-life (in windows) for profile aggregation.\n",
    "                                 # 0=uniform mean (legacy). 60=recent windows weighted ~5yr half-life at stride=21.\n",
    "                                 # Higher = B_A reflects current market structure, lower = more stable/historical.\n",
    "\n",
    "SIGMA_Z_EIGENVALUE_PCT = 0.95    #[⭐️] Keep top eigenvalues explaining 95% of Sigma_z variance.\n",
    "                                 # 1.0=no truncation. <1.0=discard noisy factor dimensions.\n",
    "\n",
    "B_A_SHRINKAGE_ALPHA = 0.0        #[⭐️] Shrink B_A towards zero: B_A *= (1 - alpha).\n",
    "                                 # 0.0=no shrinkage (default). Z-scoring per-factor is sufficient\n",
    "                                 # (Barra USE4). Uniform scaling is absorbed by variance targeting.\n",
    "\n",
    "# --- Eigenvalue power shrinkage ---\n",
    "# Compresses dominant eigenvalues to reduce factor concentration.\n",
    "# eigenvalues = eigenvalues^p where p < 1 flattens the spectrum.\n",
    "EIGENVALUE_POWER = 0.65          #[⭐️] 0.65=compress dominant eigenvalues, 1.0=off, 0.5=sqrt\n",
    "\n",
    "HP_GRID = None                   # HP GRID for Phase A (set to None for default 18-config grid : 3 loss modes × 2 learning rates × 3 alphas = 18 configs)\n",
    "                                 # Phase A tries all configs on nested validation to find the best HP set per fold\n",
    "                \n",
    "# Uncomment to define a custom grid (faster, but less thorough HP search):\n",
    "# HP_GRID = [\n",
    "#     {\"mode\": \"P\", \"learning_rate\": 5e-4, \"alpha\": 1.0},   # mode: loss formulation, alpha: risk-entropy tradeoff\n",
    "#     {\"mode\": \"F\", \"learning_rate\": 1e-3, \"alpha\": 0.5},\n",
    "#     {\"mode\": \"A\", \"learning_rate\": 1e-3, \"alpha\": 2.0},\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Variance (w'Sigma w) = total portfolio risk. Minimizing it concentrates\n",
    "# weight on low-vol stocks — but can put 85% of risk on 2-3 latent\n",
    "# factors without noticing (fragile to factor shocks).\n",
    "#\n",
    "# Entropy H(w) = Shannon entropy on each factor's risk contribution.\n",
    "# Maximizing it spreads risk evenly across all active factors — no\n",
    "# single factor dominates (resilient to individual factor crashes).\n",
    "#\n",
    "# lambda_risk and alpha control the tradeoff: more lambda_risk favors\n",
    "# low total variance; more alpha favors even factor risk distribution.\n",
    "#\n",
    "# lambda_risk guide (scales daily variance w'Sigma_daily w to the objective):\n",
    "#   252  → annualized risk aversion gamma=1 (default, balanced)\n",
    "#   504  → gamma=2 (conservative)\n",
    "#   1260 → gamma=5 (near min-variance)\n",
    "#   50   → gamma~0.2 (entropy-dominated, higher variance OK)\n",
    "#\n",
    "# Penalty tuning guide (defaults calibrated for US mid/large cap):\n",
    "#   Illiquid universe (small caps) → raise kappa_1 & kappa_2\n",
    "#   Infrequent rebalancing         → lower kappa_1 & kappa_2\n",
    "#   Allow more concentrated bets   → raise w_bar or lower phi\n",
    "#   Force tighter diversification  → lower w_bar or raise phi\n",
    "#   Smoother trades                → lower delta_bar (e.g. 0.005)\n",
    "#\n",
    "# Valid ranges from DVT spec:\n",
    "#   kappa_1: [0.01, 1.0]  kappa_2: [1, 50]  delta_bar: [0.005, 0.03]\n",
    "# ============================================================\n",
    "\n",
    "LAMBDA_RISK = 252.0              #[⭐️] Risk aversion (252=annualized gamma=1; higher = more conservative)\n",
    "\n",
    "W_MAX = 0.03                     # Hard cap per stock (CVXPY constraint: w_i <= w_max)\n",
    "W_MIN = 0.001                    # Min active weight: below this, stock is eliminated (0 or >= 0.1%)\n",
    "W_BAR = 0.015                    # Soft cap: concentration penalty threshold (phi penalizes w_i > w_bar)\n",
    "\n",
    "PHI = 5.0                       # Concentration penalty strength: phi * sum(max(0, w_i - w_bar)^2). 0=off, 5=moderate\n",
    "KAPPA_1 = 0.1                    # Linear turnover penalty (penalizes trading costs at rebalance)\n",
    "KAPPA_2 = 7.5                    # Quadratic turnover penalty (penalizes large trades more than small ones)\n",
    "DELTA_BAR = 0.01                 # Turnover below 1% is not penalized (de minimis threshold)\n",
    "MAX_TURNOVER = 0.30              #[⭐️] Max 30% turnover per rebalance (prevents excessive trading costs)\n",
    "\n",
    "# ============================================================\n",
    "# OOS REBALANCING (DVT §4.2)\n",
    "# ============================================================\n",
    "REBALANCING_FREQUENCY = 63       #[⭐️] Days between rebalancing (0 = buy-and-hold, 21 = monthly, 63 = quarterly)\n",
    "ENTROPY_TRIGGER_ALPHA = 0.90     # Exceptional rebalancing if H drops below 90% of last rebalancing H\n",
    "\n",
    "ALPHA_GRID = [0, 0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0]  #[⭐️] 12-point grid for reliable Kneedle elbow detection\n",
    "\n",
    "SCA_N_STARTS = 5                 # Multi-start optimizations (more = better optimum, proportionally slower)\n",
    "\n",
    "# --- Entropy gradient normalization (Phase 14, Finding 1) ---\n",
    "# Rescales entropy gradient to match risk gradient magnitude at each SCA iteration.\n",
    "# Without this, entropy dominates risk ~700:1, making the optimizer a pure entropy maximizer.\n",
    "NORMALIZE_ENTROPY_GRADIENT = True  #[⭐️] True = balance entropy/risk gradients (recommended)\n",
    "\n",
    "# --- Entropy budget mode (Phase 14, Finding 2) ---\n",
    "# \"proportional\" = tilted entropy with budget b_k = lambda_k / sum(lambda)\n",
    "#   Targets risk contributions proportional to eigenvalues (respects spectral structure)\n",
    "# \"uniform\" = standard Shannon entropy (equal risk contributions across factors)\n",
    "ENTROPY_BUDGET_MODE = \"proportional\"  #[⭐️] \"proportional\" (recommended) or \"uniform\"\n",
    "\n",
    "# --- Cross-sectional momentum (expected return signal) ---\n",
    "# Jegadeesh & Titman (1993): 12-month cumulative return minus last month,\n",
    "# z-scored cross-sectionally. Set MOMENTUM_ENABLED=True to activate.\n",
    "MOMENTUM_ENABLED = False           #[⭐️] DISABLED — violates Principle 1 (no return predictions) and Principle 9 (removable layer)\n",
    "MOMENTUM_LOOKBACK = 252           # Lookback window in trading days (~12 months)\n",
    "MOMENTUM_SKIP = 21                # Skip last month (avoids short-term reversal)\n",
    "MOMENTUM_WEIGHT = 0.0            #[⭐️] Scaling factor for momentum signal (gamma_mom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pipeline Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `n_stocks` | Universe size — how many stocks to keep (ranked by market cap). More = better diversification, slower training. |\n",
    "| `window_length` | Length of each input window in trading days. 504 ≈ 2 years of daily data. Each stock produces many overlapping windows. |\n",
    "| `n_features` | Features per timestep. 2 = log-return + realized volatility. |\n",
    "| `vol_window` | Lookback (trading days) for trailing volatility computation. 252 ≈ 1 year. |\n",
    "| `vix_lookback_percentile` | VIX percentile above which a day is labeled \"crisis\". Higher = fewer crisis days = less overweighting. |\n",
    "| `min_valid_fraction` | Minimum fraction of non-missing data to keep a stock. 0.80 = stocks missing >20% of their history are dropped. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAE Architecture Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `K` | Maximum latent capacity — how many risk factors the VAE *can* learn. Only \"active units\" (AU ≤ K) are actually used. Typical: 100-200. |\n",
    "| `sigma_sq_init` | Starting value of learned observation noise σ². 1.0 = assume noise equals signal at first. The model learns the true value during training (Mode P/A). |\n",
    "| `sigma_sq_min` / `sigma_sq_max` | Clamp bounds for σ². Prevents extreme values: too small = overfitting (model claims perfect reconstruction), too large = underfitting (model gives up). |\n",
    "| `window_length` / `n_features` | Must match DataPipelineConfig — determines input tensor shape (T×F). |\n",
    "| `r_max` | Maximum ratio of model parameters to data points. Safety guard — if the CNN has more parameters than the data can support, training is rejected. Relaxed automatically for small universes. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `mode` | Loss formulation. **P** = full probabilistic ELBO with learned σ² (recommended). **F** = simplified with β warmup, σ² frozen at 1.0 (use if Mode P diverges). **A** = hybrid with tunable KL weight β. |\n",
    "| `gamma` | Crisis overweighting factor. 3.0 = windows falling in crisis periods (high VIX) count 3× more in reconstruction loss. Forces the model to learn crisis dynamics well. |\n",
    "| `lambda_co_max` | Maximum co-movement loss weight. Controls how strongly latent distances must match stock Spearman correlations. Active during Phases 1-2, decays to 0 in Phase 3. |\n",
    "| `beta_fixed` | Fixed KL weight for Mode A (must be 1.0 for Mode P). Values <1 reduce regularization pressure, giving more freedom to reconstruction. |\n",
    "| `warmup_fraction` | Fraction of training where β ramps 0→1 (Mode F only). Prevents posterior collapse by letting the model learn to reconstruct before enforcing KL regularization. |\n",
    "| `max_pairs` | Max stock pairs sampled per batch for co-movement loss. Limits compute cost — full pairwise is O(B²). |\n",
    "| `delta_sync` | Maximum date gap (calendar days) for windows to be \"synchronized\" in the same time block. 21 ≈ 1 month. Ensures co-movement comparisons are temporally valid. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `max_epochs` | Hard limit on training duration. Training may stop earlier via early stopping. |\n",
    "| `batch_size` | Windows per gradient update. Larger = smoother gradients but more memory. 512 is a good default on A100. |\n",
    "| `learning_rate` | Initial optimizer step size (η₀). Too high → loss diverges. Too low → converges very slowly. Typical: 1e-4 to 1e-3. |\n",
    "| `weight_decay` | L2 regularization on model weights. Penalizes large weights to reduce overfitting. 1e-5 is mild. |\n",
    "| `adam_betas` / `adam_eps` | Adam optimizer internals (momentum and numerical stability). Rarely need tuning. |\n",
    "| `patience` | **Early stopping** — if validation ELBO doesn't improve for this many consecutive epochs, stop training and restore the best checkpoint. |\n",
    "| `lr_patience` | **LR reduction** — if validation stagnates for this many epochs, multiply LR by `lr_factor`. Triggers before early stopping. |\n",
    "| `lr_factor` | LR reduction multiplier. 0.5 = halve the learning rate each time it triggers. |\n",
    "| `n_strata` | Number of volatility-based groups for stratified batching (Phases 1-2). Ensures each batch contains stocks from all risk profiles, not just one cluster. |\n",
    "| `curriculum_phase1_frac` | Fraction of epochs for Phase 1 (co-movement at full strength + synchronized batching). |\n",
    "| `curriculum_phase2_frac` | Fraction of epochs for Phase 2 (co-movement linearly decaying). Phase 3 = remainder (free refinement, random batching). |\n",
    "| `training_stride` | Window stride for training data. 21 = one window per month per stock (21× fewer windows, much faster). Inference always uses stride=1. |\n",
    "| `compile_model` | Enable `torch.compile` on CUDA/MPS for faster forward/backward passes. ~30s warmup cost, then ~20-40% speedup. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `batch_size` | Batch size for inference pass. Can be larger than training (no gradients stored = less memory). |\n",
    "| `au_threshold` | KL threshold in nats to consider a latent dimension \"active\". 0.01 is standard — dimensions with KL < 0.01 are effectively unused (posterior ≈ prior). |\n",
    "| `r_min` | Minimum observations-per-parameter ratio. Caps AU_max = ⌊√(2·N_obs/r_min)⌋ to prevent more active factors than the data can reliably estimate. |\n",
    "| `aggregation_method` | How to combine predictions from overlapping windows for the same stock. \"mean\" averages all windows. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Risk Model Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `winsorize_lo` / `winsorize_hi` | Percentile bounds for clipping extreme volatility ratios during rescaling. [5, 95] = trim the 5% most extreme values on each side. |\n",
    "| `d_eps_floor` | Minimum idiosyncratic (stock-specific) variance. Prevents division by zero when a stock has near-zero residual risk. |\n",
    "| `conditioning_threshold` | If the covariance matrix condition number exceeds this, the factor regression switches to ridge regression for numerical stability. |\n",
    "| `ridge_scale` | Ridge regularization strength when the fallback activates. Small value (1e-6) = minimal regularization, just enough to stabilize. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Portfolio Optimization Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `lambda_risk` | Risk aversion. Higher = portfolio avoids variance more aggressively, at the cost of lower expected return. |\n",
    "| `w_max` | Hard cap per stock. 0.05 = no stock can exceed 5% of the portfolio. |\n",
    "| `w_min` | Minimum active weight. Stocks allocated below this are eliminated entirely (semi-continuous: either 0 or ≥ w_min). |\n",
    "| `w_bar` | Concentration penalty threshold. Stocks above this weight get penalized to encourage diversification. |\n",
    "| `phi` | Concentration penalty strength. Higher = more aggressively pushes weights below w_bar. |\n",
    "| `kappa_1` / `kappa_2` | Linear and quadratic turnover penalties. Penalize trading costs when rebalancing. Higher = more stable portfolio across rebalances. |\n",
    "| `delta_bar` | Turnover penalty threshold — small weight changes below this are not penalized. |\n",
    "| `tau_max` | Maximum one-way turnover per rebalance. 0.30 = at most 30% of the portfolio can change in a single rebalance. |\n",
    "| `n_starts` | Multi-start initializations for the SCA optimizer. More starts = higher chance of finding the global optimum, but proportionally slower. |\n",
    "| `sca_max_iter` / `sca_tol` | SCA (Sequential Convex Approximation) iteration limit and convergence tolerance. |\n",
    "| `armijo_*` | Line search parameters (sufficient decrease, backtracking factor, max steps). Controls step size selection within SCA. |\n",
    "| `max_cardinality_elim` | Maximum rounds of sequential stock elimination to enforce the minimum weight constraint. |\n",
    "| `entropy_eps` | Tiny constant (1e-30) added inside log() to avoid log(0). Pure numerical safety. |\n",
    "| `alpha_grid` | Grid of α values for the variance-entropy frontier. The optimizer tries each α and picks the best tradeoff between risk and factor diversification. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Walk-Forward Validation Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `total_years` | Total history length used for the walk-forward. More years = more folds = more robust evaluation, but requires more data. |\n",
    "| `min_training_years` | Minimum training window. The first fold starts with this many years of training data. Too small = unreliable model, too large = few folds. |\n",
    "| `oos_months` | Out-of-sample test period per fold. After training, the portfolio is tested on this many months, then the window slides forward. |\n",
    "| `embargo_days` | Gap (trading days) between training end and OOS start. Prevents information leakage from overlapping windows near the boundary. 21 ≈ 1 month. |\n",
    "| `holdout_years` | Final holdout period excluded from all training/testing. Reserved for ultimate out-of-sample validation. |\n",
    "| `val_years` | Nested validation window within training for Phase A hyperparameter selection. Used to score HP configs without touching OOS data. |\n",
    "| `score_lambda_pen` | Weight of maximum drawdown penalty in the composite HP scoring function. Higher = favor configs with lower drawdowns. |\n",
    "| `score_lambda_est` | Weight of estimation quality penalty (variance ratio) in scoring. Higher = favor configs with more accurate risk predictions. |\n",
    "| `score_mdd_threshold` | Maximum drawdown threshold. Drawdowns beyond this are heavily penalized in the composite score. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Quick Mode\n",
    "\n",
    "Run **only this cell** to configure the pipeline for a minimal end-to-end test. Skip Section 2b entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# QUICK MODE — Minimal config for end-to-end testing\n",
    "# Run ONLY this cell, then jump to Section 3\n",
    "# ============================================================\n",
    "\n",
    "if QUICK_MODE == True or DATA_SOURCE == \"synthetic\":\n",
    "    DATA_PATH = \"\"\n",
    "    N_STOCKS = 50   # How many stocks in the universe (ranked by market cap)\n",
    "    N_YEARS = 20    # Years of history to use\n",
    "\n",
    "    config = PipelineConfig(\n",
    "        data=DataPipelineConfig(\n",
    "            n_stocks=N_STOCKS,       # Universe size (top N by market cap)\n",
    "            window_length=504,       # Input window = 504 trading days (~2 years)\n",
    "            n_features=2,            # 2 features per timestep: log-return + realized vol\n",
    "            training_stride=TRAINING_STRIDE,  # Window stride for training (21 = monthly, 1 = daily)\n",
    "        ),\n",
    "        vae=VAEArchitectureConfig(\n",
    "            K=100,                   # Max latent factors the VAE can discover (only AU will be active)\n",
    "            window_length=504,       # Must match data window_length\n",
    "            n_features=2,            # Must match data n_features\n",
    "            r_max=5.0,               # Max param/data ratio (relaxed for small universes, auto-adjusted)\n",
    "        ),\n",
    "        loss=LossConfig(\n",
    "            mode=\"P\",                # \"P\" = full probabilistic ELBO with learned observation noise sigma^2\n",
    "        ),\n",
    "        training=TrainingConfig(\n",
    "            max_epochs=50,           # Hard limit on training (early stopping may trigger sooner)\n",
    "            batch_size=512,          # Windows per gradient update (512 = good balance speed/stability)\n",
    "            learning_rate=1e-4,      # Initial optimizer step size (typical: 1e-4 to 1e-3)\n",
    "            patience=30,             # Stop if validation doesn't improve for 30 epochs\n",
    "            compile_model=COMPILE_MODEL,  # torch.compile on CUDA/MPS\n",
    "        ),\n",
    "        inference=InferenceConfig(),   # Defaults: batch_size=512, au_threshold=0.01, r_min=2\n",
    "        risk_model=RiskModelConfig(),  # Defaults: winsorize=[5,95], cond_threshold=1e6, ridge=1e-6\n",
    "        portfolio=PortfolioConfig(\n",
    "            n_starts=2,              # Multi-start optimizations (2 = fast, 5+ = production)\n",
    "        ),\n",
    "        walk_forward=WalkForwardConfig(\n",
    "            total_years=N_YEARS,             # Total history for walk-forward folds\n",
    "            min_training_years=max(3, N_YEARS // 3),  # Min training window (>=3y for reliability)\n",
    "            holdout_years=max(1, N_YEARS // 5),       # Final holdout excluded from all folds (>=1y)\n",
    "        ),\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    # Single HP config (skip Phase A grid search for speed)\n",
    "    HP_GRID = [{\"mode\": \"P\", \"learning_rate\": 1e-4, \"alpha\": 1.0}]\n",
    "\n",
    "    print(f\"[Quick mode] {N_STOCKS} stocks, {N_YEARS} years, K={config.vae.K}\")\n",
    "    print(f\"  max_epochs={config.training.max_epochs}, patience={config.training.patience}, HP_GRID=1 config, n_starts=2\")\n",
    "    print(f\"  r_max={config.vae.r_max:.0e}, training_stride={config.data.training_stride}, compile={config.training.compile_model}\")\n",
    "    print(f\"  Walk-forward: {config.walk_forward.total_years}y total, \"\n",
    "        f\"{config.walk_forward.min_training_years}y min training, \"\n",
    "        f\"{config.walk_forward.holdout_years}y holdout\")\n",
    "    print(f\"  Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Real Data (Production)\n",
    "\n",
    "Run **all cells below** (through \"ASSEMBLE FULL CONFIG\") for full production configuration. Skip Section 2a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA SOURCE — Real data\n",
    "# ============================================================\n",
    "# For CSV source:\n",
    "DATA_PATH = \"data/stock_data.csv\"  # <-- Set path to your stock data CSV\n",
    "\n",
    "# For Tiingo source: run download first:\n",
    "#   python scripts/download_tiingo.py --phase all --keys-file keys.txt\n",
    "# Then set DATA_SOURCE = \"tiingo\" in Global cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pipeline Parameters\n",
    "\n",
    "| Parameter | What it controls |\n",
    "|---|---|\n",
    "| `n_stocks` | Universe size — how many stocks to keep (ranked by market cap). More = better diversification, slower training. |\n",
    "| `window_length` | Length of each input window in trading days. 504 ≈ 2 years of daily data. Each stock produces many overlapping windows. |\n",
    "| `n_features` | Features per timestep. 2 = log-return + realized volatility. |\n",
    "| `vol_window` | Lookback (trading days) for trailing volatility computation. 252 ≈ 1 year. |\n",
    "| `vix_lookback_percentile` | VIX percentile above which a day is labeled \"crisis\". Higher = fewer crisis days = less overweighting. |\n",
    "| `min_valid_fraction` | Minimum fraction of non-missing data to keep a stock. 0.80 = stocks missing >20% of their history are dropped. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA PIPELINE (MOD-001)\n",
    "# ============================================================\n",
    "data_cfg = DataPipelineConfig(\n",
    "    n_stocks=N_STOCKS,           # universe cap (same as N_STOCKS above)\n",
    "    window_length=T,             # T: sliding window length (trading days)\n",
    "    n_features=N_FEATURES,       # F: features per timestep (return + realized vol)\n",
    "    vol_window=252,              # trailing vol lookback (days)\n",
    "    vix_lookback_percentile=80.0,# VIX percentile for crisis threshold\n",
    "    min_valid_fraction=0.80,     # Drop stocks missing >20% of their price history\n",
    "    training_stride=TRAINING_STRIDE,  # Window stride for training (21 = monthly, 1 = daily; inference always uses stride=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VAE ARCHITECTURE (MOD-002)\n",
    "# ============================================================\n",
    "vae_cfg = VAEArchitectureConfig(\n",
    "    K=K,                                   # Max latent factors the VAE can discover (only \"active units\" AU<=K are used)\n",
    "    sigma_sq_init=1.0,                     # Initial observation noise (1.0 = assume noise = signal, model learns true value)\n",
    "    sigma_sq_min=1e-4,                     # Lower clamp for sigma^2 (prevents overfitting: model can't claim 0 noise)\n",
    "    sigma_sq_max=10.0,                     # Upper clamp for sigma^2 (prevents divergence: model can't give up entirely)\n",
    "    window_length=data_cfg.window_length,  # Must match data_cfg.window_length\n",
    "    n_features=data_cfg.n_features,        # Must match data_cfg.n_features\n",
    "    r_max=5.0,                             # Max model params / data points ratio (safety guard against overfitting)\n",
    "    dropout=DROPOUT,                           # Dropout rate for VAE residual blocks (0.1=standard, 0.2=reinforced for small universes)\n",
    ")\n",
    "\n",
    "print(f\"Encoder depth L={vae_cfg.encoder_depth}, \"\n",
    "      f\"Final width C_L={vae_cfg.final_layer_width}, \"\n",
    "      f\"D={vae_cfg.D}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOSS FUNCTION (MOD-004)\n",
    "# ============================================================\n",
    "loss_cfg = LossConfig(\n",
    "    mode=LOSS_MODE,                        # Loss mode: \"P\"=full ELBO with learned sigma^2, \"F\"=simplified with beta warmup, \"A\"=hybrid\n",
    "    gamma=CRISIS_OVERWEIGHTING,            # Crisis overweighting: crisis windows count 3x more in reconstruction loss\n",
    "    lambda_co_max=MAX_CO_MOVEMENT_WEIGHT,  # Max co-movement weight: how strongly latent distances must match Spearman correlations\n",
    "    beta_fixed=1.0,                        # Fixed KL weight for Mode A (must be 1.0 for Mode P)\n",
    "    warmup_fraction=0.20,                  # Mode F only: fraction of epochs to ramp beta 0->1 (prevents posterior collapse)\n",
    "    max_pairs=2048,                        # Max stock pairs per batch for co-movement loss (limits O(B^2) compute)\n",
    "    delta_sync=21,                         # Max date gap (days) for windows to be \"synchronized\" (21 ≈ 1 month)\n",
    "    lambda_cs=LAMBDA_CS,                   # Cross-sectional R² loss weight (forces encoder to produce useful factor exposures)\n",
    "    cs_n_sample_dates=CS_N_SAMPLE_DATES,   # Time offsets sampled per batch for cross-sectional loss\n",
    "    feature_weights=FEATURE_WEIGHTS,       # Per-feature reconstruction weights [returns, vol] (addresses 10× reconstruction imbalance)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING (MOD-005)\n",
    "# ============================================================\n",
    "training_cfg = TrainingConfig(\n",
    "    max_epochs=MAX_EPOCHS,            # Hard limit on training duration (early stopping usually triggers sooner)\n",
    "    batch_size=BATCH_SIZE,            # Windows per gradient step (larger = smoother but more memory)\n",
    "    learning_rate=LEARNING_RATE,      # Initial optimizer step size (too high = diverges, too low = slow)\n",
    "    weight_decay=1e-5,                # L2 penalty on weights to reduce overfitting (1e-5 = mild)\n",
    "    adam_betas=(0.9, 0.999),          # Adam momentum parameters (rarely need tuning)\n",
    "    adam_eps=1e-8,                    # Adam numerical stability constant (rarely need tuning)\n",
    "    patience=EARLY_STOPPING_PATIENCE, # Early stopping: stop after N epochs without sufficient improvement\n",
    "    es_min_delta=ES_MIN_DELTA,        # Min ELBO drop to count as improvement (higher = stops sooner)\n",
    "    lr_patience=LR_PATIENCE,          # Reduce LR if validation stagnates for N epochs (triggers before early stop)\n",
    "    lr_factor=LR_FACTOR,              # LR multiplier when reducing (0.75 = reduce by 25% each time)\n",
    "    n_strata=15,                      # Volatility groups for stratified batching (ensures risk diversity per batch)\n",
    "    curriculum_phase1_frac=0.30,      # Phase 1: 30% of epochs with full co-movement + synchronized batching\n",
    "    curriculum_phase2_frac=0.30,      # Phase 2: 30% of epochs with co-movement linearly decaying to 0\n",
    "                                      # Phase 3: remaining 30% with no co-movement, random batching\n",
    "    compile_model=COMPILE_MODEL,      # torch.compile on CUDA/MPS (faster forward/backward, ~30s warmup)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INFERENCE (MOD-006)\n",
    "# After training, the encoder runs on all windows to extract each stock's\n",
    "# latent profile. These parameters control that extraction pass and\n",
    "# decide how many latent dimensions are kept as \"active\" risk factors.\n",
    "# ============================================================\n",
    "inference_cfg = InferenceConfig(\n",
    "    batch_size=512,                          # Larger than training (no gradients = less memory needed)\n",
    "    au_threshold=AU_THRESHOLD,               # KL > 0.01 nats = dimension is \"active\" (below = unused, posterior ≈ prior)\n",
    "    r_min=2,                                 # Min data/param ratio — caps AU_max to prevent more factors than data supports\n",
    "    aggregation_method=AGGREGATION_METHOD,   # Average overlapping window predictions for each stock's final profile\n",
    "    aggregation_half_life=AGGREGATION_HALF_LIFE,  # Exponential decay: 0=uniform, 60=recent windows weighted (~5yr half-life)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RISK MODEL (MOD-007)\n",
    "# Transforms latent exposures (B matrix) into a full covariance\n",
    "# matrix Sigma_assets via cross-sectional regression and\n",
    "# Ledoit-Wolf shrinkage. These parameters handle numerical\n",
    "# edge cases (extreme vol ratios, ill-conditioned matrices).\n",
    "# ============================================================\n",
    "risk_model_cfg = RiskModelConfig(\n",
    "    winsorize_lo=5.0,            # Clip extreme vol ratios below 5th percentile (removes outliers)\n",
    "    winsorize_hi=95.0,           # Clip extreme vol ratios above 95th percentile\n",
    "    d_eps_floor=1e-6,            # Min idiosyncratic variance per stock (prevents division by zero)\n",
    "    conditioning_threshold=1e6,  # Switch to ridge regression if covariance condition number exceeds this\n",
    "    ridge_scale=1e-6,            # Ridge regularization strength (tiny, just enough to stabilize)\n",
    "    sigma_z_eigenvalue_pct=SIGMA_Z_EIGENVALUE_PCT,  # Keep top eigenvalues explaining X% of Sigma_z variance (1.0=all)\n",
    "    b_a_shrinkage_alpha=B_A_SHRINKAGE_ALPHA,  # Shrink B_A towards zero (0.0=off, 0.1=10% shrinkage)\n",
    "    au_max_bai_ng_factor=0.0,    # DISABLED (0=off) — confirmed harmful in V2 (eigenvalue concentration 88%)\n",
    "    vt_clamp_min=0.5,            # VT clamp: min scale factor (prevents pro-cyclical VT from halving eigenvalues)\n",
    "    vt_clamp_max=2.0,            # VT clamp: max scale factor (limits over-correction during calm periods)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PORTFOLIO OPTIMIZATION (MOD-008)\n",
    "# Constraints identical for VAE and all 6 benchmarks (INV-012)\n",
    "#\n",
    "# Variance (w'Σw) = total portfolio risk. Minimizing it concentrates\n",
    "# weight on low-vol stocks — but can put 85% of risk on 2-3 latent\n",
    "# factors without noticing (fragile to factor shocks).\n",
    "#\n",
    "# Entropy H(w) = Shannon entropy on each factor's risk contribution.\n",
    "# Maximizing it spreads risk evenly across all active factors — no\n",
    "# single factor dominates (resilient to individual factor crashes).\n",
    "#\n",
    "# lambda_risk and alpha control the tradeoff: more lambda_risk favors\n",
    "# low total variance; more alpha favors even factor risk distribution.\n",
    "#\n",
    "# lambda_risk guide:\n",
    "#   0.1  → entropy dominates (pure factor risk parity, higher variance OK)\n",
    "#   0.5  → entropy favored, soft variance constraint\n",
    "#   1.0  → balanced (default, let alpha_grid find the right ratio)\n",
    "#   2.0  → variance favored (more conservative, fewer dominant factors OK)\n",
    "#   5.0+ → near min-variance (entropy has little influence)\n",
    "#\n",
    "# Penalty tuning guide (defaults calibrated for US mid/large cap):\n",
    "#   Illiquid universe (small caps) → raise kappa_1 & kappa_2\n",
    "#   Infrequent rebalancing         → lower kappa_1 & kappa_2\n",
    "#   Allow more concentrated bets   → lower phi and/or raise w_bar\n",
    "#   Force tighter diversification  → raise phi and/or lower w_bar\n",
    "#   Smoother trades                → lower delta_bar (e.g. 0.005)\n",
    "#\n",
    "# Valid ranges from DVT spec:\n",
    "#   phi: [5, 100]  kappa_1: [0.01, 1.0]  kappa_2: [1, 50]  delta_bar: [0.005, 0.03]\n",
    "# ============================================================\n",
    "portfolio_cfg = PortfolioConfig(\n",
    "    lambda_risk=LAMBDA_RISK,     # Risk aversion (higher = more conservative, lower variance portfolio)\n",
    "    w_max=W_MAX,                 # Hard cap per stock (CVXPY constraint: w_i <= w_max)\n",
    "    w_min=W_MIN,                 # Min active weight: below this, stock is eliminated (0 or >= 0.1%)\n",
    "    w_bar=W_BAR,                 # Soft cap: concentration penalty kicks in above this weight\n",
    "    phi=PHI,                     # Concentration penalty strength above w_bar (phi * sum(max(0, w_i - w_bar)^2))\n",
    "    kappa_1=KAPPA_1,             # Linear turnover penalty (penalizes trading costs at rebalance)\n",
    "    kappa_2=KAPPA_2,             # Quadratic turnover penalty (penalizes large trades more than small ones)\n",
    "    delta_bar=DELTA_BAR,         # Turnover below 1% is not penalized (de minimis threshold)\n",
    "    tau_max=MAX_TURNOVER,        # Max 30% portfolio change per rebalance (hard cap on turnover)\n",
    "    n_starts=SCA_N_STARTS,       # Multi-start optimizations (more = better optimum, proportionally slower)\n",
    "    sca_max_iter=300,            # Max iterations for the SCA convex optimizer\n",
    "    sca_tol=1e-5,                # SCA convergence tolerance (stop when improvement < this)\n",
    "    armijo_c=1e-4,               # Line search: sufficient decrease constant\n",
    "    armijo_rho=0.5,              # Line search: backtracking factor (halve step on each retry)\n",
    "    armijo_max_iter=20,          # Line search: max backtracking attempts per SCA step\n",
    "    max_cardinality_elim=100,    # Max rounds of stock elimination to enforce w_min constraint\n",
    "    entropy_eps=1e-30,           # Tiny constant in log() for numerical safety (avoids log(0))\n",
    "    alpha_grid=ALPHA_GRID,       # Risk-entropy tradeoff grid (optimizer picks best alpha)\n",
    "    normalize_entropy_gradient=NORMALIZE_ENTROPY_GRADIENT,  # Balance entropy/risk gradient scales\n",
    "    entropy_budget_mode=ENTROPY_BUDGET_MODE,                # \"proportional\" or \"uniform\" entropy target\n",
    "    momentum_enabled=MOMENTUM_ENABLED,    # Toggle momentum signal (True/False)\n",
    "    momentum_lookback=MOMENTUM_LOOKBACK,  # 12-month lookback (trading days)\n",
    "    momentum_skip=MOMENTUM_SKIP,          # Skip last month (reversal)\n",
    "    momentum_weight=MOMENTUM_WEIGHT,      # Scaling factor for momentum signal\n",
    "    # OOS Rebalancing (DVT §4.2)\n",
    "    rebalancing_frequency_days=REBALANCING_FREQUENCY,  # Days between rebalancing (0=buy-hold)\n",
    "    entropy_trigger_alpha=ENTROPY_TRIGGER_ALPHA,       # Exceptional trigger threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# WALK-FORWARD VALIDATION (MOD-009)\n",
    "# ============================================================\n",
    "walk_forward_cfg = WalkForwardConfig(\n",
    "    total_years=N_YEARS,            # Total history used (more years = more folds = more robust evaluation)\n",
    "    min_training_years=MIN_N_YEARS, # First fold trains on at least MIN_N_YEARS (shorter = unreliable model)\n",
    "    oos_months=6,                   # Test each fold on 6 months out-of-sample, then slide forward\n",
    "    embargo_days=21,                # 21-day gap between training and test (prevents data leakage from windows)\n",
    "    holdout_years=3,                # Last 3 years excluded from all folds (final out-of-sample validation)\n",
    "    val_years=2,                    # 2-year nested validation within training for HP selection (Phase A)\n",
    "    score_lambda_pen=5.0,           # HP scoring: penalty weight for max drawdown (higher = favor low-drawdown configs)\n",
    "    score_lambda_est=2.0,           # HP scoring: penalty weight for poor risk estimation accuracy\n",
    "    score_mdd_threshold=0.20,       # HP scoring: drawdowns above 20% get heavily penalized\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ASSEMBLE FULL CONFIG\n",
    "# ============================================================\n",
    "if DATA_SOURCE != \"synthetic\" and QUICK_MODE == False:\n",
    "      config = PipelineConfig(\n",
    "            data=data_cfg,\n",
    "            vae=vae_cfg,\n",
    "            loss=loss_cfg,\n",
    "            training=training_cfg,\n",
    "            inference=inference_cfg,\n",
    "            risk_model=risk_model_cfg,\n",
    "            portfolio=portfolio_cfg,\n",
    "            walk_forward=walk_forward_cfg,\n",
    "            seed=SEED,\n",
    "      )\n",
    "\n",
    "      print(\"PipelineConfig assembled.\")\n",
    "      print(f\"  Walk-forward: {config.walk_forward.total_years}y total, \"\n",
    "            f\"{config.walk_forward.min_training_years}y min training, \"\n",
    "            f\"{config.walk_forward.holdout_years}y holdout\")\n",
    "      print(f\"  VAE: K={config.vae.K}, T={config.vae.window_length}, F={config.vae.n_features}\")\n",
    "      print(f\"  Training: {config.training.max_epochs} max epochs, \"\n",
    "            f\"bs={config.training.batch_size}, lr={config.training.learning_rate}\")\n",
    "      print(f\"  Loss mode: {config.loss.mode}, gamma={config.loss.gamma}\")\n",
    "      print(f\"  Capacity guard r_max: {config.vae.r_max}\")\n",
    "      print(f\"  Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Tiingo Download (run once)\n",
    "\n",
    "Run this cell **only once** to download Tiingo data. After the first run, the data is saved locally and reused automatically.\n",
    "Set `MAX_TICKERS` to a small number (e.g. 5) for testing, or `None` for the full universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TIINGO DOWNLOAD — Run once, data is saved locally\n",
    "# Skip this cell if data is already downloaded or DATA_SOURCE != \"tiingo\"\n",
    "# ============================================================\n",
    "\n",
    "if DATA_SOURCE == \"tiingo\":\n",
    "    import importlib.util\n",
    "\n",
    "    _spec = importlib.util.spec_from_file_location(\n",
    "        \"download_tiingo\", str(PROJECT_ROOT / \"scripts\" / \"download_tiingo.py\")\n",
    "    )\n",
    "    _mod = importlib.util.module_from_spec(_spec)\n",
    "    _spec.loader.exec_module(_mod)  # type: ignore[union-attr]\n",
    "\n",
    "    MAX_TICKERS = None  # Set to None for full universe (~22k tickers)\n",
    "\n",
    "    _mod.run_download(\n",
    "        api_keys=TIINGO_API_KEYS,\n",
    "        data_dir=DATA_DIR,\n",
    "        max_tickers=MAX_TICKERS,\n",
    "        end_date=END_DATE,\n",
    "        sp500_first=True,  # Download SP500 tickers first (priority)\n",
    "    )\n",
    "else:\n",
    "    print(f\"DATA_SOURCE={DATA_SOURCE}, skipping Tiingo download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "\n",
    "stock_data, start_date = load_data_source(\n",
    "    source=DATA_SOURCE,\n",
    "    data_path=DATA_PATH if DATA_SOURCE == \"csv\" else \"\",\n",
    "    data_dir=DATA_DIR,\n",
    "    n_stocks=N_STOCKS,\n",
    "    n_years=N_YEARS,\n",
    "    seed=SEED,\n",
    "    end_date=END_DATE,\n",
    ")\n",
    "\n",
    "print(f\"Data source: {DATA_SOURCE}\")\n",
    "print(f\"Stock data shape: {stock_data.shape}\")\n",
    "print(f\"Date range: {stock_data['date'].min()} to {stock_data['date'].max()}\")\n",
    "print(f\"Unique stocks: {stock_data['permno'].nunique()}\")\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log-returns and trailing volatility\n",
    "returns = compute_log_returns(stock_data)\n",
    "trailing_vol = compute_trailing_volatility(returns, window=config.data.vol_window)\n",
    "\n",
    "nb_effective_years = returns.shape[0]/252\n",
    "nb_effective_stocks = returns.shape[1]\n",
    "\n",
    "print(f\"Returns: {returns.shape[0]} dates x {returns.shape[1]} stocks\")\n",
    "print(f\"Trailing vol: {trailing_vol.shape} (first {config.data.vol_window-1} rows NaN)\")\n",
    "print(f\"Returns date range: {returns.index[0]} to {returns.index[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Run Pipeline\n",
    "\n",
    "Executes the full walk-forward validation: Phase A (HP selection) + Phase B (deployment) on each fold, then benchmarks, statistical tests, and report generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Value Interpretation\n",
    "\n",
    "The input windows are **z-scored** (mean=0, std=1), so metric values are directly interpretable as fractions of signal variance.\n",
    "\n",
    "#### `reconstruction` (Step/ and Loss/)\n",
    "Raw per-element MSE **before** D/(2σ²) scaling. Since data is z-scored, L_recon ≈ fraction of unexplained variance.\n",
    "\n",
    "| L_recon | RMSE | Interpretation |\n",
    "|---|---|---|\n",
    "| **1.0** | 1.0 | No better than predicting the mean. Not learning. |\n",
    "| **0.5** | 0.71 | ~50% variance explained. Poor. |\n",
    "| **0.1** | 0.32 | ~90% variance explained. Decent. |\n",
    "| **0.05** | 0.22 | ~95% explained. Good for financial data. |\n",
    "| **0.01** | 0.10 | ~99% explained. Likely overfitting on noisy data. |\n",
    "\n",
    "Good converged range on real data: **0.05–0.20**.\n",
    "\n",
    "#### `kl_divergence` (Step/ and Loss/)\n",
    "KL **summed over K dimensions**, averaged over batch. Raw value scales linearly with K — divide by K for per-dimension KL (nats).\n",
    "\n",
    "| L_KL (K=100) | KL/dim | Interpretation |\n",
    "|---|---|---|\n",
    "| **0** | 0 | **Posterior collapse.** Encoder ignores input entirely. |\n",
    "| **1** | 0.01 | Right at AU threshold. Quasi-collapsed. |\n",
    "| **10** | 0.1 | ~10 weakly active dimensions. Most collapsed. |\n",
    "| **50** | 0.5 | ~50 active dims, moderate info per dim. Typical early-mid training. |\n",
    "| **100** | 1.0 | ~100 active dims at ~1 nat each. Healthy, full capacity used. |\n",
    "| **250** | 2.5 | Very informative. If AU ≪ 100, a few dims are dominating. |\n",
    "\n",
    "Good converged range (K=100): **25–150**. For K=200, double these values.\n",
    "\n",
    "#### `co_movement` (Step/ and Loss/)\n",
    "MSE between latent cosine distances and Spearman correlation targets: `(1/|P|) × Σ (d_cos(μ_i, μ_j) − (1 − ρ_ij))²`.\n",
    "\n",
    "| L_co | Avg error | Interpretation |\n",
    "|---|---|---|\n",
    "| **0.01** | 0.10 | Excellent alignment between latent distances and Spearman. |\n",
    "| **0.05** | 0.22 | Good. Reasonable after Phase 1. |\n",
    "| **0.10** | 0.32 | Moderate. Early training territory. |\n",
    "| **0.30** | 0.55 | Poor. Random initialization level. |\n",
    "| **> 0.5** | > 0.7 | Latent space disorganized w.r.t. co-movements. |\n",
    "\n",
    "**Key: this value is only meaningful when λ_co > 0 (Phases 1-2).** In Phase 3, the value is 0.0 because `compute_co_movement_loss` is skipped entirely.\n",
    "\n",
    "#### `sigma_sq` (Step/ and Loss/)\n",
    "Learned observation noise σ² = clamp(exp(log_sigma_sq), 1e-4, 10). **At equilibrium in Mode P, σ² converges to ≈ L_recon** (the model's own reconstruction MSE).\n",
    "\n",
    "| σ² | Variance explained | Interpretation |\n",
    "|---|---|---|\n",
    "| **10.0** (clamp max) | ~0% | Model giving up on reconstruction. Diverging. |\n",
    "| **1.0** | ~0% | Noise = signal. Initial state / Mode F (frozen). |\n",
    "| **0.3–0.5** | 50–70% | Early-mid convergence. |\n",
    "| **0.05–0.2** | 80–95% | Healthy convergence on financial data. |\n",
    "| **0.01** | ~99% | Possibly overfitting. |\n",
    "| **1e-4** (clamp min) | ~100% | Hitting floor. Overfitting. |\n",
    "\n",
    "Mode F: σ² is **frozen at 1.0** — ignore this metric.\n",
    "\n",
    "#### `Validation/ELBO` (val_elbo)\n",
    "**Primary model selection metric** — used for early stopping and LR scheduling. Combines reconstruction quality, KL regularization, and observation noise into a single comparable score:\n",
    "\n",
    "`L_val = D/(2σ²) · MSE(γ=1) + (D/2)·ln(σ²) + KL`\n",
    "\n",
    "where D = T × F = 1008 (for T=504, F=2). **Excludes** crisis weighting (γ=1) and co-movement loss (INV-011), so it is comparable across curriculum phases.\n",
    "\n",
    "Since D scales the first two terms, raw values are large. Typical ranges for D=1008:\n",
    "\n",
    "| val_elbo | Regime | Interpretation |\n",
    "|---|---|---|\n",
    "| **> 1000** | Untrained | Random initialization or diverging. D/2 · ln(σ²=1) ≈ 0 + D/2 · MSE(≈1) ≈ 504. Above 1000 = KL also high. |\n",
    "| **500–1000** | Early training | Reconstruction improving, σ² still near 1.0. |\n",
    "| **200–500** | Mid training | σ² dropping, reconstruction sharpening. Healthy convergence territory. |\n",
    "| **50–200** | Good convergence | Strong reconstruction, well-calibrated σ², meaningful latent structure. |\n",
    "| **< 50** | Excellent / overfitting | If AU is healthy (>20), excellent. If AU is very low, may indicate posterior collapse with tight σ². |\n",
    "\n",
    "**Key: lower is better.** A plateau or increase triggers LR reduction (after `lr_patience` epochs) and eventually early stopping (after `patience` epochs). The best checkpoint (lowest val_elbo) is restored at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LIVE TENSORBOARD (stable in Colab)\n",
    "# Run this cell BEFORE starting training (Section 4).\n",
    "# The dashboard stays live during training — no need to re-run.\n",
    "#\n",
    "# --reload_interval 30 : reload every 30s instead of default 5s\n",
    "#   → this is what prevents the crash after a few minutes\n",
    "# ============================================================\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/ --reload_interval 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_FULL_PIPELINE:\n",
    "      print(\"=\" * 60)\n",
    "      print(\"PIPELINE CONFIGURATION SUMMARY\")\n",
    "      print(\"=\" * 60)\n",
    "      print(f\"  Seed: {SEED} | Device: {DEVICE} | Data: {DATA_SOURCE}\")\n",
    "      if CHECKPOINT_PATH:\n",
    "            print(f\"  Pretrained model: {CHECKPOINT_PATH} (skipping VAE training)\")\n",
    "      print()\n",
    "      print(f\"  [Data]      N_STOCKS={N_STOCKS}, N_effective_stocks={nb_effective_stocks}, T={T}, N_FEATURES={N_FEATURES}, N_YEARS={N_YEARS}, N_effective_years={nb_effective_years}\")\n",
    "      print(f\"  [VAE]       K={K}, LOSS_MODE={LOSS_MODE}\")\n",
    "      print(f\"  [Training]  MAX_EPOCHS={MAX_EPOCHS}, BATCH_SIZE={BATCH_SIZE}, LEARNING_RATE={LEARNING_RATE}, EARLY_STOPPING_PATIENCE={EARLY_STOPPING_PATIENCE}\")\n",
    "      print(f\"              TRAINING_STRIDE={TRAINING_STRIDE}, COMPILE_MODEL={COMPILE_MODEL}\")\n",
    "      print(f\"  [Loss]      CRISIS_OVERWEIGHTING={CRISIS_OVERWEIGHTING}, MAX_CO_MOVEMENT_WEIGHT={MAX_CO_MOVEMENT_WEIGHT}\")\n",
    "      print(f\"  [Portfolio]  LAMBDA_RISK={LAMBDA_RISK}, W_MAX={W_MAX}, W_MIN={W_MIN}, SCA_N_STARTS={SCA_N_STARTS}\")\n",
    "      print(f\"  [HP Grid]   {len(HP_GRID) if HP_GRID else '18 (default)'} configs\")\n",
    "      print(\"=\" * 60)\n",
    "\n",
    "      TB_DIR = \"runs/\"  # TensorBoard log directory (set to None to disable)\n",
    "\n",
    "      pipeline = FullPipeline(config, tensorboard_dir=TB_DIR)\n",
    "\n",
    "      skip_phase_a = True\n",
    "\n",
    "      results = pipeline.run(\n",
    "      stock_data=stock_data,\n",
    "      returns=returns,\n",
    "      trailing_vol=trailing_vol,\n",
    "      skip_phase_a=(DATA_SOURCE == \"synthetic\" or QUICK_MODE == True or skip_phase_a),\n",
    "      vix_data=None,\n",
    "      start_date=start_date,\n",
    "      hp_grid=HP_GRID,\n",
    "      device=DEVICE,\n",
    "      pretrained_model=CHECKPOINT_PATH,\n",
    "      )\n",
    "\n",
    "      print(\"Pipeline complete.\")\n",
    "      print(f\"Folds processed: {len(results['vae_results'])}\")\n",
    "      print(f\"Benchmarks: {list(results['benchmark_results'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4b. Direct Training (Skip Walk-Forward)\n",
    "\n",
    "Train the VAE on the entire period minus a holdout, then evaluate on the holdout.\n",
    "**Run EITHER Section 4 (walk-forward) OR Section 4b (direct) -- not both.**\n",
    "\n",
    "Use this mode for:\n",
    "- Quick iteration during development\n",
    "- Single-split evaluation before committing to full walk-forward\n",
    "- Inspecting model internals (B matrix, latent factors, risk model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RUN DIRECT TRAINING\n",
    "# ============================================================\n",
    "if not RUN_FULL_PIPELINE:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PIPELINE CONFIGURATION SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Seed: {SEED} | Device: {DEVICE} | Data: {DATA_SOURCE}\")\n",
    "    if CHECKPOINT_PATH:\n",
    "        print(f\"  Pretrained model: {CHECKPOINT_PATH} (skipping VAE training)\")\n",
    "    print()\n",
    "    print(f\"  [Data]      N_STOCKS={N_STOCKS}, N_effective_stocks={nb_effective_stocks}, T={T}, N_FEATURES={N_FEATURES}, N_YEARS={N_YEARS}, N_effective_years={nb_effective_years}\")\n",
    "    print(f\"  [VAE]       K={K}, LOSS_MODE={LOSS_MODE}\")\n",
    "    print(f\"  [Training]  MAX_EPOCHS={MAX_EPOCHS}, BATCH_SIZE={BATCH_SIZE}, LEARNING_RATE={LEARNING_RATE}, EARLY_STOPPING_PATIENCE={EARLY_STOPPING_PATIENCE}\")\n",
    "    print(f\"              TRAINING_STRIDE={TRAINING_STRIDE}, COMPILE_MODEL={COMPILE_MODEL}\")\n",
    "    print(f\"  [Loss]      CRISIS_OVERWEIGHTING={CRISIS_OVERWEIGHTING}, MAX_CO_MOVEMENT_WEIGHT={MAX_CO_MOVEMENT_WEIGHT}\")\n",
    "    print(f\"  [Portfolio]  LAMBDA_RISK={LAMBDA_RISK}, W_MAX={W_MAX}, W_MIN={W_MIN}, SCA_N_STARTS={SCA_N_STARTS}\")\n",
    "    print(f\"  [HP Grid]   {len(HP_GRID) if HP_GRID else '18 (default)'} configs\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    HOLDOUT_START = None  # Option A: Set an explicit holdout start date (e.g. \"2023-01-03\") — set to None for automatic split\n",
    "    HOLDOUT_FRACTION = 0.10  # Option B: Fraction of dates reserved for holdout (ignored if HOLDOUT_START is set)\n",
    "    RUN_BENCHMARKS = True # Run benchmarks on the same split for comparison?\n",
    "\n",
    "    TB_DIR = \"runs/\"\n",
    "\n",
    "    pipeline = FullPipeline(config, tensorboard_dir=TB_DIR)\n",
    "\n",
    "    results = pipeline.run_direct(\n",
    "        stock_data=stock_data,\n",
    "        returns=returns,\n",
    "        trailing_vol=trailing_vol,\n",
    "        vix_data=None,\n",
    "        start_date=start_date,\n",
    "        hp_grid=HP_GRID,\n",
    "        device=DEVICE,\n",
    "        holdout_start=HOLDOUT_START,\n",
    "        holdout_fraction=HOLDOUT_FRACTION,\n",
    "        run_benchmarks=RUN_BENCHMARKS,\n",
    "        pretrained_model=CHECKPOINT_PATH,\n",
    "    )\n",
    "\n",
    "    print(\"Direct training complete.\")\n",
    "    if CHECKPOINT_PATH:\n",
    "        print(f\"  (Loaded pretrained encoder from: {CHECKPOINT_PATH})\")\n",
    "    print(f\"  Train: {results['fold_schedule'][0]['train_start']} to {results['train_end']}\")\n",
    "    print(f\"  Test:  {results['oos_start']} to {results['oos_end']}\")\n",
    "    print(f\"  Sharpe: {results['vae_results'][0].get('sharpe', 0.0):.3f}\")\n",
    "    print(f\"  AU: {results['vae_results'][0].get('AU', 0):.0f}\")\n",
    "    print(f\"  E* (best epoch): {results['vae_results'][0].get('e_star', 0):.0f}\")\n",
    "    if \"checkpoint_path\" in results:\n",
    "        print(f\"  Checkpoint saved: {results['checkpoint_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING HISTORY\n",
    "# ============================================================\n",
    "if (\"state\" in results\n",
    "    and results[\"state\"].get(\"fit_result\") is not None\n",
    "    and \"history\" in results[\"state\"][\"fit_result\"]\n",
    "    and not RUN_FULL_PIPELINE):\n",
    "    history = results[\"state\"][\"fit_result\"][\"history\"]\n",
    "    history_df = pd.DataFrame(history)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "    axes[0, 0].plot(history_df[\"train_loss\"], label=\"Train Loss\")\n",
    "    axes[0, 0].plot(history_df[\"val_elbo\"], label=\"Val ELBO\")\n",
    "    axes[0, 0].set_title(\"Loss Curves\")\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0, 1].plot(history_df[\"train_recon\"], label=\"Reconstruction\")\n",
    "    axes[0, 1].plot(history_df[\"train_kl\"], label=\"KL Divergence\")\n",
    "    axes[0, 1].set_title(\"Loss Components\")\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1, 0].plot(history_df[\"AU\"], color=\"#2563eb\")\n",
    "    axes[1, 0].set_title(\"Active Units (AU)\")\n",
    "    axes[1, 0].set_ylabel(\"AU\")\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1, 1].plot(history_df[\"sigma_sq\"], color=\"#dc2626\")\n",
    "    axes[1, 1].set_title(\"Observation Noise (sigma^2)\")\n",
    "    axes[1, 1].set_ylabel(\"sigma^2\")\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "    fig.suptitle(\"Direct Training History\", fontsize=13, fontweight=\"bold\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "elif not RUN_FULL_PIPELINE:\n",
    "    if CHECKPOINT_PATH:\n",
    "        print(\"Training history not available (loaded from checkpoint, training was skipped).\")\n",
    "    else:\n",
    "        print(\"Training history not available (state bag not populated).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL INSPECTION\n",
    "# ============================================================\n",
    "if \"state\" in results and not(RUN_FULL_PIPELINE):\n",
    "    state = results[\"state\"]\n",
    "    B_A = state.get(\"B_A\")\n",
    "    stock_ids = state.get(\"inferred_stock_ids\", [])\n",
    "    AU = state.get(\"AU\", 0)\n",
    "\n",
    "    if B_A is not None and B_A.shape[1] > 0:\n",
    "        n_show = min(20, B_A.shape[1])\n",
    "        fig, ax = plt.subplots(figsize=(12, max(4, len(stock_ids) * 0.15)))\n",
    "        im = ax.imshow(B_A[:, :n_show], aspect=\"auto\", cmap=\"RdBu_r\")\n",
    "        ax.set_xlabel(f\"Active Latent Dimension (showing {n_show}/{AU})\")\n",
    "        ax.set_ylabel(\"Stock\")\n",
    "        ax.set_title(f\"Exposure Matrix B_A ({B_A.shape[0]} stocks x {AU} active dims)\")\n",
    "        fig.colorbar(im, ax=ax)\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        kl = state.get(\"kl_per_dim\")\n",
    "        if kl is not None:\n",
    "            fig, ax = plt.subplots(figsize=(10, 3))\n",
    "            ax.bar(range(len(kl)), np.sort(kl)[::-1], color=\"#2563eb\", alpha=0.7)\n",
    "            ax.axhline(0.01, color=\"#dc2626\", linestyle=\"--\", label=\"AU threshold (0.01)\")\n",
    "            ax.set_xlabel(\"Dimension (sorted by KL)\")\n",
    "            ax.set_ylabel(\"Marginal KL (nats)\")\n",
    "            ax.set_title(f\"Latent Dimension Usage — AU={AU}/{len(kl)}\")\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"No state available (run Section 4b first).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Results - Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text summary\n",
    "print(format_summary_table(results[\"report\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployment recommendation\n",
    "deployment = results[\"report\"][\"deployment\"]\n",
    "print(f\"Scenario: {deployment['scenario']}\")\n",
    "print(f\"Recommendation: {deployment['recommendation']}\")\n",
    "print()\n",
    "print(\"Per-benchmark wins (VAE vs benchmark on primary metrics):\")\n",
    "for bench, info in deployment[\"per_benchmark\"].items():\n",
    "    print(f\"  {bench:20s}: {info['wins']}/{info['total']} metrics won\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE summary statistics\n",
    "vae_df = aggregate_fold_metrics(results[\"vae_results\"])\n",
    "vae_summary = summary_statistics(vae_df)\n",
    "print(\"VAE Summary Statistics:\")\n",
    "style_summary_table(vae_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark summary statistics\n",
    "for bench_name, bench_metrics in results[\"benchmark_results\"].items():\n",
    "    bench_df = aggregate_fold_metrics(bench_metrics)\n",
    "    bench_summary = summary_statistics(bench_df)\n",
    "    print(f\"\\n{bench_name} Summary:\")\n",
    "    display(style_summary_table(bench_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Results - Per-Fold Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE per-fold metrics\n",
    "print(\"VAE Per-Fold Metrics:\")\n",
    "style_fold_table(vae_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E* distribution\n",
    "e_star_summary = results[\"report\"][\"e_star_summary\"]\n",
    "print(f\"E* epochs: mean={e_star_summary['mean']:.1f}, \"\n",
    "      f\"std={e_star_summary['std']:.1f}, \"\n",
    "      f\"range=[{e_star_summary['min']}, {e_star_summary['max']}]\")\n",
    "\n",
    "plot_e_star_distribution(results[\"e_stars\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fold metrics: VAE vs benchmarks\n",
    "plot_fold_metrics(results[\"vae_results\"], results[\"benchmark_results\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Results - Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise tests heatmap\n",
    "plot_pairwise_heatmap(results[\"report\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed pairwise test results\n",
    "tests = results[\"report\"][\"statistical_tests\"]\n",
    "print(f\"Total comparisons: {tests['n_tests']} (alpha={tests['alpha']})\")\n",
    "print()\n",
    "\n",
    "for bench_name, metrics in tests[\"pairwise\"].items():\n",
    "    print(f\"VAE vs {bench_name}:\")\n",
    "    for metric, result in metrics.items():\n",
    "        if result.get(\"skipped\", False):\n",
    "            print(f\"  {metric}: skipped ({result['reason']})\")\n",
    "            continue\n",
    "        sig = \" *\" if result.get(\"significant_corrected\", False) else \"\"\n",
    "        print(f\"  {metric}: delta={result['median_delta']:+.4f} \"\n",
    "              f\"[{result['ci_lower']:+.4f}, {result['ci_upper']:+.4f}] \"\n",
    "              f\"p={result.get('p_corrected', result['p_value']):.4f}{sig}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"results/\"\n",
    "\n",
    "written = export_results(results, asdict(config), output_dir=OUTPUT_DIR)\n",
    "\n",
    "print(f\"Results saved to {OUTPUT_DIR}\")\n",
    "for path in written:\n",
    "    print(f\"  {os.path.basename(path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Pipeline Diagnostic (E2E)\n",
    "\n",
    "Calls `run_diagnostic()` **in-process** — tqdm progress bars render as native Jupyter widgets.\n",
    "Uses the same `stock_data`, `returns`, `trailing_vol` and `config` loaded in Sections 2–3.\n",
    "The underlying function is shared with `scripts/run_diagnostic.py` (CLI entry point).\n",
    "\n",
    "### Output (`results/diagnostic/`)\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `diagnostic_report.md` | Human-readable Markdown report |\n",
    "| `diagnostic_data.json` | Machine-readable JSON (all metrics) |\n",
    "| `health_checks.csv` | Health check results table |\n",
    "| `training_history.csv` | Per-epoch training metrics |\n",
    "| `strategy_comparison.csv` | VAE vs 6 benchmarks |\n",
    "| `plots/*.png` | 9 diagnostic plots |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9a. RUN DIAGNOSTIC (in-process — tqdm renders as Jupyter widget)\n",
    "#\n",
    "# Base config = Section 2b (`config`), which inherits ALL parameters:\n",
    "#   sigma_sq_min, dropout, weight_decay, lr, patience, phi, kappa, etc.\n",
    "#\n",
    "# Only speed-sensitive parameters are overridden per profile below.\n",
    "# ============================================================\n",
    "from dataclasses import replace as dc_replace\n",
    "from scripts.run_diagnostic import run_diagnostic\n",
    "from src.integration.pipeline_state import load_run_data\n",
    "\n",
    "# ---------- EDIT THESE PARAMETERS ----------\n",
    "DIAG_PROFILE = \"full\"              # \"quick\" (< 10 min) or \"full\" (production)\n",
    "DIAG_OUTPUT_DIR = \"results/diagnostic\"\n",
    "DIAG_HOLDOUT_FRACTION = 0.2\n",
    "DIAG_HOLDOUT_START = None          # or \"2020-01-01\" to override fraction\n",
    "DIAG_RUN_BENCHMARKS = True\n",
    "DIAG_GENERATE_PLOTS = True\n",
    "\n",
    "# === EXTENDED CHECKPOINTING ===\n",
    "# RUN_DIR: Path to existing run folder for resume/load. \n",
    "#   If None: creates new timestamped folder (e.g., results/diagnostic_runs/2026-02-21_143052/)\n",
    "#   If path: uses existing folder (for resume or load-only)\n",
    "RUN_DIR: str | None = \"results/diagnostic_runs/2026-02-23_233353\"  # e.g., \"results/diagnostic_runs/2026-02-21_143052\"\n",
    "\n",
    "# LOAD_EXISTING: Skip execution and load from existing run folder\n",
    "#   If True: loads diagnostics from RUN_DIR, skips pipeline execution\n",
    "#   If False: runs the full diagnostic pipeline (default behavior)\n",
    "LOAD_EXISTING: bool = True\n",
    "\n",
    "# VAE_CHECKPOINT_OVERRIDE: Override VAE checkpoint path (optional)\n",
    "VAE_CHECKPOINT_OVERRIDE: str | None = None\n",
    "\n",
    "# Speed overrides per profile (only these affect execution time).\n",
    "# \"full\" = {} means use Section 2b values as-is.\n",
    "DIAG_SPEED_OVERRIDES: dict[str, dict] = {\n",
    "    \"quick\": {\"n_stocks\": 50, \"K\": 30, \"max_epochs\": 15, \"batch_size\": 256},\n",
    "    \"full\":  {},\n",
    "}\n",
    "# -------------------------------------------\n",
    "\n",
    "# Apply speed overrides on top of Section 2b config\n",
    "_overrides = DIAG_SPEED_OVERRIDES.get(DIAG_PROFILE, {})\n",
    "diag_config = dc_replace(\n",
    "    config,\n",
    "    data=dc_replace(config.data,\n",
    "                    n_stocks=_overrides.get(\"n_stocks\", config.data.n_stocks)),\n",
    "    vae=dc_replace(config.vae,\n",
    "                   K=_overrides.get(\"K\", config.vae.K)),\n",
    "    training=dc_replace(config.training,\n",
    "                        max_epochs=_overrides.get(\"max_epochs\", config.training.max_epochs),\n",
    "                        batch_size=_overrides.get(\"batch_size\", config.training.batch_size)),\n",
    ")\n",
    "\n",
    "print(f\"Diagnostic profile: {DIAG_PROFILE} (base = Section 2b config)\")\n",
    "print(f\"  Speed overrides: {_overrides if _overrides else 'none (production values)'}\")\n",
    "print(f\"  K={diag_config.vae.K}, max_epochs={diag_config.training.max_epochs}, \"\n",
    "      f\"batch={diag_config.training.batch_size}, n_stocks={diag_config.data.n_stocks}\")\n",
    "print(f\"  sigma_sq_min={diag_config.vae.sigma_sq_min}, dropout={diag_config.vae.dropout}, \"\n",
    "      f\"weight_decay={diag_config.training.weight_decay}, lr={diag_config.training.learning_rate}, \"\n",
    "      f\"patience={diag_config.training.patience}\")\n",
    "\n",
    "if LOAD_EXISTING and RUN_DIR:\n",
    "    # Load from existing run folder (no pipeline execution)\n",
    "    print(f\"\\n=== LOADING FROM EXISTING RUN: {RUN_DIR} ===\")\n",
    "    run_data = load_run_data(RUN_DIR)\n",
    "    diagnostics = run_data.get(\"diagnostics\", {})\n",
    "    _w_loaded = run_data.get(\"weights\")\n",
    "    _stock_ids_loaded = run_data.get(\"stock_ids\", [])\n",
    "    \n",
    "    # Expose data for sections 9b-10\n",
    "    if _w_loaded is not None:\n",
    "        diagnostics[\"_raw_weights\"] = _w_loaded\n",
    "    if _stock_ids_loaded:\n",
    "        diagnostics[\"_raw_stock_ids\"] = _stock_ids_loaded\n",
    "    \n",
    "    ACTIVE_RUN_DIR = RUN_DIR\n",
    "    print(f\"  Loaded diagnostics: {len(diagnostics)} top-level keys\")\n",
    "    print(f\"  Weights: {'loaded' if _w_loaded is not None else 'not found'}\")\n",
    "    print(f\"  Stock IDs: {len(_stock_ids_loaded)} stocks\")\n",
    "else:\n",
    "    # Run full diagnostic pipeline\n",
    "    if CHECKPOINT_PATH:\n",
    "        print(f\"  Pretrained model: {CHECKPOINT_PATH} (skipping VAE training)\")\n",
    "    if RUN_DIR:\n",
    "        print(f\"  Resuming from: {RUN_DIR}\")\n",
    "    \n",
    "    run_data = run_diagnostic(\n",
    "        stock_data=stock_data,\n",
    "        returns=returns,\n",
    "        trailing_vol=trailing_vol,\n",
    "        config=diag_config,\n",
    "        output_dir=DIAG_OUTPUT_DIR,\n",
    "        device=DEVICE,\n",
    "        holdout_fraction=DIAG_HOLDOUT_FRACTION,\n",
    "        holdout_start=DIAG_HOLDOUT_START,\n",
    "        loss_mode=config.loss.mode,\n",
    "        run_benchmarks=DIAG_RUN_BENCHMARKS,\n",
    "        generate_plots=DIAG_GENERATE_PLOTS,\n",
    "        tensorboard_dir=\"runs/diagnostic\",\n",
    "        profile_name=DIAG_PROFILE,\n",
    "        data_source_label=DATA_SOURCE,\n",
    "        seed=SEED,\n",
    "        pretrained_model=CHECKPOINT_PATH or VAE_CHECKPOINT_OVERRIDE,\n",
    "        run_dir=RUN_DIR,\n",
    "        vae_checkpoint_override=VAE_CHECKPOINT_OVERRIDE,\n",
    "    )\n",
    "    \n",
    "    # Extract diagnostics from result dict (new return format)\n",
    "    diagnostics = run_data.get(\"diagnostics\", run_data)\n",
    "    ACTIVE_RUN_DIR = run_data.get(\"run_dir\", DIAG_OUTPUT_DIR)\n",
    "    \n",
    "    # Also store weights and stock_ids for sections 9c-9d\n",
    "    if \"weights\" in run_data:\n",
    "        diagnostics[\"_raw_weights\"] = run_data[\"weights\"]\n",
    "    if \"stock_ids\" in run_data:\n",
    "        diagnostics[\"_raw_stock_ids\"] = run_data[\"stock_ids\"]\n",
    "    \n",
    "    print(f\"\\n=== RUN COMPLETE ===\")\n",
    "    print(f\"  Run directory: {ACTIVE_RUN_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9b-9e. CONSOLIDATED DIAGNOSTIC DISPLAY\n",
    "# ============================================================\n",
    "# Displays: plots, report, holdings, exposures, ML diagnostics, ZIP export\n",
    "from src.integration.notebook_helpers import display_diagnostic_results\n",
    "\n",
    "summary = display_diagnostic_results(\n",
    "    diagnostics=diagnostics,\n",
    "    run_data=run_data,\n",
    "    output_dir=ACTIVE_RUN_DIR,\n",
    "    data_dir=DATA_DIR,\n",
    "    stock_data=stock_data if 'stock_data' in dir() else None,\n",
    ")\n",
    "print(f\"\\nDisplay Summary: {summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9f. OOS REPLAY FROM CHECKPOINT\n",
    "# ============================================================\n",
    "# Re-runs ONLY the OOS simulation using saved checkpoint data.\n",
    "# Skips: VAE training (~5h), portfolio optimization (~5min), benchmarks (~10min).\n",
    "# Reconstructs risk model from B_A + data (~30s), then runs OOS (~5-10min).\n",
    "#\n",
    "# Set REPLAY_OOS = True to run. Requires data from Section 2 (returns,\n",
    "# trailing_vol, stock_data) and a completed run in RUN_DIR (Section 9a).\n",
    "# ============================================================\n",
    "from importlib import reload\n",
    "import src.walk_forward.oos_rebalancing as _oos; reload(_oos)\n",
    "import src.integration.notebook_helpers as _nh; reload(_nh)\n",
    "from src.integration.notebook_helpers import replay_oos_simulation\n",
    "\n",
    "REPLAY_OOS = True\n",
    "REPLAY_REFRESH_RISK_MODEL = True  # Enable risk model refresh during OOS\n",
    "\n",
    "# ---- Performance overrides (tune these to speed up / improve quality) ----\n",
    "# Entropy trigger: 0.90 = very sensitive (many exceptional rebals).\n",
    "# Set to 0.0 to disable exceptional triggers entirely.\n",
    "REPLAY_ENTROPY_TRIGGER_ALPHA = 0.0   # Disable exceptional triggers (scheduled only)\n",
    "REPLAY_N_STARTS_SCHEDULED = 1        # 1 start for scheduled rebals (fast)\n",
    "REPLAY_N_STARTS_EXCEPTIONAL = 2      # 2 starts for exceptional (if enabled)\n",
    "REPLAY_MAX_ITER_SCHEDULED = 50       # SCA iterations for scheduled\n",
    "REPLAY_MAX_ITER_EXCEPTIONAL = 50     # SCA iterations for exceptional\n",
    "\n",
    "if REPLAY_OOS and RUN_DIR:\n",
    "    replay_result = replay_oos_simulation(\n",
    "        run_dir=RUN_DIR,\n",
    "        returns=returns,\n",
    "        trailing_vol=trailing_vol,\n",
    "        config=config,\n",
    "        stock_data=stock_data if 'stock_data' in dir() else None,\n",
    "        holdout_fraction=DIAG_HOLDOUT_FRACTION,\n",
    "        holdout_start=DIAG_HOLDOUT_START,\n",
    "        refresh_risk_model=REPLAY_REFRESH_RISK_MODEL,\n",
    "        entropy_trigger_alpha_override=REPLAY_ENTROPY_TRIGGER_ALPHA,\n",
    "        oos_n_starts_scheduled_override=REPLAY_N_STARTS_SCHEDULED,\n",
    "        oos_n_starts_exceptional_override=REPLAY_N_STARTS_EXCEPTIONAL,\n",
    "        oos_sca_max_iter_scheduled_override=REPLAY_MAX_ITER_SCHEDULED,\n",
    "        oos_sca_max_iter_exceptional_override=REPLAY_MAX_ITER_EXCEPTIONAL,\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    m = replay_result['metrics']\n",
    "    d = replay_result['dates']\n",
    "    oos = replay_result['oos_result']\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"OOS REPLAY RESULTS  (refresh={REPLAY_REFRESH_RISK_MODEL})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Period:       [{d['oos_start']}] to [{d['oos_end']}]\")\n",
    "    print(f\"  Sharpe (net): {m.get('sharpe', 0):.3f}\")\n",
    "    print(f\"  Ann. return:  {m.get('ann_return', 0):.2%}\")\n",
    "    print(f\"  Ann. vol:     {m.get('ann_vol_oos', 0):.2%}\")\n",
    "    print(f\"  Max drawdown: {m.get('max_drawdown_oos', 0):.2%}\")\n",
    "    print(f\"  Sortino:      {m.get('sortino', 0):.3f}\")\n",
    "    print(f\"  Calmar:       {m.get('calmar', 0):.3f}\")\n",
    "    print(f\"  Rebalances:   {oos.n_scheduled_rebalances} scheduled, \"\n",
    "          f\"{oos.n_exceptional_rebalances} exceptional\")\n",
    "    print(f\"  Turnover:     {oos.cumulative_turnover*100:.1f}%\")\n",
    "    print(f\"  TC:           {oos.total_transaction_cost*100:.2f}%\")\n",
    "    print(f\"  Total time:   {replay_result['total_time_sec']:.1f}s\")\n",
    "else:\n",
    "    print('OOS Replay skipped (set REPLAY_OOS=True and RUN_DIR to run)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Decision Synthesis\n",
    "\n",
    "This section provides an automated root cause analysis and actionable recommendations\n",
    "based on the diagnostic scores from Section 9. It helps identify the underlying causes\n",
    "of any issues and suggests specific configuration changes.\n",
    "\n",
    "**Components:**\n",
    "- Root Cause Analysis with matched decision rules\n",
    "- Causal chain visualization\n",
    "- Executable configuration recommendations\n",
    "- Validated JSON output for automation\n",
    "\n",
    "**Source modules:**\n",
    "- `src/integration/decision_rules.py` — 10 decision rules + causal graph + 8 metric patterns\n",
    "- `src/integration/action_specs.py` — 35+ config change specifications\n",
    "- `src/integration/diagnostic_schema.py` — JSON schema validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10. DECISION SYNTHESIS (Consolidated)\n",
    "# ============================================================\n",
    "# Root cause analysis, decision rules, causal diagram, recommendations, JSON export\n",
    "from src.integration.notebook_helpers import run_decision_synthesis\n",
    "\n",
    "synthesis = run_decision_synthesis(\n",
    "    diagnostics=diagnostics,\n",
    "    output_dir=DIAG_OUTPUT_DIR,\n",
    ")\n",
    "print(f\"\\nSynthesis Summary: analysis={'matched' if synthesis['matched_rules'] else 'nominal'}, json_valid={synthesis['json_valid']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Reference: Decision Rules\n",
    "\n",
    "| Rule ID | Condition | Diagnosis |\n",
    "|---------|-----------|----------|\n",
    "| PURE_OPTIMIZATION | solver<60, constraint>80 | Optimization problem, not constraint-related |\n",
    "| CONSTRAINT_DOMINATED | solver>70, constraint<50 | Constraints limiting optimization freedom |\n",
    "| COVARIANCE_DEGRADATION | covariance<55 | Risk model miscalibrated |\n",
    "| VAE_COLLAPSE | reconstruction<50, vae_health<50 | VAE posterior collapse detected |\n",
    "| FACTOR_MODEL_WEAK | factor_model<50 | Factor model specification issues |\n",
    "| OVERALL_DEGRADATION | solver<60, covariance<60, recon<60 | Systemic pipeline degradation |\n",
    "| HEALTHY_PIPELINE | solver>75, constraint>60, cov>70, recon>70 | Pipeline operating normally |\n",
    "| RECONSTRUCTION_IMBALANCE | recon 40-70, vae_health>60 | Feature reconstruction imbalanced |\n",
    "| OVERFITTING_DETECTED | reconstruction<55 + overfit_ratio>1.3 | VAE overfitting to training data |\n",
    "| SOLVER_CONSTRAINT_CONFLICT | solver<60, constraint<60 | Infeasible optimization problem |\n",
    "\n",
    "**Action Categories:**\n",
    "- `solver`: sca_max_iter, sca_tol, n_starts, armijo_rho\n",
    "- `constraints`: w_max, w_min, tau_max, kappa_1, kappa_2\n",
    "- `vae`: dropout, K\n",
    "- `training`: max_epochs, learning_rate, patience, batch_size\n",
    "- `loss`: beta_fixed, gamma, lambda_co_max\n",
    "- `risk_model`: ridge_scale, d_eps_floor, sigma_z_shrinkage\n",
    "\n",
    "**Reference:** `docs/diagnostic.md` for full interpretation guide."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Latent_risk_factor (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
