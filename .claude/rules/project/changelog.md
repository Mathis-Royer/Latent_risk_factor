# Changelog

> **Purpose:** Track recent significant changes. Update after each major modification.
> Keep ~10 entries. Merge similar entries rather than adding duplicates.

## Recent Changes

| # | Date | Modification |
|---|------|--------------|
| 1 | 2026-02-17 | **Comprehensive VAE training pipeline audit — 25 steps, 8 tension points, 0 code changes**: Created `docs/vae_training_pipeline_audit.md` (890 lines). Analyzed every pipeline step from data preparation (T=504, F=2, crisis labeling) through architecture (InceptionHead, ResBlocks, K=50, σ²), loss function (D/(2σ²), KL, 3 modes, γ=3.0, co-movement D/2, curriculum 30/30/40), training loop (AdamW lr=5e-3, ReduceLROnPlateau, ES patience=20, β annealing, gradient clipping, AMP, curriculum batching), and validation (ELBO, overfit detection, AU). Each step: current implementation with code refs, modification history from git, literature review (Kingma & Welling 2014, Lucas et al. 2019, Higgins 2017, Bowman 2016, Burda 2015, Loshchilov & Hutter 2019, Bengio 2009, Goyal 2017, etc.), strategy philosophy check, cross-decision contradiction check. **All 25 verdicts: ✅ KEEP.** 8 cross-step tension points resolved (no contradictions). Minor notes: (1) ES/LR patience ratio 2:1 gives ~1 LR reduction before stopping — 3:1 could be better but current works; (2) HP grid doc says K∈{100,200} but default is K=50 — doc needs update. No code changes required. |
| 2 | 2026-02-17 | **Deep audit phase 15: VAE training pipeline — 2 findings across 6 files**: (1) **IMPORTANT: Co-movement loss D/2 normalization** (Higgins et al. 2017, β-VAE; Zhao et al. 2019, InfoVAE) — λ_co·L_co was ~1000× weaker than D/(2σ²)·L_recon because reconstruction is scaled by D=T×F=1008 while co-movement was unscaled. Analogous to the Phase 1-2 lambda_risk scale mismatch (1→252). Fix: `co_term = λ_co * (D/2) * L_co` in all 3 modes. Reduced `lambda_co_max` 0.5→0.1 to compensate. (2) **MODERATE: Curriculum fractions not propagated** — `get_lambda_co()` hardcoded 0.30/0.60 fractions, ignoring `TrainingConfig.curriculum_phase1_frac`/`curriculum_phase2_frac`. Config validated but never used. Fix: pass fractions through compute_loss→get_lambda_co→trainer→pipeline. 6 files modified (loss.py, config.py, trainer.py, pipeline.py, test_loss_function.py, changelog.md), pyright 0 errors. |
| 3 | 2026-02-17 | **Deep audit phase 14: Post-VAE pipeline — 5 fixes across 3 files**: (1) **CRITICAL: EWMA eigenvalue deflation** in covariance.py — `analytical_nonlinear` path used `np.cov(z_input)` on EWMA sqrt(w)-weighted data, deflating eigenvalues ~4800x. Fix: use `z_input.T @ z_input` (matching existing `spiked` path). (2) **CRITICAL: n_signal propagation** — `_analytical_nonlinear_shrinkage` discarded n_signal from spiked fallback; gap estimator fails on DGJ-shrunk flat-noise spectrum. Fix: return n_signal from function, use BBP-based estimate from spiked path when EWMA active. (3) `entropy_idio_weight` 0.05→0.0, (4) `entropy_budget_mode` proportional→uniform, (5) `phi` 5.0→0.0. 3 files modified (covariance.py, config.py, changelog.md). |
| 4 | 2026-02-16 | **Deep audit phase 13: Cardinality momentum + CCD ERC solver — 2 findings across 4 files**: (1) **CRITICAL: Cardinality MIQP/two_stage objectives drop momentum signal** (Roncalli 2013, Ch. 7) — SCA solver includes `ret_term = mu @ w` in its objective, but both `_enforce_miqp()` and `_enforce_two_stage()` build independent CVXPY problems that omit this term. Since `auto` resolves to `two_stage` with MOSEK, the default cardinality path erases all return-based tilts. This explains persistent selection_ratio=1.01 despite Phase 12 winsorization + momentum_weight=0.50. Phase 12 incorrectly rejected this finding, confusing mu *passing* (Phase 1 fix: mu added to sca_kwargs) with mu *usage* in objectives (never added to MIQP/two_stage). Added `ret_term = mu @ w_var` to both objectives. (2) **ERC CCD fallback solver** (Griveau-Billion, Richard & Roncalli 2013) — Newton solver fails for n=488 (Jacobian ill-conditioned, κ=24,000). Extended backtracking from 20→50 halvings. Added `_ccd_erc()` cyclical coordinate descent: solves fixed-point equation one coordinate at a time via quadratic formula, **guaranteed positivity** by construction for any PD Σ. Solver chain: CVXPY → Newton → CCD → 1/N. 4 files modified (cardinality.py, erc.py, test_benchmarks.py, test_portfolio_optimization.py), pyright 0 errors, 480 tests pass (5 new). |
| 5 | 2026-02-16 | **Deep audit phase 12: Momentum winsorization + Newton ERC solver — 2 findings across 3 files**: (1) **Momentum signal winsorization at ±3σ** (Asness, Moskowitz & Pedersen 2013) — two-pass clip before z-scoring in momentum.py. Extreme negative outliers (z-score range [-7.86, 1.93]) were inflating cross-sectional std and compressing the positive tail, making momentum inert (selection ratio=1.01). After winsorization, z-scores are symmetric and the signal contributes meaningful tilts within the entropy-feasible region. (2) **Newton-based ERC fallback solver** (Roncalli 2013, Ch. 11, Algorithm 11.1) — added `_newton_erc()` as fallback between CVXPY solver chain and 1/N in erc.py. All three CVXPY solvers (MOSEK, ECOS, SCS) failed for n=636 stocks, making ERC identical to EW. Newton solver operates directly on the fixed-point equation (Σy)_i·y_i=1 with damped Newton steps and inverse-vol initialization. 3 files modified (momentum.py, erc.py, test_benchmarks.py), pyright 0 errors, 475 tests pass (7 new). |
| 6 | 2026-02-16 | **Deep audit phase 11: Alpha selection + PCA benchmark ENB fix — 2 findings across 4 files**: (1) **IMPORTANT: Min-variance among qualifying alphas** (Meucci 2009, DeMiguel et al. 2009) — `select_operating_alpha()` now picks the alpha with **minimum variance** among all qualifying points (ENB_mono ≥ target), instead of the smallest qualifying alpha. On U-shaped variance-entropy frontiers, the old logic selected alpha=0.005 (Var=6.22e-5) while alpha=2.0 was Pareto-dominant (-16.7% variance, +37.6% entropy). At low alpha entropy is too weak to diversify; at high alpha entropy forces factor diversification which reduces variance through correlation structure. (2) **BUG: PCA benchmark naive ENB target (INV-012 violation)** — PCA used `max(2.0, k/2)` = 15.0 for k=30, but ENB_spectrum=1.71 making target 8.8x unreachable → forced to extreme alpha → negative Sharpe (-0.006/-0.009). Extracted shared `compute_adaptive_enb_target()` function into frontier.py, used by both pipeline.py and pca_factor_rp.py. Formula: `max(2.0, min(n_signal/2, ENB_spectrum * 0.7))`. PCA benchmarks now produce positive Sharpe. 4 files modified (frontier.py, pipeline.py, pca_factor_rp.py, test_portfolio_optimization.py), pyright 0 errors, 468 tests pass (5 new). |
| 7 | 2026-02-16 | **Phase 10: Cardinality + momentum activation — 3 changes across 3 files**: (1) `_MAX_BINARY_VARS` 80→150 in cardinality.py — lifts MIQP ceiling to avoid fallback to greedy elimination for larger universes. (2) `momentum_enabled` False→True, `momentum_weight` 0.02→0.05 in config.py — activates momentum signal by default. (3) `np.asarray`→`np.array(copy=True)` in momentum.py — fixes read-only array error from pandas Series. |
| 8 | 2026-02-16 | **Phase 9 audit: 5 fixes across 5 files** (post-VAE portfolio + benchmarks): (1) **CRITICAL: Adaptive ENB target** (Meucci 2009, Roncalli 2013 Ch. 7) — replaced naive `n_signal/2` target with `min(n_signal/2, ENB_spectrum * 0.7)` where `ENB_spectrum = exp(H(eigenvalue_spectrum))` is the maximum achievable ENB for the current eigenvalue structure. (2) **BUG: Fallback argmax(enb_mono)** — `select_operating_alpha` fallback used raw `enb` instead of `enb_mono` (monotone envelope). (3) **INV-012: constraint_params complete** — added missing `w_bar` and `alpha_grid`. (4) **INV-012: effective_cap = min(w_bar, w_max)** in base.py and min_variance.py. (5) **DOC: ERC near-uniform warning**. 5+2 files modified, pyright 0 errors, 440 unit tests pass. |
| 9 | 2026-02-16 | **Deep audit phase 8: 7 literature-backed fixes across 5 files**: (1) w_bar 0.01→0.03 (Brodie 2009). (2) Market PC1 excluded from entropy (Meucci 2009). (3) B_A_SHRINKAGE_ALPHA 0.15→0.0. (4) PCA benchmark frontier (INV-012). (5) ERC NaN handling. (6) var_ratio fillna(0)@w. (7) PHI 20→0.0. 5 files modified, pyright 0 errors, 440 unit tests pass. |
| 10 | 2026-02-15 | **Deep audit phase 7: 5 fixes across 5 files**: (1) w_old alignment by stock ID. (2) Bayesian VT shrinkage (Barra USE4). (3) entropy_idio_weight 0.2→0.05. (4) lstsq rank diagnostic. (5) Diversified frontier seeds. 5 files modified, pyright 0 errors, 77 unit tests pass. |


---

## Current State

- **Status**: Development — All 4 phases + tests + dashboard + performance optimizations + TensorBoard + diagnostic pipeline complete. 48 source files + 13 test files + 1 notebook. SP500 priority download + penny stock/min history filters active.
- **Main features**: Full data pipeline, 1D-CNN VAE, 3-mode loss with co-movement curriculum, training loop with AMP + TensorBoard + curriculum batching + CUDA T4 optimizations (TF32, fused AdamW, gradient accumulation/checkpointing), inference + AU with AMP autocast, dual-rescaled factor risk model, portfolio optimization (SCA+Armijo, parametric CVXPY, Cholesky, parallel multi-starts, 4-strategy cardinality enforcement with MIQP pre-screening + two-stage decomposition), early training checkpoint, 6 benchmarks, walk-forward (34 folds) + direct training mode, statistical tests, reporting, **E2E diagnostic pipeline** (health checks, MD/JSON/CSV/PNG reports, quick/full profiles), 3 CLI entry points, dashboard notebook, SP500-first download with data quality filters
- **Next**: Full Tiingo SP500 diagnostic re-run to verify Phase 14-15 fixes (co-movement active via D/2 scaling, EWMA eigenvalues correct, entropy uniform budget mode). Comprehensive pipeline audit complete (`docs/vae_training_pipeline_audit.md`) — all 25 steps validated, no code changes needed.

---

## Update Protocol

After each significant modification:

1. Add new entry at position 1, shift others down
2. If similar entry exists, update it instead of adding
3. If > 10 entries, remove entry #10
4. Update "Current State" if project status changed
