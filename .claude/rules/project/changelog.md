# Changelog

> **Purpose:** Track recent significant changes. Update after each major modification.
> Keep ~10 entries. Merge similar entries rather than adding duplicates.

## Recent Changes

| # | Date | Modification |
|---|------|--------------|
| 1 | 2026-02-22 | **Fast SCA sub-problem solver via projected gradient descent (~10-50× per-iteration speedup)** — Replaces CVXPY interior-point with Nesterov-accelerated PGD for the SCA sub-problem: (1) **sca_solver.py** (~200 lines added) — New `project_simplex_box()` implements Michelot 1986 + box constraints (O(n log n) via sorting); `solve_sca_subproblem_fast()` Nesterov PGD with adaptive step size (Lipschitz from Cholesky), handles concentration penalty, turnover penalty, expected returns. (2) **sca_optimize()** — New params `use_fast_subproblem=True` (default), `fast_subproblem_max_iter=50`, `fast_subproblem_tol=1e-7`; branches between fast PGD and legacy CVXPY. (3) **config.py** — 3 new `PortfolioConfig` params for fast solver. (4) **10 new unit tests** in `test_portfolio_optimization.py`: `TestFastSubproblemSolver` class with simplex projection (4 tests), PGD vs CVXPY quality (2 tests), speedup benchmark, turnover handling, multi-start integration, frontier integration. 2 files modified, 75 tests pass, pyright 0 errors. Impact: Per-SCA-iteration sub-problem ~10-50× faster (CVXPY 0.5-3s → PGD 5-50ms for n=1000); combined with frontier early stopping, total frontier time ~100× reduction for typical cases. |
| 2 | 2026-02-22 | **Variance-entropy frontier early stopping optimization (~50-70% time reduction)** — Two-phase adaptive grid with early stopping reduces frontier computation time significantly: (1) **config.py** — 8 new `PortfolioConfig` parameters: `frontier_coarse_grid` (default [0.001, 0.01, 0.1, 1.0, 5.0]), `frontier_early_stop_patience=2`, `frontier_n_starts_after_target=2`, `frontier_max_iter_after_target=50`, `frontier_refine_enabled=True`, `frontier_refine_points=3`, `frontier_n_starts_refine=3`, `frontier_max_iter_refine=75`. (2) **frontier.py** (~150 lines refactored) — Two-phase `compute_variance_entropy_frontier()`: Phase 1 uses coarse grid with early stopping when ENB target reached + variance increases `patience` times; uses reduced solver quality (`n_starts_after_target`, `max_iter_after_target`) post-target. Phase 2 (optional) refines around elbow with log-uniform interpolation. Added `target_enb`, `coarse_grid`, and quality params. (3) **pipeline.py** (~30 lines) — Computes `effective_target_enb` BEFORE frontier call (enables early stopping); passes all 8 new params to `compute_variance_entropy_frontier()`. (4) **5 new unit tests** in `test_portfolio_optimization.py`: `TestFrontierEarlyStop` class covering early stop trigger, Kneedle mode bypass, adaptive quality, Phase 2 refinement, legacy fallback. 3 files modified, 70 tests pass, pyright 0 errors. Impact: ENB-easy cases (target reached at α=0.01) use ~3-12 solver calls vs 55 (full grid), ~78% reduction. |
| 3 | 2026-02-22 | **Consolidated diagnostic persistence + extended ML data saving** — Unified diagnostic folder structure + added critical ML diagnostic arrays: (1) **pipeline_state.py** (~120 lines added) — `create_latest_symlink()` for backwards-compatible `results/diagnostic` → latest timestamped run; `save_benchmark_weights()` saves all 6 benchmark weight vectors as NPY; `save_state_bag_for_stage(VAE_TRAINED)` now extracts kl_per_dim_history (E, K) and log_var_bounds_history (E, 2, K) from training history; INFERENCE_DONE saves B_full before AU filtering; COVARIANCE_DONE saves pca_loadings, pca_eigenvalues, literature_comparison JSON. (2) **pipeline.py** (~60 lines) — Computes PCA loadings on training returns for VAE comparison; calls `compute_literature_comparison()` for Marchenko-Pastur/Bai-Ng AU validation; saves benchmark weights after benchmarks complete. (3) **diagnostics.py** (~80 lines) — New `compute_literature_comparison()` computes MP edge, counts eigenvalues above random matrix bulk, compares AU with Bai-Ng k and MP signal count. (4) **run_diagnostic.py** — Calls `create_latest_symlink("results/diagnostic")` after run for rétrocompatibilité. (5) **9 new unit tests** in test_pipeline_state.py covering kl_per_dim_history, B_full, benchmark weights, symlink, literature_comparison, PCA loadings. 4 files modified, 69 tests pass, pyright 0 errors. |
| 4 | 2026-02-22 | **Google Drive persistence for Colab** — Automatic checkpointing to Google Drive when running in Colab, enabling crash recovery and persistent storage across runtime disconnections: (1) **NEW: `colab_drive.py`** (~180 lines) — `is_colab()` detection, `mount_drive()` mounting, `setup_drive_persistence()` main entry (creates symlink local→Drive), `cleanup_old_runs()` rotation (keep N most recent), `list_runs()` metadata extraction, `get_latest_run()` for resume. (2) **dashboard.ipynb** — 2 new cells at start: Cell 0 for Drive setup (creates symlink before any writes), Cell 1 for resume after disconnect (commented template). (3) **Architecture**: Local `results/diagnostic_runs/` becomes symlink to Drive — all existing code writes transparently to Drive. (4) **17 unit tests** in `test_colab_drive.py`. 3 files modified/created, pyright 0 errors. |
| 5 | 2026-02-22 | **Phase 2 memory optimization (~3GB additional savings)** — 9 targeted optimizations completing the memory reduction work: (1) **data_loader.py** — `sort_values(..., ignore_index=True)` replaces `sort_values().reset_index()` (~640MB), `groupby().filter()` replaces `transform("count")` (~40MB), vectorized `generate_synthetic_csv()` builds arrays directly instead of 1.3M dict records (~800MB). (2) **windowing.py** — Pre-allocated metadata arrays (`meta_stock_ids`, `meta_start_idx`, `meta_end_idx`) replace list-of-dicts accumulation (~2.1GB for stride=1 inference), removed redundant `.copy()` and `.astype(float32)` calls (~40MB/stock transient). (3) **returns.py** — NumPy-based log-return computation avoids triple DataFrame temporaries (~186MB), vectorized `winsorize_returns()` replaces `apply(axis=1)` (~124MB). (4) **diagnostics.py** — Explicit `del returns_centered` after Bai-Ng IC2 call (~72MB). 4 files modified, 1016 tests pass, pyright 0 errors. Combined with Phase 1, total estimated peak reduction: ~43GB. |
| 6 | 2026-02-22 | **Memory explosion fix during VAE checkpoint saving (130GB+ RAM)** — Fixed critical memory explosion (~130GB RAM crash) during checkpoint saving after VAE training. Root causes: inference windows (stride=1 vs training stride=21 = 21× more data ~32GB), full fit_result history with per-dim KL arrays, and recursive JSON serialization without size guards. (1) **pipeline_state.py** — `_serialize_for_json()` now has size guards: arrays >100k elements return placeholder, lists >10k items return placeholder, DataFrames >10k cells return placeholder, max recursion depth=10. `save_state_bag_for_stage(VAE_TRAINED, ...)` now saves only fit_result summary scalars + compact training_curve (epoch, train_loss, val_elbo, AU, sigma_sq, learning_rate) — excludes large kl_per_dim and log_var_stats arrays. (2) **pipeline.py** — Added `import gc`. After aggregate_profiles(): `del infer_windows, infer_metadata, trajectories; gc.collect(); clear_device_cache(device)` (~32GB freed). After trainer.fit(): `trainer.cleanup(); del train_w, val_w, train_raw, metadata, train_metadata, train_strata; gc.collect()`. Before save_state_bag: clears train_returns, B_A_by_date, universe_snapshots from state_bag. (3) **early_stopping.py** — `restore_best()` now sets `self.best_state = None` after restoring weights (~20MB freed). (4) **trainer.py** — New `cleanup()` method: closes TensorBoard, clears torch.compile cache via `torch._dynamo.reset()`, clears early_stopping.best_state, zeros optimizer gradients, clears GradScaler, empties CUDA/MPS cache. (5) **11 new unit tests**: 7 for size guards/fit_result filtering in test_pipeline_state.py, 4 for trainer cleanup in test_training.py. 4 files modified, 75 tests pass, pyright 0 errors. Expected impact: RAM during checkpoint <5GB (vs 130GB+ crash). |
| 7 | 2026-02-21 | **Extended checkpointing for complete diagnostic recovery** — Implemented timestamped run folders with full state_bag persistence for dashboard sections 9-10: (1) **pipeline_state.py** (~200 lines added) — `DiagnosticRunManager` class (timestamped folder creation `results/diagnostic_runs/YYYY-MM-DD_HHMMSS/`, VAE checkpoint discovery, `save_run_config()` with git hash), `load_run_data()` helper for notebook loading. Extended `PipelineStateManager` with 8 methods: `save_json()`/`load_json()`, `save_stage_arrays()`/`load_stage_arrays()`, `save_scalars()`/`load_scalars()`, `save_state_bag_for_stage()`/`load_state_bag_for_stage()` — routes arrays→NPY, dicts→JSON, scalars→merged JSON. (2) **diagnostic_report.py** — `save_diagnostic_report()` now accepts `weights` and `stock_ids` params, saves to `arrays/portfolio/w_vae.npy` and `json/inferred_stock_ids.json`. (3) **run_diagnostic.py** — 3 new CLI args: `--run-dir PATH` (resume existing), `--vae-checkpoint PATH` (override), `--base-dir PATH` (new runs). Returns dict with `run_dir`, `weights`, `stock_ids`. (4) **pipeline.py** — Enhanced resume logic loads state_bag for all completed stages via `load_state_bag_for_stage()`. (5) **dashboard.ipynb** cell 63 — New config: `RUN_DIR`, `LOAD_EXISTING`, `VAE_CHECKPOINT_OVERRIDE`; conditional logic to load from existing folder or run diagnostic; exposes `ACTIVE_RUN_DIR`. (6) **27 new unit tests** in `test_pipeline_state.py`: `TestExtendedStateBagPersistence` (9), `TestDiagnosticRunManager` (11), `TestLoadRunData` (7). 5 files modified, 53 tests pass, pyright 0 errors. |
| 8 | 2026-02-21 | **OOS rebalancing performance optimization** — Implemented differentiated solver quality settings for scheduled vs exceptional rebalancing, reducing OOS simulation time by ~67%: (1) **config.py** — 5 new/modified parameters: `rebalancing_frequency_days` 21→63 (quarterly), `oos_n_starts_scheduled=2` (quick), `oos_n_starts_exceptional=5` (full quality), `oos_sca_max_iter_scheduled=50`, `oos_sca_max_iter_exceptional=100`. Added validation in `__post_init__`. (2) **oos_rebalancing.py** — `_execute_rebalancing()` now accepts `trigger` parameter to differentiate quality settings; passes `max_iter` to `multi_start_optimize`. Enhanced logging: INFO level with turnover breakdown (one-way/two-way/cumulative), rebalance counter, H before→after, n_active. Added `expected_scheduled` computation. (3) **pipeline.py** — `oos_constraint_params` dict now includes all 4 OOS quality settings. Impact: 74 scheduled rebalances → ~25, solver time per scheduled 2-3min → 30-60s, exceptional triggers retain full quality. 3 files modified, 60 tests pass, pyright 0 errors. |
| 9 | 2026-02-21 | **Incremental checkpointing + robust error handling** — Implemented full state management system to prevent losing hours of computation after crashes: (1) **NEW: `pipeline_state.py`** (~400 lines) — `PipelineStage` enum (10 stages: NOT_STARTED→COMPLETE), `StageStatus` enum (pending/success/failed/fallback), `StageInfo` dataclass (timestamps, error tracking), `PipelineState` dataclass (full state serializable to JSON), `PipelineStateManager` class (save/load state, save/load arrays as NPY, mark stages success/failed/fallback, get_resume_stage, benchmark status tracking). (2) **diagnostics.py** — Added `_safe_diagnostic()` wrapper that catches exceptions and returns `{available: False, error: "..."}` instead of crashing; wrapped all 11 diagnostic functions + health_checks + composite_scores. (3) **pipeline.py** — Added state manager initialization, resume logic (`resume: bool, force_stage: str | None` params), stage marking (start/success/failed), array checkpoints (B_A, kl_per_dim, Sigma_assets, w_vae), individual benchmark error isolation with fallback status tracking. (4) **run_diagnostic.py** — Added `--resume` and `--force-stage STAGE` CLI arguments passed through to pipeline. (5) **26 new unit tests** in `test_pipeline_state.py` covering all state management functionality + resume scenarios. 5 files modified/created, 947 tests pass, pyright 0 errors. |
| 10 | 2026-02-21 | **dropna() bug fix across 5 files** — Fixed "Found array with 0 sample(s)" errors caused by `.dropna()` dropping ALL rows when 1205 stocks had scattered NaN on different dates. (1) **Benchmarks (3 files)**: `min_variance.py`, `pca_factor_rp.py`, `pca_vol.py` — replaced `.dropna()` with ERC-style handling: filter stocks >50% NaN + `fillna(0.0)`. (2) **inverse_vol.py**: fallback volatility calculation now uses `fillna(0.0)` instead of `dropna()`. (3) **pipeline.py:2615**: OOS correlation metric now uses `fillna(0.0)` instead of `dropna(how="any")`. Audited all `.dropna()` calls — remaining usages are single-column Series (safe). 38 benchmark tests pass, pyright 0 new errors. |


---

## Current State

- **Status**: Development — All 4 phases + tests + dashboard + performance optimizations + TensorBoard + diagnostic pipeline + **factor quality diagnostics** + **10 composite scores** + **Decision Synthesis (Section 10)** + **comprehensive runtime validation audit** + **extended checkpointing** + **OOS rebalancing optimization** + **memory explosion fix** + **training pipeline memory optimization** + **Google Drive persistence** + **consolidated diagnostic persistence with extended ML data saving** complete. 53 source files + 18 test files + 1 notebook (78 cells). SP500 priority download + penny stock/min history filters active.
- **Main features**: Full data pipeline, 1D-CNN VAE, 3-mode loss with co-movement curriculum, training loop with AMP + TensorBoard + curriculum batching + CUDA T4 optimizations (TF32, fused AdamW, gradient accumulation/checkpointing), inference + AU with AMP autocast, dual-rescaled factor risk model, portfolio optimization (SCA+Armijo, **fast PGD sub-problem solver** ~10-50× speedup per iteration, Cholesky, parallel multi-starts, 4-strategy cardinality enforcement with MIQP pre-screening + two-stage decomposition, **frontier early stopping** with two-phase adaptive grid ~50-70% time reduction), early training checkpoint, 6 benchmarks, walk-forward (34 folds) + direct training mode, statistical tests, reporting, **E2E diagnostic pipeline** (health checks, MD/JSON/CSV/PNG reports, quick/full profiles, **factor quality dashboard** with AU validation via Bai-Ng IC2 + Onatski + latent stability tracking + Marchenko-Pastur comparison, **10 composite scores** (0-100) covering all pipeline phases, **decision synthesis** with root cause analysis + causal chain + config recommendations), 3 CLI entry points, dashboard notebook, SP500-first download with data quality filters, **comprehensive validation utilities** (23 functions in src/validation.py, 149 unit tests), **winsorization + Bonferroni HP correction**, **extended checkpointing** (DiagnosticRunManager for timestamped folders, full state_bag persistence by stage, `--run-dir`/`--vae-checkpoint` CLI, dashboard LOAD_EXISTING mode, kl_per_dim_history + B_full + benchmark weights + PCA loadings saved), **OOS rebalancing optimization** (quarterly frequency, differentiated solver quality for scheduled/exceptional, ~67% time reduction), **memory optimizations** (float32 windowing, two-pass pre-allocation, pre-allocated metadata arrays, vectorized CSV generation, NumPy-based log-returns, lazy train_returns, ~43GB combined peak reduction), **Google Drive persistence** (auto-symlink to Drive on Colab, rotation of old runs, `results/diagnostic` symlink to latest run)
- **Next**: Full Tiingo SP500 diagnostic re-run to verify memory optimizations on Colab.

---

## Update Protocol

After each significant modification:

1. Add new entry at position 1, shift others down
2. If similar entry exists, update it instead of adding
3. If > 10 entries, remove entry #10
4. Update "Current State" if project status changed
