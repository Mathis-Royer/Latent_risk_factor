# Changelog

> **Purpose:** Track recent significant changes. Update after each major modification.
> Keep ~10 entries. Merge similar entries rather than adding duplicates.

## Recent Changes

| # | Date | Modification |
|---|------|--------------|
| 1 | 2026-02-23 | **Deep post-mortem: 8 structural fixes for VAE factor model + solver** — Implements full deep post-mortem analysis plan (8 actions). (1) **Cross-sectional R² loss** (`loss.py`, `config.py`, `trainer.py`): New `compute_cross_sectional_loss()` — differentiable OLS R² loss forcing encoder mu to serve as useful factor exposures; `lambda_cs=0.5`, `cs_n_sample_dates=20`; integrated in training loop with `B >= K+1` guard. (2) **Per-feature reconstruction weighting** (`loss.py`, `config.py`): `compute_weighted_reconstruction_loss()` with `feature_weights=[2.0, 0.5]` to fix 10× returns/vol reconstruction imbalance. (3) **AU post-filter via Bai-Ng** (`active_units.py`, `pipeline.py`): `post_filter_au_bai_ng()` caps AU at `max(k_bai_ng * factor, 2 * k_onatski)` with minimum 2; reduces 64 → ~21 effective factors. (4) **Entropy gradient clipping** (`entropy.py`): `max_grad_norm=10.0` in `compute_entropy_and_gradient()` prevents blow-up from near-zero risk contributions. (5) **Inverse-vol warm start** (`frontier.py`, `pipeline.py`): Frontier solver initializes from inverse-volatility weights instead of equal-weight. (6) **VT clamping [0.5, 2.0]** (`pipeline.py`, `config.py`): Per-factor and idiosyncratic VT now clamped via `vt_clamp_min/max` config params, preventing pro-cyclical over-shrinkage. (7) **Log-transform vol** (`windowing.py`): `log_transform_vol=True` compresses GARCH-like vol distribution before z-scoring; float32 guard threshold `1e-4` for z-score validation. (8) **Notebook config** (`dashboard.ipynb`): `λ_co_max` 0.1→0.5, curriculum 40/30/30, all new params wired. **21 new tests** (5 cross-sectional loss, 5 weighted recon, 6 AU post-filter, 3 entropy clipping, 2 warm start). 1113 tests pass, pyright 0 errors. |
| 2 | 2026-02-23 | **Fix diagnostic root causes: training config + SCA solver convergence** — Comprehensive fix for VAE underperformance (Sharpe -0.85 vs +0.28-0.67 benchmarks). (1) **Training config (ROOT CAUSE)**: `weight_decay` 1e-3→1e-5 (100× reduction), `lambda_co_max` 0.5→0.1, `learning_rate` 1e-3→5e-3, `dropout` 0.3→0.2, `patience` 50→20, `es_min_delta` 0.5→0.0. (2) **SCA solver 4 bugs fixed** in `sca_solver.py`: (a) Convergence now requires BOTH objective change AND gradient norm < sqrt(tol) (lines 1000-1009); (b) PGD convergence adds projected gradient check (line 323); (c) Gradient recalculated AFTER feasibility clipping (lines 985-995); (d) Warning threshold now adaptive `100*tol` instead of hardcoded 1e-4 (line 1020). (3) **Diagnostic reporting fix**: `pipeline.py:2632` now uses `pc.n_starts` instead of hardcoded 1 for accurate solver stats. (4) **Notebook config**: `SCA_N_STARTS` 3→5, `sca_max_iter` 100→300. 3 files modified. Impact: Fixes root cause of variance ratio=6.25 (target 0.5-2.0) and grad_norm=5.70 (target <1e-3). |
| 2 | 2026-02-22 | **Consolidate notebook sections 9b+ and 10+ into single cells** — Further notebook simplification: 15 cells → 2 cells (~420 lines → ~10 lines). (1) **notebook_helpers.py** (+300 lines) — Two new consolidation functions: `display_diagnostic_results()` combines 9b-9e (plots, report, holdings, exposures, ZIP, ML diagnostics including KL heatmap, PCA spectrum, literature table, VAE/PCA correlation); `run_decision_synthesis()` combines 10a-10e (root cause analysis, rules table, causal diagram, recommendations, validated JSON export). Both return summary dicts with display status. (2) **dashboard.ipynb** — Cells 66-75 (9 code cells for 9b-9e) replaced with single consolidated cell; Cells 77-81 (5 code cells for 10a-10e) replaced with single consolidated cell. Total cells reduced: 83→70. (3) **7 new unit tests** in test_visualization.py: TestDisplayDiagnosticResults (3), TestRunDecisionSynthesis (4). 2 files modified, 32 visualization tests pass, pyright 0 errors. Impact: Each notebook section is now a single function call, maximizing maintainability and testability. |
| 2 | 2026-02-22 | **Fast SCA sub-problem solver via projected gradient descent (~10-50× per-iteration speedup)** — Replaces CVXPY interior-point with Nesterov-accelerated PGD for the SCA sub-problem: (1) **sca_solver.py** (~200 lines added) — New `project_simplex_box()` implements Michelot 1986 + box constraints (O(n log n) via sorting); `solve_sca_subproblem_fast()` Nesterov PGD with adaptive step size (Lipschitz from Cholesky), handles concentration penalty, turnover penalty, expected returns. (2) **sca_optimize()** — New params `use_fast_subproblem=True` (default), `fast_subproblem_max_iter=50`, `fast_subproblem_tol=1e-7`; branches between fast PGD and legacy CVXPY. (3) **config.py** — 3 new `PortfolioConfig` params for fast solver. (4) **10 new unit tests** in `test_portfolio_optimization.py`: `TestFastSubproblemSolver` class with simplex projection (4 tests), PGD vs CVXPY quality (2 tests), speedup benchmark, turnover handling, multi-start integration, frontier integration. 2 files modified, 75 tests pass, pyright 0 errors. Impact: Per-SCA-iteration sub-problem ~10-50× faster (CVXPY 0.5-3s → PGD 5-50ms for n=1000); combined with frontier early stopping, total frontier time ~100× reduction for typical cases. |
| 2 | 2026-02-22 | **Variance-entropy frontier early stopping optimization (~50-70% time reduction)** — Two-phase adaptive grid with early stopping reduces frontier computation time significantly: (1) **config.py** — 8 new `PortfolioConfig` parameters: `frontier_coarse_grid` (default [0.001, 0.01, 0.1, 1.0, 5.0]), `frontier_early_stop_patience=2`, `frontier_n_starts_after_target=2`, `frontier_max_iter_after_target=50`, `frontier_refine_enabled=True`, `frontier_refine_points=3`, `frontier_n_starts_refine=3`, `frontier_max_iter_refine=75`. (2) **frontier.py** (~150 lines refactored) — Two-phase `compute_variance_entropy_frontier()`: Phase 1 uses coarse grid with early stopping when ENB target reached + variance increases `patience` times; uses reduced solver quality (`n_starts_after_target`, `max_iter_after_target`) post-target. Phase 2 (optional) refines around elbow with log-uniform interpolation. Added `target_enb`, `coarse_grid`, and quality params. (3) **pipeline.py** (~30 lines) — Computes `effective_target_enb` BEFORE frontier call (enables early stopping); passes all 8 new params to `compute_variance_entropy_frontier()`. (4) **5 new unit tests** in `test_portfolio_optimization.py`: `TestFrontierEarlyStop` class covering early stop trigger, Kneedle mode bypass, adaptive quality, Phase 2 refinement, legacy fallback. 3 files modified, 70 tests pass, pyright 0 errors. Impact: ENB-easy cases (target reached at α=0.01) use ~3-12 solver calls vs 55 (full grid), ~78% reduction. |
| 3 | 2026-02-22 | **Consolidated diagnostic persistence + extended ML data saving + notebook visualizations** — Unified diagnostic folder structure + added critical ML diagnostic arrays + notebook visualization cells: (1) **pipeline_state.py** (~150 lines added) — `create_latest_symlink()` for backwards-compatible `results/diagnostic` → latest timestamped run; `save_benchmark_weights()` saves all 6 benchmark weight vectors as NPY; `save_state_bag_for_stage(VAE_TRAINED)` now extracts kl_per_dim_history (E, K) and log_var_bounds_history (E, 2, K) from training history; INFERENCE_DONE saves B_full before AU filtering; COVARIANCE_DONE saves pca_loadings, pca_eigenvalues, literature_comparison JSON; **`load_run_data()` extended** to load all 6 new arrays. (2) **pipeline.py** (~60 lines) — Computes PCA loadings on training returns for VAE comparison; calls `compute_literature_comparison()` for Marchenko-Pastur/Bai-Ng AU validation; saves benchmark weights after benchmarks complete. (3) **diagnostics.py** (~80 lines) — New `compute_literature_comparison()` computes MP edge, counts eigenvalues above random matrix bulk, compares AU with Bai-Ng k and MP signal count. (4) **run_diagnostic.py** — Calls `create_latest_symlink("results/diagnostic")` after run for rétrocompatibilité. (5) **dashboard.ipynb** — New Section 9e "ML Diagnostic Visualizations" with 5 cells: 9e-1 KL per-dim heatmap (training dynamics), 9e-2 PCA eigenvalue spectrum + Marchenko-Pastur edge, 9e-3 literature comparison table (AU vs Bai-Ng vs MP), 9e-4 VAE vs PCA loading correlation. (6) **13 new unit tests** in test_pipeline_state.py covering all new `load_run_data()` arrays. 5 files modified, 73 tests pass, pyright 0 errors. |
| 4 | 2026-02-22 | **Google Drive persistence for Colab** — Automatic checkpointing to Google Drive when running in Colab, enabling crash recovery and persistent storage across runtime disconnections: (1) **NEW: `colab_drive.py`** (~180 lines) — `is_colab()` detection, `mount_drive()` mounting, `setup_drive_persistence()` main entry (creates symlink local→Drive), `cleanup_old_runs()` rotation (keep N most recent), `list_runs()` metadata extraction, `get_latest_run()` for resume. (2) **dashboard.ipynb** — 2 new cells at start: Cell 0 for Drive setup (creates symlink before any writes), Cell 1 for resume after disconnect (commented template). (3) **Architecture**: Local `results/diagnostic_runs/` becomes symlink to Drive — all existing code writes transparently to Drive. (4) **17 unit tests** in `test_colab_drive.py`. 3 files modified/created, pyright 0 errors. |
| 5 | 2026-02-22 | **Phase 2 memory optimization (~3GB additional savings)** — 9 targeted optimizations completing the memory reduction work: (1) **data_loader.py** — `sort_values(..., ignore_index=True)` replaces `sort_values().reset_index()` (~640MB), `groupby().filter()` replaces `transform("count")` (~40MB), vectorized `generate_synthetic_csv()` builds arrays directly instead of 1.3M dict records (~800MB). (2) **windowing.py** — Pre-allocated metadata arrays (`meta_stock_ids`, `meta_start_idx`, `meta_end_idx`) replace list-of-dicts accumulation (~2.1GB for stride=1 inference), removed redundant `.copy()` and `.astype(float32)` calls (~40MB/stock transient). (3) **returns.py** — NumPy-based log-return computation avoids triple DataFrame temporaries (~186MB), vectorized `winsorize_returns()` replaces `apply(axis=1)` (~124MB). (4) **diagnostics.py** — Explicit `del returns_centered` after Bai-Ng IC2 call (~72MB). 4 files modified, 1016 tests pass, pyright 0 errors. Combined with Phase 1, total estimated peak reduction: ~43GB. |
| 6 | 2026-02-22 | **Memory explosion fix during VAE checkpoint saving (130GB+ RAM)** — Fixed critical memory explosion (~130GB RAM crash) during checkpoint saving after VAE training. Root causes: inference windows (stride=1 vs training stride=21 = 21× more data ~32GB), full fit_result history with per-dim KL arrays, and recursive JSON serialization without size guards. (1) **pipeline_state.py** — `_serialize_for_json()` now has size guards: arrays >100k elements return placeholder, lists >10k items return placeholder, DataFrames >10k cells return placeholder, max recursion depth=10. `save_state_bag_for_stage(VAE_TRAINED, ...)` now saves only fit_result summary scalars + compact training_curve (epoch, train_loss, val_elbo, AU, sigma_sq, learning_rate) — excludes large kl_per_dim and log_var_stats arrays. (2) **pipeline.py** — Added `import gc`. After aggregate_profiles(): `del infer_windows, infer_metadata, trajectories; gc.collect(); clear_device_cache(device)` (~32GB freed). After trainer.fit(): `trainer.cleanup(); del train_w, val_w, train_raw, metadata, train_metadata, train_strata; gc.collect()`. Before save_state_bag: clears train_returns, B_A_by_date, universe_snapshots from state_bag. (3) **early_stopping.py** — `restore_best()` now sets `self.best_state = None` after restoring weights (~20MB freed). (4) **trainer.py** — New `cleanup()` method: closes TensorBoard, clears torch.compile cache via `torch._dynamo.reset()`, clears early_stopping.best_state, zeros optimizer gradients, clears GradScaler, empties CUDA/MPS cache. (5) **11 new unit tests**: 7 for size guards/fit_result filtering in test_pipeline_state.py, 4 for trainer cleanup in test_training.py. 4 files modified, 75 tests pass, pyright 0 errors. Expected impact: RAM during checkpoint <5GB (vs 130GB+ crash). |
| 7 | 2026-02-21 | **Extended checkpointing for complete diagnostic recovery** — Implemented timestamped run folders with full state_bag persistence for dashboard sections 9-10: (1) **pipeline_state.py** (~200 lines added) — `DiagnosticRunManager` class (timestamped folder creation `results/diagnostic_runs/YYYY-MM-DD_HHMMSS/`, VAE checkpoint discovery, `save_run_config()` with git hash), `load_run_data()` helper for notebook loading. Extended `PipelineStateManager` with 8 methods: `save_json()`/`load_json()`, `save_stage_arrays()`/`load_stage_arrays()`, `save_scalars()`/`load_scalars()`, `save_state_bag_for_stage()`/`load_state_bag_for_stage()` — routes arrays→NPY, dicts→JSON, scalars→merged JSON. (2) **diagnostic_report.py** — `save_diagnostic_report()` now accepts `weights` and `stock_ids` params, saves to `arrays/portfolio/w_vae.npy` and `json/inferred_stock_ids.json`. (3) **run_diagnostic.py** — 3 new CLI args: `--run-dir PATH` (resume existing), `--vae-checkpoint PATH` (override), `--base-dir PATH` (new runs). Returns dict with `run_dir`, `weights`, `stock_ids`. (4) **pipeline.py** — Enhanced resume logic loads state_bag for all completed stages via `load_state_bag_for_stage()`. (5) **dashboard.ipynb** cell 63 — New config: `RUN_DIR`, `LOAD_EXISTING`, `VAE_CHECKPOINT_OVERRIDE`; conditional logic to load from existing folder or run diagnostic; exposes `ACTIVE_RUN_DIR`. (6) **27 new unit tests** in `test_pipeline_state.py`: `TestExtendedStateBagPersistence` (9), `TestDiagnosticRunManager` (11), `TestLoadRunData` (7). 5 files modified, 53 tests pass, pyright 0 errors. |
| 8 | 2026-02-21 | **OOS rebalancing performance optimization** — Implemented differentiated solver quality settings for scheduled vs exceptional rebalancing, reducing OOS simulation time by ~67%: (1) **config.py** — 5 new/modified parameters: `rebalancing_frequency_days` 21→63 (quarterly), `oos_n_starts_scheduled=2` (quick), `oos_n_starts_exceptional=5` (full quality), `oos_sca_max_iter_scheduled=50`, `oos_sca_max_iter_exceptional=100`. Added validation in `__post_init__`. (2) **oos_rebalancing.py** — `_execute_rebalancing()` now accepts `trigger` parameter to differentiate quality settings; passes `max_iter` to `multi_start_optimize`. Enhanced logging: INFO level with turnover breakdown (one-way/two-way/cumulative), rebalance counter, H before→after, n_active. Added `expected_scheduled` computation. (3) **pipeline.py** — `oos_constraint_params` dict now includes all 4 OOS quality settings. Impact: 74 scheduled rebalances → ~25, solver time per scheduled 2-3min → 30-60s, exceptional triggers retain full quality. 3 files modified, 60 tests pass, pyright 0 errors. |


---

## Current State

- **Status**: Development — All pipeline phases complete + **deep post-mortem structural fixes** (cross-sectional R² loss, per-feature weighting, AU Bai-Ng post-filter, entropy gradient clipping, inverse-vol warm start, VT clamping, log-transform vol, updated curriculum). 53 source files + 18 test files + 1 notebook (70 cells). 1113 unit tests pass, pyright 0 errors.
- **Main features**: Full data pipeline, 1D-CNN VAE with **cross-sectional R² loss + per-feature reconstruction weighting**, 3-mode loss with co-movement curriculum (40/30/30), training loop with AMP + TensorBoard + curriculum batching, inference + **AU post-filtering via Bai-Ng IC2** (64 → ~21 effective factors), dual-rescaled factor risk model with **VT clamping [0.5, 2.0]** + **log-transformed volatility**, portfolio optimization (SCA+Armijo with **entropy gradient clipping** + **inverse-vol warm start**, fast PGD sub-problem solver, frontier early stopping), 6 benchmarks, walk-forward (34 folds), E2E diagnostic pipeline with 10 composite scores + decision synthesis, extended checkpointing, Google Drive persistence.
- **Next**: Re-run full Tiingo SP500 diagnostic to validate all 8 structural fixes. Target metrics: variance_ratio ∈ [0.5, 3.0], grad_norm < 0.01, cross-sectional R² > 25%, Sharpe > 0.0, AU_effective 15-25.

---

## Update Protocol

After each significant modification:

1. Add new entry at position 1, shift others down
2. If similar entry exists, update it instead of adding
3. If > 10 entries, remove entry #10
4. Update "Current State" if project status changed
