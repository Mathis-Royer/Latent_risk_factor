# Changelog

> **Purpose:** Track recent significant changes. Update after each major modification.
> Keep ~10 entries. Merge similar entries rather than adding duplicates.

## Recent Changes

| # | Date | Modification |
|---|------|--------------|
| 1 | 2026-02-24 | **Phase 0 revert: undo harmful V2 fixes + strategic paths document** — V2 diagnostic showed 86% Sharpe degradation (-1.584 vs -0.854). Per-fix autopsy confirmed: CS R² loss (harmful), AU Bai-Ng post-filter (harmful, eigenvalue concentration 88%), lambda_co_max increase (harmful), momentum (Principle 1/9 violation). (1) **config.py** — `momentum_enabled` default → False, `au_max_bai_ng_factor` default → 0.0 (disabled), validation range widened [0, 1000]. (2) **dashboard.ipynb** — 8 parameter reverts: `LAMBDA_CS` 0.5→0.0, `MAX_CO_MOVEMENT_WEIGHT` 0.5→0.1, `curriculum_phase1_frac` 0.40→0.30, `au_max_bai_ng_factor` 1.0→0.0, `MOMENTUM_ENABLED` True→False, `MOMENTUM_WEIGHT` 0.30→0.0, `PHI` 15.0→5.0, `sca_tol` 1e-8→1e-5. Kept: VT clamping, gradient clipping, inverse-vol warm start, fast PGD, OOS refresh, feature_weights, log_transform_vol. (3) **docs/strategic_paths_post_v2.md** — Detailed Path A (incremental: reduce universe, Mode A, n_signal-only entropy), Path B (architecture: FactorVAE, hybrid PCA+VAE, CS training fix), Path C (Principle 10: adopt PCA Factor RP). 2 files modified + 1 doc created, 166 relevant tests pass. |
| 1 | 2026-02-24 | **OOS replay from checkpoint + train vs OOS Sharpe comparison** — (1) **notebook_helpers.py** (~200 lines) — New `replay_oos_simulation()` loads checkpoint data (w_vae, B_A, stock_ids, alpha_opt), reconstructs risk model from B_A + data (~30s), then runs only the OOS simulation with current config. Skips VAE training (~5h), portfolio optimization (~5 min), and benchmarks (~10 min). Returns oos_result, metrics, risk_model. (2) **diagnostics.py** — `collect_diagnostics()` now computes training-period Sharpe (+ ann_return, ann_vol, max_drawdown) from w_vae applied to training returns, injects into portfolio dict. (3) **diagnostic_report.py** — Section 6 adds "Train vs OOS Comparison" table (Sharpe, return, vol, MDD side by side) before existing VAE Portfolio details. (4) **pipeline_state.py** — COVARIANCE_DONE now saves/loads `D_eps_port` array for future checkpoint completeness. 4 files modified, 1120 tests pass, pyright 0 errors. |
| 1 | 2026-02-24 | **OOS risk model refresh at each rebalancing (DVT §4.7)** — Eliminates evaluation asymmetry where benchmarks re-estimate their risk model at each OOS rebalancing but the VAE used a frozen training-period model. (1) **config.py** — New `refresh_risk_model_oos: bool = True` in `PortfolioConfig`. (2) **oos_rebalancing.py** (~200 lines) — New `_refresh_risk_model()` performs full risk model re-estimation at each rebalancing using expanding window `[train_start, current_date]`: extends `B_A_by_date` and `universe_snapshots` with new OOS dates, runs `rescale_estimation()` on new dates only, `estimate_factor_returns()` on full window, `compute_residuals()`, `estimate_sigma_z()`, `estimate_d_eps()`, `rescale_portfolio()`, `assemble_risk_model()`, signal/noise split, per-factor VT with holdout. `simulate_oos_rebalancing()` gains 6 new params (`refresh_risk_model`, `returns_full`, `train_start`, `risk_model_config`, `B_A_by_date_initial`, `universe_snapshots_initial`); loop uses `B_prime_current`/`eigenvalues_current` (mutable). Removed dead `_compute_d_eps_rolling()`. (3) **pipeline.py** — Passes all new params including `risk_model_config` dict and `B_A_by_date`/`universe_snapshots` from state_bag. (4) **7 new tests** in `test_oos_rebalancing.py`: refresh changes sigma, returns finite, frozen backward compat, expanding window grows, market intercept propagated, refresh helper unit, requires params validation. 3 files modified, 647 unit tests pass (1 pre-existing unrelated failure), pyright 0 errors. Impact: ~2-3s overhead per rebalancing (~50-75s total over 6-year OOS), negligible vs SCA solver time. |
| 2 | 2026-02-23 | **Deep post-mortem: 8 structural fixes for VAE factor model + solver** — Implements full deep post-mortem analysis plan (8 actions). (1) **Cross-sectional R² loss** (`loss.py`, `config.py`, `trainer.py`): New `compute_cross_sectional_loss()` — differentiable OLS R² loss forcing encoder mu to serve as useful factor exposures; `lambda_cs=0.5`, `cs_n_sample_dates=20`; integrated in training loop with `B >= K+1` guard. (2) **Per-feature reconstruction weighting** (`loss.py`, `config.py`): `compute_weighted_reconstruction_loss()` with `feature_weights=[2.0, 0.5]` to fix 10× returns/vol reconstruction imbalance. (3) **AU post-filter via Bai-Ng** (`active_units.py`, `pipeline.py`): `post_filter_au_bai_ng()` caps AU at `max(k_bai_ng * factor, 2 * k_onatski)` with minimum 2; reduces 64 → ~21 effective factors. (4) **Entropy gradient clipping** (`entropy.py`): `max_grad_norm=10.0` in `compute_entropy_and_gradient()` prevents blow-up from near-zero risk contributions. (5) **Inverse-vol warm start** (`frontier.py`, `pipeline.py`): Frontier solver initializes from inverse-volatility weights instead of equal-weight. (6) **VT clamping [0.5, 2.0]** (`pipeline.py`, `config.py`): Per-factor and idiosyncratic VT now clamped via `vt_clamp_min/max` config params, preventing pro-cyclical over-shrinkage. (7) **Log-transform vol** (`windowing.py`): `log_transform_vol=True` compresses GARCH-like vol distribution before z-scoring; float32 guard threshold `1e-4` for z-score validation. (8) **Notebook config** (`dashboard.ipynb`): `λ_co_max` 0.1→0.5, curriculum 40/30/30, all new params wired. **21 new tests** (5 cross-sectional loss, 5 weighted recon, 6 AU post-filter, 3 entropy clipping, 2 warm start). 1113 tests pass, pyright 0 errors. |
| 2 | 2026-02-23 | **Fix diagnostic root causes: training config + SCA solver convergence** — Comprehensive fix for VAE underperformance (Sharpe -0.85 vs +0.28-0.67 benchmarks). (1) **Training config (ROOT CAUSE)**: `weight_decay` 1e-3→1e-5 (100× reduction), `lambda_co_max` 0.5→0.1, `learning_rate` 1e-3→5e-3, `dropout` 0.3→0.2, `patience` 50→20, `es_min_delta` 0.5→0.0. (2) **SCA solver 4 bugs fixed** in `sca_solver.py`: (a) Convergence now requires BOTH objective change AND gradient norm < sqrt(tol) (lines 1000-1009); (b) PGD convergence adds projected gradient check (line 323); (c) Gradient recalculated AFTER feasibility clipping (lines 985-995); (d) Warning threshold now adaptive `100*tol` instead of hardcoded 1e-4 (line 1020). (3) **Diagnostic reporting fix**: `pipeline.py:2632` now uses `pc.n_starts` instead of hardcoded 1 for accurate solver stats. (4) **Notebook config**: `SCA_N_STARTS` 3→5, `sca_max_iter` 100→300. 3 files modified. Impact: Fixes root cause of variance ratio=6.25 (target 0.5-2.0) and grad_norm=5.70 (target <1e-3). |
| 2 | 2026-02-22 | **Consolidate notebook sections 9b+ and 10+ into single cells** — Further notebook simplification: 15 cells → 2 cells (~420 lines → ~10 lines). (1) **notebook_helpers.py** (+300 lines) — Two new consolidation functions: `display_diagnostic_results()` combines 9b-9e (plots, report, holdings, exposures, ZIP, ML diagnostics including KL heatmap, PCA spectrum, literature table, VAE/PCA correlation); `run_decision_synthesis()` combines 10a-10e (root cause analysis, rules table, causal diagram, recommendations, validated JSON export). Both return summary dicts with display status. (2) **dashboard.ipynb** — Cells 66-75 (9 code cells for 9b-9e) replaced with single consolidated cell; Cells 77-81 (5 code cells for 10a-10e) replaced with single consolidated cell. Total cells reduced: 83→70. (3) **7 new unit tests** in test_visualization.py: TestDisplayDiagnosticResults (3), TestRunDecisionSynthesis (4). 2 files modified, 32 visualization tests pass, pyright 0 errors. Impact: Each notebook section is now a single function call, maximizing maintainability and testability. |
| 2 | 2026-02-22 | **Fast SCA sub-problem solver via projected gradient descent (~10-50× per-iteration speedup)** — Replaces CVXPY interior-point with Nesterov-accelerated PGD for the SCA sub-problem: (1) **sca_solver.py** (~200 lines added) — New `project_simplex_box()` implements Michelot 1986 + box constraints (O(n log n) via sorting); `solve_sca_subproblem_fast()` Nesterov PGD with adaptive step size (Lipschitz from Cholesky), handles concentration penalty, turnover penalty, expected returns. (2) **sca_optimize()** — New params `use_fast_subproblem=True` (default), `fast_subproblem_max_iter=50`, `fast_subproblem_tol=1e-7`; branches between fast PGD and legacy CVXPY. (3) **config.py** — 3 new `PortfolioConfig` params for fast solver. (4) **10 new unit tests** in `test_portfolio_optimization.py`: `TestFastSubproblemSolver` class with simplex projection (4 tests), PGD vs CVXPY quality (2 tests), speedup benchmark, turnover handling, multi-start integration, frontier integration. 2 files modified, 75 tests pass, pyright 0 errors. Impact: Per-SCA-iteration sub-problem ~10-50× faster (CVXPY 0.5-3s → PGD 5-50ms for n=1000); combined with frontier early stopping, total frontier time ~100× reduction for typical cases. |
| 2 | 2026-02-22 | **Variance-entropy frontier early stopping optimization (~50-70% time reduction)** — Two-phase adaptive grid with early stopping reduces frontier computation time significantly: (1) **config.py** — 8 new `PortfolioConfig` parameters: `frontier_coarse_grid` (default [0.001, 0.01, 0.1, 1.0, 5.0]), `frontier_early_stop_patience=2`, `frontier_n_starts_after_target=2`, `frontier_max_iter_after_target=50`, `frontier_refine_enabled=True`, `frontier_refine_points=3`, `frontier_n_starts_refine=3`, `frontier_max_iter_refine=75`. (2) **frontier.py** (~150 lines refactored) — Two-phase `compute_variance_entropy_frontier()`: Phase 1 uses coarse grid with early stopping when ENB target reached + variance increases `patience` times; uses reduced solver quality (`n_starts_after_target`, `max_iter_after_target`) post-target. Phase 2 (optional) refines around elbow with log-uniform interpolation. Added `target_enb`, `coarse_grid`, and quality params. (3) **pipeline.py** (~30 lines) — Computes `effective_target_enb` BEFORE frontier call (enables early stopping); passes all 8 new params to `compute_variance_entropy_frontier()`. (4) **5 new unit tests** in `test_portfolio_optimization.py`: `TestFrontierEarlyStop` class covering early stop trigger, Kneedle mode bypass, adaptive quality, Phase 2 refinement, legacy fallback. 3 files modified, 70 tests pass, pyright 0 errors. Impact: ENB-easy cases (target reached at α=0.01) use ~3-12 solver calls vs 55 (full grid), ~78% reduction. |
| 3 | 2026-02-22 | **Consolidated diagnostic persistence + extended ML data saving + notebook visualizations** — Unified diagnostic folder structure + added critical ML diagnostic arrays + notebook visualization cells: (1) **pipeline_state.py** (~150 lines added) — `create_latest_symlink()` for backwards-compatible `results/diagnostic` → latest timestamped run; `save_benchmark_weights()` saves all 6 benchmark weight vectors as NPY; `save_state_bag_for_stage(VAE_TRAINED)` now extracts kl_per_dim_history (E, K) and log_var_bounds_history (E, 2, K) from training history; INFERENCE_DONE saves B_full before AU filtering; COVARIANCE_DONE saves pca_loadings, pca_eigenvalues, literature_comparison JSON; **`load_run_data()` extended** to load all 6 new arrays. (2) **pipeline.py** (~60 lines) — Computes PCA loadings on training returns for VAE comparison; calls `compute_literature_comparison()` for Marchenko-Pastur/Bai-Ng AU validation; saves benchmark weights after benchmarks complete. (3) **diagnostics.py** (~80 lines) — New `compute_literature_comparison()` computes MP edge, counts eigenvalues above random matrix bulk, compares AU with Bai-Ng k and MP signal count. (4) **run_diagnostic.py** — Calls `create_latest_symlink("results/diagnostic")` after run for rétrocompatibilité. (5) **dashboard.ipynb** — New Section 9e "ML Diagnostic Visualizations" with 5 cells: 9e-1 KL per-dim heatmap (training dynamics), 9e-2 PCA eigenvalue spectrum + Marchenko-Pastur edge, 9e-3 literature comparison table (AU vs Bai-Ng vs MP), 9e-4 VAE vs PCA loading correlation. (6) **13 new unit tests** in test_pipeline_state.py covering all new `load_run_data()` arrays. 5 files modified, 73 tests pass, pyright 0 errors. |
| 4 | 2026-02-22 | **Google Drive persistence for Colab** — Automatic checkpointing to Google Drive when running in Colab, enabling crash recovery and persistent storage across runtime disconnections: (1) **NEW: `colab_drive.py`** (~180 lines) — `is_colab()` detection, `mount_drive()` mounting, `setup_drive_persistence()` main entry (creates symlink local→Drive), `cleanup_old_runs()` rotation (keep N most recent), `list_runs()` metadata extraction, `get_latest_run()` for resume. (2) **dashboard.ipynb** — 2 new cells at start: Cell 0 for Drive setup (creates symlink before any writes), Cell 1 for resume after disconnect (commented template). (3) **Architecture**: Local `results/diagnostic_runs/` becomes symlink to Drive — all existing code writes transparently to Drive. (4) **17 unit tests** in `test_colab_drive.py`. 3 files modified/created, pyright 0 errors. |
| 5 | 2026-02-22 | **Phase 2 memory optimization (~3GB additional savings)** — 9 targeted optimizations completing the memory reduction work: (1) **data_loader.py** — `sort_values(..., ignore_index=True)` replaces `sort_values().reset_index()` (~640MB), `groupby().filter()` replaces `transform("count")` (~40MB), vectorized `generate_synthetic_csv()` builds arrays directly instead of 1.3M dict records (~800MB). (2) **windowing.py** — Pre-allocated metadata arrays (`meta_stock_ids`, `meta_start_idx`, `meta_end_idx`) replace list-of-dicts accumulation (~2.1GB for stride=1 inference), removed redundant `.copy()` and `.astype(float32)` calls (~40MB/stock transient). (3) **returns.py** — NumPy-based log-return computation avoids triple DataFrame temporaries (~186MB), vectorized `winsorize_returns()` replaces `apply(axis=1)` (~124MB). (4) **diagnostics.py** — Explicit `del returns_centered` after Bai-Ng IC2 call (~72MB). 4 files modified, 1016 tests pass, pyright 0 errors. Combined with Phase 1, total estimated peak reduction: ~43GB. |
| 6 | 2026-02-22 | **Memory explosion fix during VAE checkpoint saving (130GB+ RAM)** — Fixed critical memory explosion (~130GB RAM crash) during checkpoint saving after VAE training. Root causes: inference windows (stride=1 vs training stride=21 = 21× more data ~32GB), full fit_result history with per-dim KL arrays, and recursive JSON serialization without size guards. (1) **pipeline_state.py** — `_serialize_for_json()` now has size guards: arrays >100k elements return placeholder, lists >10k items return placeholder, DataFrames >10k cells return placeholder, max recursion depth=10. `save_state_bag_for_stage(VAE_TRAINED, ...)` now saves only fit_result summary scalars + compact training_curve (epoch, train_loss, val_elbo, AU, sigma_sq, learning_rate) — excludes large kl_per_dim and log_var_stats arrays. (2) **pipeline.py** — Added `import gc`. After aggregate_profiles(): `del infer_windows, infer_metadata, trajectories; gc.collect(); clear_device_cache(device)` (~32GB freed). After trainer.fit(): `trainer.cleanup(); del train_w, val_w, train_raw, metadata, train_metadata, train_strata; gc.collect()`. Before save_state_bag: clears train_returns, B_A_by_date, universe_snapshots from state_bag. (3) **early_stopping.py** — `restore_best()` now sets `self.best_state = None` after restoring weights (~20MB freed). (4) **trainer.py** — New `cleanup()` method: closes TensorBoard, clears torch.compile cache via `torch._dynamo.reset()`, clears early_stopping.best_state, zeros optimizer gradients, clears GradScaler, empties CUDA/MPS cache. (5) **11 new unit tests**: 7 for size guards/fit_result filtering in test_pipeline_state.py, 4 for trainer cleanup in test_training.py. 4 files modified, 75 tests pass, pyright 0 errors. Expected impact: RAM during checkpoint <5GB (vs 130GB+ crash). |


---

## Current State

- **Status**: Development — **Phase 0 revert applied** (harmful V2 fixes reverted). 53 source files + 18 test files + 1 notebook (70 cells). 1120 unit tests pass (1 pre-existing unrelated failure), pyright 0 errors.
- **Main features**: Full data pipeline, 1D-CNN VAE with per-feature reconstruction weighting, 3-mode loss with co-movement curriculum (30/30/40), training loop with AMP + TensorBoard + curriculum batching, inference (AU uncapped, Bai-Ng post-filter disabled), dual-rescaled factor risk model with **VT clamping [0.5, 2.0]** + **log-transformed volatility** + **OOS expanding-window refresh**, portfolio optimization (SCA+Armijo with **entropy gradient clipping** + **inverse-vol warm start**, fast PGD sub-problem solver, frontier early stopping, **sca_tol=1e-5**, **momentum disabled**), 6 benchmarks, walk-forward (34 folds), E2E diagnostic pipeline with 10 composite scores + decision synthesis, extended checkpointing, Google Drive persistence.
- **Next**: Run Phase 1 clean diagnostic to establish baseline after V2 revert. Decision point: Path A (incremental, if CS R² > 10%), Path B (architecture change, if CS R² < 5%), Path C (Principle 10: adopt PCA Factor RP). See `docs/strategic_paths_post_v2.md`.

---

## Update Protocol

After each significant modification:

1. Add new entry at position 1, shift others down
2. If similar entry exists, update it instead of adding
3. If > 10 entries, remove entry #10
4. Update "Current State" if project status changed
